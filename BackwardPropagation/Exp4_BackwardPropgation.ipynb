{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Backward propagation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network Architecture\n",
    "![](../imgs/nn_model.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 两种损失函数\n",
    "**吴恩达讲解版本**\n",
    "$$Loss = -\\sum_{k=1}^{K}{ q(k|x)\\log{p(k|x) - (1-q(k|x))(1-\\log{p(k|x))}}$$\n",
    "**PyTorch官方文档**\n",
    "$$Loss = -\\sum_{k=1}^{K}{ q(k|x)\\log{p(k|x)}$$\n",
    "\n",
    "PyTorch官方版本只关注正确分类的能力，对于不是目标类的误分类概率不考虑，吴恩达版本同时更倾向于使得每个类**误分类的概率最小**\n",
    "例如，存在一个标签 $\\left[ \\begin{matrix} 1 \\\\ 0 \\\\ 0 \\end{matrix} \\right]$\n",
    "给出两种预测的分布：$\\left[ \\begin{matrix} 0.8 \\\\ 0.2 \\\\ 0 \\end{matrix} \\right]$ 和 $\\left[ \\begin{matrix} 0.8 \\\\ 0.1 \\\\ 0.1 \\end{matrix} \\right]$\n",
    "\n",
    "对于PyTorch版本，两种损失计算得到均为$-\\log{0.8}$\n",
    "\n",
    "对于吴恩达版本，第一种预测的损失为$-(\\log{0.8}+\\log{0.8}+\\log{1})\\approx 0.1938$，第二种预测的损失为$-(\\log{0.8}+\\log{0.9}+\\log{0.9}) \\approx 0.1884$\n",
    "\n",
    "设预测正确类的概率为$p$，则在误分类概率均匀分布的时候，损失的附加项为$-\\log{(1-\\frac{1-p}{K-1})^{K-1}}$(对数运算加变乘)，误分类损失集中在一个类的时候，损失的附加项为$-\\log{(1-(1-p))}=-\\log{p}$\n",
    "二者函数曲线图如下所示\n",
    "![](../imgs/Wu_MultiClass_Loss.png)\n",
    "\n",
    "数学推导：\n",
    "$-\\log{x}$是关于x的递减函数，只需比较x，$(1-\\frac{1-p}{K-1})^{K-1}$ VS $p$\n",
    "$K=2$，两项均为$p$，事实上在二分类的情况下，Wu损失函数的附加项和PyTorch交叉熵相等，所以BCE损失函数只用了一项\n",
    "$(1-\\frac{1-p}{K-1})^{K-1} = (\\frac{K-2+p}{K-1})^{K-1}$，设$\\alpha =K-1>1$，$\\beta =K-2>0$\n",
    "$(\\frac{(\\beta+p)^{\\alpha}}{\\alpha^{\\alpha}})$ VS $p$\n",
    "$(\\beta+p)^{\\alpha}$ VS $\\alpha^{\\alpha} p$\n",
    "$(\\beta+p)^{\\alpha} < p^{\\alpha} < \\alpha p < \\alpha^{\\alpha} p \\qquad p\\in(0,1)$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import matplotlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "加载数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "((5000, 400), (5000, 1))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = scio.loadmat('ex4data1.mat')\n",
    "x = data.get('X')\n",
    "y = data.get('y')\n",
    "x.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对标签进行Onehot编码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "((5000, 10), (10,))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from NueralNetwork.Network import  onehot_encode\n",
    "y_onehot, cls = onehot_encode(y)\n",
    "y_onehot.shape, cls.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "划分验证集进行验证，在验证集上的结果证明，模型及其容易过拟合"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\t500 train samples\t500 val samples\n",
      "Class 2:\t500 train samples\t500 val samples\n",
      "Class 3:\t500 train samples\t500 val samples\n",
      "Class 4:\t500 train samples\t500 val samples\n",
      "Class 5:\t500 train samples\t500 val samples\n",
      "Class 6:\t500 train samples\t500 val samples\n",
      "Class 7:\t500 train samples\t500 val samples\n",
      "Class 8:\t500 train samples\t500 val samples\n",
      "Class 9:\t500 train samples\t500 val samples\n",
      "Class 10:\t500 train samples\t500 val samples\n",
      "Total train samples: 5000\n",
      "Total val samples: 5000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# train_x, val_x, train_y, val_y = train_test_split(x, y_onehot, test_size=0.2)\n",
    "train_x, val_x, train_y, val_y = x,x,y_onehot,y_onehot\n",
    "for cls_idx in cls:\n",
    "    train_sample_n = np.where(train_y[:,cls_idx-1]==1)[0].shape[0]\n",
    "    val_sample_n = np.where(val_y[:,cls_idx-1]==1)[0].shape[0]\n",
    "    print(\"Class {}:\\t{} train samples\\t{} val samples\".format(cls_idx, train_sample_n, val_sample_n))\n",
    "print(\"Total train samples: {}\\n\"\n",
    "      \"Total val samples: {}\".format(train_y.shape[0],val_y.shape[0]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "加载参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "((401, 25), (26, 10))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = scio.loadmat('ex4weights.mat')\n",
    "theta1 = parameters.get('Theta1').T\n",
    "theta2 = parameters.get('Theta2').T\n",
    "theta1.shape, theta2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用前馈传播，查看性能"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9820    0.9684    0.9752       507\n",
      "           2     0.9700    0.9818    0.9759       494\n",
      "           3     0.9600    0.9776    0.9687       491\n",
      "           4     0.9680    0.9699    0.9690       499\n",
      "           5     0.9840    0.9723    0.9781       506\n",
      "           6     0.9860    0.9782    0.9821       504\n",
      "           7     0.9700    0.9778    0.9739       496\n",
      "           8     0.9820    0.9781    0.9800       502\n",
      "           9     0.9580    0.9657    0.9618       496\n",
      "          10     0.9920    0.9822    0.9871       505\n",
      "\n",
      "    accuracy                         0.9752      5000\n",
      "   macro avg     0.9752    0.9752    0.9752      5000\n",
      "weighted avg     0.9753    0.9752    0.9752      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from BackwardPropagation import BackPropModel, regularized_loss\n",
    "model_val = BackPropModel()\n",
    "model_val.load_parameters([theta1, theta2])\n",
    "pred_prob = model_val(val_x)\n",
    "\n",
    "pred = np.argmax(pred_prob,axis=1) + 1\n",
    "target =np.argmax(val_y, axis=1) + 1\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(pred, target, digits=4)\n",
    "print(report)\n",
    "loss = regularized_loss(pred_prob, val_y, [model_val.theta1, model_val.theta2], scale=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "进行梯度校验\n",
    "$$f^{\\prime}(\\theta) \\approx \\frac{J(\\theta+\\epsilon) - J(\\theta-\\epsilon)}{2\\times\\epsilon}$$\n",
    "对于$\\theta$的每一项，单独进行$\\epsilon$的加减，根据公式求其导数，随后和相对的梯队比较，随后求相对误差\n",
    "$$diff = \\Vert grad - grad_{approx}\\Vert_2 / (\\Vert gard \\Vert_2 + \\Vert grad_{approx} \\Vert_2)$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "e = 1e-4\n",
    "regularized = False\n",
    "\n",
    "theta = np.concatenate((theta1.flatten(),theta2.flatten()))\n",
    "theta_matrix = np.array(np.matrix(np.ones(theta.shape[0])).T @ np.matrix(theta))\n",
    "epsilon_matrix = np.identity(len(theta)) * e\n",
    "\n",
    "plus_matrix = theta_matrix + epsilon_matrix\n",
    "minus_matrix = theta_matrix - epsilon_matrix\n",
    "from BackwardPropagation import BackPropModel, gradient, loss\n",
    "model_g = BackPropModel()\n",
    "g1 = []\n",
    "for i in range(len(theta)):\n",
    "    theta_p1 = plus_matrix[i][:401*25].reshape(401,25)\n",
    "    theta_p2 = plus_matrix[i][401*25:].reshape(26,10)\n",
    "    model_g.load_parameters([theta_p1,theta_p2])\n",
    "    output_g = model_g(x)\n",
    "    plus_g = loss(output_g, y_onehot) / output_g.shape[0]\n",
    "\n",
    "    theta_m1 = minus_matrix[i][:401*25].reshape(401,25)\n",
    "    theta_m2 = minus_matrix[i][401*25:].reshape(26,10)\n",
    "    model_g.load_parameters([theta_m1,theta_m2])\n",
    "    output_g = model_g(x)\n",
    "    minus_g = loss(output_g, y_onehot) / output_g.shape[0]\n",
    "\n",
    "    g1.append((plus_g - minus_g) / (2 * e))\n",
    "\n",
    "g1 = np.array(g1)\n",
    "model = BackPropModel()\n",
    "model.load_parameters([theta1, theta2])\n",
    "out = model(x)\n",
    "\n",
    "g = gradient(model, out, y_onehot)\n",
    "\n",
    "g2 = np.concatenate((g[0].flatten(), g[1].flatten()))\n",
    "diff = np.linalg.norm(g1 - g2) / (np.linalg.norm(g1) + np.linalg.norm(g2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Check Result (Regularized: False):\n",
      "The relative difference is 2.145756e-09, assuming epsilon is 0.0001\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Check Result (Regularized: {}):\\n\"\n",
    "      \"The relative difference is {:e}, assuming epsilon is {}\".format(regularized, diff, e))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用Backpropagation进行训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 6.6658\tVal Loss: 5.7531\n",
      "Epoch: 1\tTrain Loss: 5.7525\tVal Loss: 5.3290\n",
      "Epoch: 2\tTrain Loss: 5.3300\tVal Loss: 3.7822\n",
      "Epoch: 3\tTrain Loss: 3.7818\tVal Loss: 3.4132\n",
      "Epoch: 4\tTrain Loss: 3.4135\tVal Loss: 3.3025\n",
      "Epoch: 5\tTrain Loss: 3.3024\tVal Loss: 3.2606\n",
      "Epoch: 6\tTrain Loss: 3.2607\tVal Loss: 3.2481\n",
      "Epoch: 7\tTrain Loss: 3.2481\tVal Loss: 3.2387\n",
      "Epoch: 8\tTrain Loss: 3.2388\tVal Loss: 3.2302\n",
      "Epoch: 9\tTrain Loss: 3.2302\tVal Loss: 3.2208\n",
      "Epoch: 10\tTrain Loss: 3.2208\tVal Loss: 3.2101\n",
      "Epoch: 11\tTrain Loss: 3.2101\tVal Loss: 3.1974\n",
      "Epoch: 12\tTrain Loss: 3.1974\tVal Loss: 3.1821\n",
      "Epoch: 13\tTrain Loss: 3.1821\tVal Loss: 3.1636\n",
      "Epoch: 14\tTrain Loss: 3.1636\tVal Loss: 3.1414\n",
      "Epoch: 15\tTrain Loss: 3.1414\tVal Loss: 3.1149\n",
      "Epoch: 16\tTrain Loss: 3.1150\tVal Loss: 3.0841\n",
      "Epoch: 17\tTrain Loss: 3.0841\tVal Loss: 3.0488\n",
      "Epoch: 18\tTrain Loss: 3.0489\tVal Loss: 3.0097\n",
      "Epoch: 19\tTrain Loss: 3.0098\tVal Loss: 2.9674\n",
      "Epoch: 20\tTrain Loss: 2.9676\tVal Loss: 2.9227\n",
      "Epoch: 21\tTrain Loss: 2.9229\tVal Loss: 2.8757\n",
      "Epoch: 22\tTrain Loss: 2.8758\tVal Loss: 2.8263\n",
      "Epoch: 23\tTrain Loss: 2.8265\tVal Loss: 2.7750\n",
      "Epoch: 24\tTrain Loss: 2.7752\tVal Loss: 2.7223\n",
      "Epoch: 25\tTrain Loss: 2.7226\tVal Loss: 2.6694\n",
      "Epoch: 26\tTrain Loss: 2.6697\tVal Loss: 2.6169\n",
      "Epoch: 27\tTrain Loss: 2.6172\tVal Loss: 2.5650\n",
      "Epoch: 28\tTrain Loss: 2.5653\tVal Loss: 2.5134\n",
      "Epoch: 29\tTrain Loss: 2.5137\tVal Loss: 2.4619\n",
      "Epoch: 30\tTrain Loss: 2.4622\tVal Loss: 2.4108\n",
      "Epoch: 31\tTrain Loss: 2.4111\tVal Loss: 2.3609\n",
      "Epoch: 32\tTrain Loss: 2.3612\tVal Loss: 2.3129\n",
      "Epoch: 33\tTrain Loss: 2.3132\tVal Loss: 2.2669\n",
      "Epoch: 34\tTrain Loss: 2.2673\tVal Loss: 2.2228\n",
      "Epoch: 35\tTrain Loss: 2.2232\tVal Loss: 2.1803\n",
      "Epoch: 36\tTrain Loss: 2.1807\tVal Loss: 2.1394\n",
      "Epoch: 37\tTrain Loss: 2.1398\tVal Loss: 2.0997\n",
      "Epoch: 38\tTrain Loss: 2.1001\tVal Loss: 2.0613\n",
      "Epoch: 39\tTrain Loss: 2.0617\tVal Loss: 2.0241\n",
      "Epoch: 40\tTrain Loss: 2.0245\tVal Loss: 1.9880\n",
      "Epoch: 41\tTrain Loss: 1.9884\tVal Loss: 1.9529\n",
      "Epoch: 42\tTrain Loss: 1.9533\tVal Loss: 1.9189\n",
      "Epoch: 43\tTrain Loss: 1.9193\tVal Loss: 1.8860\n",
      "Epoch: 44\tTrain Loss: 1.8864\tVal Loss: 1.8543\n",
      "Epoch: 45\tTrain Loss: 1.8547\tVal Loss: 1.8237\n",
      "Epoch: 46\tTrain Loss: 1.8241\tVal Loss: 1.7943\n",
      "Epoch: 47\tTrain Loss: 1.7948\tVal Loss: 1.7661\n",
      "Epoch: 48\tTrain Loss: 1.7665\tVal Loss: 1.7388\n",
      "Epoch: 49\tTrain Loss: 1.7392\tVal Loss: 1.7126\n",
      "Epoch: 50\tTrain Loss: 1.7130\tVal Loss: 1.6872\n",
      "Epoch: 51\tTrain Loss: 1.6876\tVal Loss: 1.6627\n",
      "Epoch: 52\tTrain Loss: 1.6631\tVal Loss: 1.6390\n",
      "Epoch: 53\tTrain Loss: 1.6394\tVal Loss: 1.6161\n",
      "Epoch: 54\tTrain Loss: 1.6165\tVal Loss: 1.5938\n",
      "Epoch: 55\tTrain Loss: 1.5942\tVal Loss: 1.5722\n",
      "Epoch: 56\tTrain Loss: 1.5726\tVal Loss: 1.5512\n",
      "Epoch: 57\tTrain Loss: 1.5516\tVal Loss: 1.5307\n",
      "Epoch: 58\tTrain Loss: 1.5311\tVal Loss: 1.5108\n",
      "Epoch: 59\tTrain Loss: 1.5112\tVal Loss: 1.4913\n",
      "Epoch: 60\tTrain Loss: 1.4917\tVal Loss: 1.4722\n",
      "Epoch: 61\tTrain Loss: 1.4726\tVal Loss: 1.4536\n",
      "Epoch: 62\tTrain Loss: 1.4540\tVal Loss: 1.4354\n",
      "Epoch: 63\tTrain Loss: 1.4358\tVal Loss: 1.4175\n",
      "Epoch: 64\tTrain Loss: 1.4179\tVal Loss: 1.4000\n",
      "Epoch: 65\tTrain Loss: 1.4004\tVal Loss: 1.3829\n",
      "Epoch: 66\tTrain Loss: 1.3833\tVal Loss: 1.3661\n",
      "Epoch: 67\tTrain Loss: 1.3665\tVal Loss: 1.3497\n",
      "Epoch: 68\tTrain Loss: 1.3501\tVal Loss: 1.3336\n",
      "Epoch: 69\tTrain Loss: 1.3340\tVal Loss: 1.3178\n",
      "Epoch: 70\tTrain Loss: 1.3182\tVal Loss: 1.3024\n",
      "Epoch: 71\tTrain Loss: 1.3028\tVal Loss: 1.2873\n",
      "Epoch: 72\tTrain Loss: 1.2877\tVal Loss: 1.2725\n",
      "Epoch: 73\tTrain Loss: 1.2729\tVal Loss: 1.2581\n",
      "Epoch: 74\tTrain Loss: 1.2585\tVal Loss: 1.2440\n",
      "Epoch: 75\tTrain Loss: 1.2444\tVal Loss: 1.2302\n",
      "Epoch: 76\tTrain Loss: 1.2306\tVal Loss: 1.2167\n",
      "Epoch: 77\tTrain Loss: 1.2171\tVal Loss: 1.2036\n",
      "Epoch: 78\tTrain Loss: 1.2040\tVal Loss: 1.1908\n",
      "Epoch: 79\tTrain Loss: 1.1912\tVal Loss: 1.1783\n",
      "Epoch: 80\tTrain Loss: 1.1786\tVal Loss: 1.1661\n",
      "Epoch: 81\tTrain Loss: 1.1664\tVal Loss: 1.1542\n",
      "Epoch: 82\tTrain Loss: 1.1545\tVal Loss: 1.1425\n",
      "Epoch: 83\tTrain Loss: 1.1429\tVal Loss: 1.1312\n",
      "Epoch: 84\tTrain Loss: 1.1316\tVal Loss: 1.1202\n",
      "Epoch: 85\tTrain Loss: 1.1205\tVal Loss: 1.1094\n",
      "Epoch: 86\tTrain Loss: 1.1098\tVal Loss: 1.0989\n",
      "Epoch: 87\tTrain Loss: 1.0992\tVal Loss: 1.0886\n",
      "Epoch: 88\tTrain Loss: 1.0890\tVal Loss: 1.0786\n",
      "Epoch: 89\tTrain Loss: 1.0790\tVal Loss: 1.0689\n",
      "Epoch: 90\tTrain Loss: 1.0692\tVal Loss: 1.0593\n",
      "Epoch: 91\tTrain Loss: 1.0597\tVal Loss: 1.0500\n",
      "Epoch: 92\tTrain Loss: 1.0504\tVal Loss: 1.0410\n",
      "Epoch: 93\tTrain Loss: 1.0413\tVal Loss: 1.0321\n",
      "Epoch: 94\tTrain Loss: 1.0325\tVal Loss: 1.0235\n",
      "Epoch: 95\tTrain Loss: 1.0238\tVal Loss: 1.0151\n",
      "Epoch: 96\tTrain Loss: 1.0154\tVal Loss: 1.0068\n",
      "Epoch: 97\tTrain Loss: 1.0071\tVal Loss: 0.9988\n",
      "Epoch: 98\tTrain Loss: 0.9991\tVal Loss: 0.9909\n",
      "Epoch: 99\tTrain Loss: 0.9913\tVal Loss: 0.9833\n",
      "Epoch: 100\tTrain Loss: 0.9837\tVal Loss: 0.9758\n",
      "Epoch: 101\tTrain Loss: 0.9761\tVal Loss: 0.9686\n",
      "Epoch: 102\tTrain Loss: 0.9689\tVal Loss: 0.9614\n",
      "Epoch: 103\tTrain Loss: 0.9618\tVal Loss: 0.9546\n",
      "Epoch: 104\tTrain Loss: 0.9549\tVal Loss: 0.9478\n",
      "Epoch: 105\tTrain Loss: 0.9481\tVal Loss: 0.9413\n",
      "Epoch: 106\tTrain Loss: 0.9417\tVal Loss: 0.9349\n",
      "Epoch: 107\tTrain Loss: 0.9352\tVal Loss: 0.9288\n",
      "Epoch: 108\tTrain Loss: 0.9291\tVal Loss: 0.9226\n",
      "Epoch: 109\tTrain Loss: 0.9229\tVal Loss: 0.9169\n",
      "Epoch: 110\tTrain Loss: 0.9172\tVal Loss: 0.9110\n",
      "Epoch: 111\tTrain Loss: 0.9113\tVal Loss: 0.9056\n",
      "Epoch: 112\tTrain Loss: 0.9059\tVal Loss: 0.9000\n",
      "Epoch: 113\tTrain Loss: 0.9003\tVal Loss: 0.8949\n",
      "Epoch: 114\tTrain Loss: 0.8952\tVal Loss: 0.8894\n",
      "Epoch: 115\tTrain Loss: 0.8897\tVal Loss: 0.8846\n",
      "Epoch: 116\tTrain Loss: 0.8849\tVal Loss: 0.8793\n",
      "Epoch: 117\tTrain Loss: 0.8796\tVal Loss: 0.8747\n",
      "Epoch: 118\tTrain Loss: 0.8750\tVal Loss: 0.8695\n",
      "Epoch: 119\tTrain Loss: 0.8698\tVal Loss: 0.8651\n",
      "Epoch: 120\tTrain Loss: 0.8654\tVal Loss: 0.8600\n",
      "Epoch: 121\tTrain Loss: 0.8603\tVal Loss: 0.8557\n",
      "Epoch: 122\tTrain Loss: 0.8560\tVal Loss: 0.8506\n",
      "Epoch: 123\tTrain Loss: 0.8509\tVal Loss: 0.8465\n",
      "Epoch: 124\tTrain Loss: 0.8468\tVal Loss: 0.8415\n",
      "Epoch: 125\tTrain Loss: 0.8417\tVal Loss: 0.8374\n",
      "Epoch: 126\tTrain Loss: 0.8377\tVal Loss: 0.8324\n",
      "Epoch: 127\tTrain Loss: 0.8327\tVal Loss: 0.8285\n",
      "Epoch: 128\tTrain Loss: 0.8288\tVal Loss: 0.8236\n",
      "Epoch: 129\tTrain Loss: 0.8239\tVal Loss: 0.8198\n",
      "Epoch: 130\tTrain Loss: 0.8201\tVal Loss: 0.8150\n",
      "Epoch: 131\tTrain Loss: 0.8153\tVal Loss: 0.8113\n",
      "Epoch: 132\tTrain Loss: 0.8116\tVal Loss: 0.8067\n",
      "Epoch: 133\tTrain Loss: 0.8070\tVal Loss: 0.8031\n",
      "Epoch: 134\tTrain Loss: 0.8034\tVal Loss: 0.7987\n",
      "Epoch: 135\tTrain Loss: 0.7990\tVal Loss: 0.7952\n",
      "Epoch: 136\tTrain Loss: 0.7954\tVal Loss: 0.7910\n",
      "Epoch: 137\tTrain Loss: 0.7913\tVal Loss: 0.7876\n",
      "Epoch: 138\tTrain Loss: 0.7878\tVal Loss: 0.7836\n",
      "Epoch: 139\tTrain Loss: 0.7839\tVal Loss: 0.7803\n",
      "Epoch: 140\tTrain Loss: 0.7805\tVal Loss: 0.7765\n",
      "Epoch: 141\tTrain Loss: 0.7768\tVal Loss: 0.7733\n",
      "Epoch: 142\tTrain Loss: 0.7736\tVal Loss: 0.7697\n",
      "Epoch: 143\tTrain Loss: 0.7700\tVal Loss: 0.7666\n",
      "Epoch: 144\tTrain Loss: 0.7669\tVal Loss: 0.7632\n",
      "Epoch: 145\tTrain Loss: 0.7635\tVal Loss: 0.7602\n",
      "Epoch: 146\tTrain Loss: 0.7605\tVal Loss: 0.7570\n",
      "Epoch: 147\tTrain Loss: 0.7572\tVal Loss: 0.7541\n",
      "Epoch: 148\tTrain Loss: 0.7543\tVal Loss: 0.7510\n",
      "Epoch: 149\tTrain Loss: 0.7512\tVal Loss: 0.7482\n",
      "Epoch: 150\tTrain Loss: 0.7484\tVal Loss: 0.7452\n",
      "Epoch: 151\tTrain Loss: 0.7455\tVal Loss: 0.7425\n",
      "Epoch: 152\tTrain Loss: 0.7428\tVal Loss: 0.7397\n",
      "Epoch: 153\tTrain Loss: 0.7399\tVal Loss: 0.7371\n",
      "Epoch: 154\tTrain Loss: 0.7373\tVal Loss: 0.7344\n",
      "Epoch: 155\tTrain Loss: 0.7346\tVal Loss: 0.7318\n",
      "Epoch: 156\tTrain Loss: 0.7320\tVal Loss: 0.7292\n",
      "Epoch: 157\tTrain Loss: 0.7294\tVal Loss: 0.7267\n",
      "Epoch: 158\tTrain Loss: 0.7270\tVal Loss: 0.7242\n",
      "Epoch: 159\tTrain Loss: 0.7245\tVal Loss: 0.7218\n",
      "Epoch: 160\tTrain Loss: 0.7221\tVal Loss: 0.7194\n",
      "Epoch: 161\tTrain Loss: 0.7196\tVal Loss: 0.7171\n",
      "Epoch: 162\tTrain Loss: 0.7173\tVal Loss: 0.7147\n",
      "Epoch: 163\tTrain Loss: 0.7150\tVal Loss: 0.7125\n",
      "Epoch: 164\tTrain Loss: 0.7127\tVal Loss: 0.7102\n",
      "Epoch: 165\tTrain Loss: 0.7104\tVal Loss: 0.7080\n",
      "Epoch: 166\tTrain Loss: 0.7082\tVal Loss: 0.7058\n",
      "Epoch: 167\tTrain Loss: 0.7060\tVal Loss: 0.7037\n",
      "Epoch: 168\tTrain Loss: 0.7039\tVal Loss: 0.7015\n",
      "Epoch: 169\tTrain Loss: 0.7018\tVal Loss: 0.6995\n",
      "Epoch: 170\tTrain Loss: 0.6997\tVal Loss: 0.6974\n",
      "Epoch: 171\tTrain Loss: 0.6976\tVal Loss: 0.6954\n",
      "Epoch: 172\tTrain Loss: 0.6956\tVal Loss: 0.6934\n",
      "Epoch: 173\tTrain Loss: 0.6936\tVal Loss: 0.6914\n",
      "Epoch: 174\tTrain Loss: 0.6916\tVal Loss: 0.6895\n",
      "Epoch: 175\tTrain Loss: 0.6897\tVal Loss: 0.6875\n",
      "Epoch: 176\tTrain Loss: 0.6878\tVal Loss: 0.6856\n",
      "Epoch: 177\tTrain Loss: 0.6859\tVal Loss: 0.6838\n",
      "Epoch: 178\tTrain Loss: 0.6840\tVal Loss: 0.6819\n",
      "Epoch: 179\tTrain Loss: 0.6821\tVal Loss: 0.6801\n",
      "Epoch: 180\tTrain Loss: 0.6803\tVal Loss: 0.6783\n",
      "Epoch: 181\tTrain Loss: 0.6785\tVal Loss: 0.6765\n",
      "Epoch: 182\tTrain Loss: 0.6767\tVal Loss: 0.6748\n",
      "Epoch: 183\tTrain Loss: 0.6750\tVal Loss: 0.6730\n",
      "Epoch: 184\tTrain Loss: 0.6732\tVal Loss: 0.6713\n",
      "Epoch: 185\tTrain Loss: 0.6715\tVal Loss: 0.6696\n",
      "Epoch: 186\tTrain Loss: 0.6698\tVal Loss: 0.6680\n",
      "Epoch: 187\tTrain Loss: 0.6682\tVal Loss: 0.6663\n",
      "Epoch: 188\tTrain Loss: 0.6665\tVal Loss: 0.6647\n",
      "Epoch: 189\tTrain Loss: 0.6649\tVal Loss: 0.6631\n",
      "Epoch: 190\tTrain Loss: 0.6633\tVal Loss: 0.6615\n",
      "Epoch: 191\tTrain Loss: 0.6617\tVal Loss: 0.6599\n",
      "Epoch: 192\tTrain Loss: 0.6601\tVal Loss: 0.6584\n",
      "Epoch: 193\tTrain Loss: 0.6586\tVal Loss: 0.6568\n",
      "Epoch: 194\tTrain Loss: 0.6570\tVal Loss: 0.6553\n",
      "Epoch: 195\tTrain Loss: 0.6555\tVal Loss: 0.6538\n",
      "Epoch: 196\tTrain Loss: 0.6540\tVal Loss: 0.6523\n",
      "Epoch: 197\tTrain Loss: 0.6525\tVal Loss: 0.6509\n",
      "Epoch: 198\tTrain Loss: 0.6510\tVal Loss: 0.6494\n",
      "Epoch: 199\tTrain Loss: 0.6496\tVal Loss: 0.6480\n",
      "Epoch: 200\tTrain Loss: 0.6482\tVal Loss: 0.6465\n",
      "Epoch: 201\tTrain Loss: 0.6467\tVal Loss: 0.6451\n",
      "Epoch: 202\tTrain Loss: 0.6453\tVal Loss: 0.6438\n",
      "Epoch: 203\tTrain Loss: 0.6439\tVal Loss: 0.6424\n",
      "Epoch: 204\tTrain Loss: 0.6426\tVal Loss: 0.6410\n",
      "Epoch: 205\tTrain Loss: 0.6412\tVal Loss: 0.6397\n",
      "Epoch: 206\tTrain Loss: 0.6399\tVal Loss: 0.6383\n",
      "Epoch: 207\tTrain Loss: 0.6385\tVal Loss: 0.6370\n",
      "Epoch: 208\tTrain Loss: 0.6372\tVal Loss: 0.6357\n",
      "Epoch: 209\tTrain Loss: 0.6359\tVal Loss: 0.6344\n",
      "Epoch: 210\tTrain Loss: 0.6346\tVal Loss: 0.6331\n",
      "Epoch: 211\tTrain Loss: 0.6333\tVal Loss: 0.6319\n",
      "Epoch: 212\tTrain Loss: 0.6321\tVal Loss: 0.6306\n",
      "Epoch: 213\tTrain Loss: 0.6308\tVal Loss: 0.6294\n",
      "Epoch: 214\tTrain Loss: 0.6296\tVal Loss: 0.6282\n",
      "Epoch: 215\tTrain Loss: 0.6283\tVal Loss: 0.6269\n",
      "Epoch: 216\tTrain Loss: 0.6271\tVal Loss: 0.6257\n",
      "Epoch: 217\tTrain Loss: 0.6259\tVal Loss: 0.6245\n",
      "Epoch: 218\tTrain Loss: 0.6247\tVal Loss: 0.6234\n",
      "Epoch: 219\tTrain Loss: 0.6235\tVal Loss: 0.6222\n",
      "Epoch: 220\tTrain Loss: 0.6224\tVal Loss: 0.6210\n",
      "Epoch: 221\tTrain Loss: 0.6212\tVal Loss: 0.6199\n",
      "Epoch: 222\tTrain Loss: 0.6201\tVal Loss: 0.6188\n",
      "Epoch: 223\tTrain Loss: 0.6189\tVal Loss: 0.6176\n",
      "Epoch: 224\tTrain Loss: 0.6178\tVal Loss: 0.6165\n",
      "Epoch: 225\tTrain Loss: 0.6167\tVal Loss: 0.6154\n",
      "Epoch: 226\tTrain Loss: 0.6156\tVal Loss: 0.6143\n",
      "Epoch: 227\tTrain Loss: 0.6145\tVal Loss: 0.6132\n",
      "Epoch: 228\tTrain Loss: 0.6134\tVal Loss: 0.6122\n",
      "Epoch: 229\tTrain Loss: 0.6123\tVal Loss: 0.6111\n",
      "Epoch: 230\tTrain Loss: 0.6113\tVal Loss: 0.6100\n",
      "Epoch: 231\tTrain Loss: 0.6102\tVal Loss: 0.6090\n",
      "Epoch: 232\tTrain Loss: 0.6092\tVal Loss: 0.6079\n",
      "Epoch: 233\tTrain Loss: 0.6081\tVal Loss: 0.6069\n",
      "Epoch: 234\tTrain Loss: 0.6071\tVal Loss: 0.6059\n",
      "Epoch: 235\tTrain Loss: 0.6061\tVal Loss: 0.6049\n",
      "Epoch: 236\tTrain Loss: 0.6051\tVal Loss: 0.6039\n",
      "Epoch: 237\tTrain Loss: 0.6041\tVal Loss: 0.6029\n",
      "Epoch: 238\tTrain Loss: 0.6031\tVal Loss: 0.6019\n",
      "Epoch: 239\tTrain Loss: 0.6021\tVal Loss: 0.6009\n",
      "Epoch: 240\tTrain Loss: 0.6011\tVal Loss: 0.6000\n",
      "Epoch: 241\tTrain Loss: 0.6001\tVal Loss: 0.5990\n",
      "Epoch: 242\tTrain Loss: 0.5992\tVal Loss: 0.5981\n",
      "Epoch: 243\tTrain Loss: 0.5982\tVal Loss: 0.5971\n",
      "Epoch: 244\tTrain Loss: 0.5973\tVal Loss: 0.5962\n",
      "Epoch: 245\tTrain Loss: 0.5963\tVal Loss: 0.5952\n",
      "Epoch: 246\tTrain Loss: 0.5954\tVal Loss: 0.5943\n",
      "Epoch: 247\tTrain Loss: 0.5945\tVal Loss: 0.5934\n",
      "Epoch: 248\tTrain Loss: 0.5936\tVal Loss: 0.5925\n",
      "Epoch: 249\tTrain Loss: 0.5927\tVal Loss: 0.5916\n",
      "Epoch: 250\tTrain Loss: 0.5918\tVal Loss: 0.5907\n",
      "Epoch: 251\tTrain Loss: 0.5909\tVal Loss: 0.5898\n",
      "Epoch: 252\tTrain Loss: 0.5900\tVal Loss: 0.5889\n",
      "Epoch: 253\tTrain Loss: 0.5891\tVal Loss: 0.5881\n",
      "Epoch: 254\tTrain Loss: 0.5882\tVal Loss: 0.5872\n",
      "Epoch: 255\tTrain Loss: 0.5874\tVal Loss: 0.5864\n",
      "Epoch: 256\tTrain Loss: 0.5865\tVal Loss: 0.5855\n",
      "Epoch: 257\tTrain Loss: 0.5857\tVal Loss: 0.5847\n",
      "Epoch: 258\tTrain Loss: 0.5848\tVal Loss: 0.5838\n",
      "Epoch: 259\tTrain Loss: 0.5840\tVal Loss: 0.5830\n",
      "Epoch: 260\tTrain Loss: 0.5831\tVal Loss: 0.5822\n",
      "Epoch: 261\tTrain Loss: 0.5823\tVal Loss: 0.5813\n",
      "Epoch: 262\tTrain Loss: 0.5815\tVal Loss: 0.5805\n",
      "Epoch: 263\tTrain Loss: 0.5807\tVal Loss: 0.5797\n",
      "Epoch: 264\tTrain Loss: 0.5799\tVal Loss: 0.5789\n",
      "Epoch: 265\tTrain Loss: 0.5791\tVal Loss: 0.5781\n",
      "Epoch: 266\tTrain Loss: 0.5783\tVal Loss: 0.5773\n",
      "Epoch: 267\tTrain Loss: 0.5775\tVal Loss: 0.5765\n",
      "Epoch: 268\tTrain Loss: 0.5767\tVal Loss: 0.5758\n",
      "Epoch: 269\tTrain Loss: 0.5759\tVal Loss: 0.5750\n",
      "Epoch: 270\tTrain Loss: 0.5751\tVal Loss: 0.5742\n",
      "Epoch: 271\tTrain Loss: 0.5744\tVal Loss: 0.5735\n",
      "Epoch: 272\tTrain Loss: 0.5736\tVal Loss: 0.5727\n",
      "Epoch: 273\tTrain Loss: 0.5729\tVal Loss: 0.5720\n",
      "Epoch: 274\tTrain Loss: 0.5721\tVal Loss: 0.5712\n",
      "Epoch: 275\tTrain Loss: 0.5714\tVal Loss: 0.5705\n",
      "Epoch: 276\tTrain Loss: 0.5706\tVal Loss: 0.5697\n",
      "Epoch: 277\tTrain Loss: 0.5699\tVal Loss: 0.5690\n",
      "Epoch: 278\tTrain Loss: 0.5692\tVal Loss: 0.5683\n",
      "Epoch: 279\tTrain Loss: 0.5684\tVal Loss: 0.5676\n",
      "Epoch: 280\tTrain Loss: 0.5677\tVal Loss: 0.5669\n",
      "Epoch: 281\tTrain Loss: 0.5670\tVal Loss: 0.5661\n",
      "Epoch: 282\tTrain Loss: 0.5663\tVal Loss: 0.5654\n",
      "Epoch: 283\tTrain Loss: 0.5656\tVal Loss: 0.5647\n",
      "Epoch: 284\tTrain Loss: 0.5649\tVal Loss: 0.5640\n",
      "Epoch: 285\tTrain Loss: 0.5642\tVal Loss: 0.5634\n",
      "Epoch: 286\tTrain Loss: 0.5635\tVal Loss: 0.5627\n",
      "Epoch: 287\tTrain Loss: 0.5628\tVal Loss: 0.5620\n",
      "Epoch: 288\tTrain Loss: 0.5621\tVal Loss: 0.5613\n",
      "Epoch: 289\tTrain Loss: 0.5614\tVal Loss: 0.5606\n",
      "Epoch: 290\tTrain Loss: 0.5608\tVal Loss: 0.5600\n",
      "Epoch: 291\tTrain Loss: 0.5601\tVal Loss: 0.5593\n",
      "Epoch: 292\tTrain Loss: 0.5594\tVal Loss: 0.5586\n",
      "Epoch: 293\tTrain Loss: 0.5588\tVal Loss: 0.5580\n",
      "Epoch: 294\tTrain Loss: 0.5581\tVal Loss: 0.5573\n",
      "Epoch: 295\tTrain Loss: 0.5575\tVal Loss: 0.5567\n",
      "Epoch: 296\tTrain Loss: 0.5568\tVal Loss: 0.5561\n",
      "Epoch: 297\tTrain Loss: 0.5562\tVal Loss: 0.5554\n",
      "Epoch: 298\tTrain Loss: 0.5555\tVal Loss: 0.5548\n",
      "Epoch: 299\tTrain Loss: 0.5549\tVal Loss: 0.5541\n",
      "Epoch: 300\tTrain Loss: 0.5543\tVal Loss: 0.5535\n",
      "Epoch: 301\tTrain Loss: 0.5537\tVal Loss: 0.5529\n",
      "Epoch: 302\tTrain Loss: 0.5530\tVal Loss: 0.5523\n",
      "Epoch: 303\tTrain Loss: 0.5524\tVal Loss: 0.5517\n",
      "Epoch: 304\tTrain Loss: 0.5518\tVal Loss: 0.5511\n",
      "Epoch: 305\tTrain Loss: 0.5512\tVal Loss: 0.5504\n",
      "Epoch: 306\tTrain Loss: 0.5506\tVal Loss: 0.5498\n",
      "Epoch: 307\tTrain Loss: 0.5500\tVal Loss: 0.5492\n",
      "Epoch: 308\tTrain Loss: 0.5494\tVal Loss: 0.5486\n",
      "Epoch: 309\tTrain Loss: 0.5488\tVal Loss: 0.5481\n",
      "Epoch: 310\tTrain Loss: 0.5482\tVal Loss: 0.5475\n",
      "Epoch: 311\tTrain Loss: 0.5476\tVal Loss: 0.5469\n",
      "Epoch: 312\tTrain Loss: 0.5470\tVal Loss: 0.5463\n",
      "Epoch: 313\tTrain Loss: 0.5464\tVal Loss: 0.5457\n",
      "Epoch: 314\tTrain Loss: 0.5459\tVal Loss: 0.5451\n",
      "Epoch: 315\tTrain Loss: 0.5453\tVal Loss: 0.5446\n",
      "Epoch: 316\tTrain Loss: 0.5447\tVal Loss: 0.5440\n",
      "Epoch: 317\tTrain Loss: 0.5441\tVal Loss: 0.5434\n",
      "Epoch: 318\tTrain Loss: 0.5436\tVal Loss: 0.5429\n",
      "Epoch: 319\tTrain Loss: 0.5430\tVal Loss: 0.5423\n",
      "Epoch: 320\tTrain Loss: 0.5425\tVal Loss: 0.5418\n",
      "Epoch: 321\tTrain Loss: 0.5419\tVal Loss: 0.5412\n",
      "Epoch: 322\tTrain Loss: 0.5413\tVal Loss: 0.5407\n",
      "Epoch: 323\tTrain Loss: 0.5408\tVal Loss: 0.5401\n",
      "Epoch: 324\tTrain Loss: 0.5403\tVal Loss: 0.5396\n",
      "Epoch: 325\tTrain Loss: 0.5397\tVal Loss: 0.5390\n",
      "Epoch: 326\tTrain Loss: 0.5392\tVal Loss: 0.5385\n",
      "Epoch: 327\tTrain Loss: 0.5386\tVal Loss: 0.5380\n",
      "Epoch: 328\tTrain Loss: 0.5381\tVal Loss: 0.5374\n",
      "Epoch: 329\tTrain Loss: 0.5376\tVal Loss: 0.5369\n",
      "Epoch: 330\tTrain Loss: 0.5370\tVal Loss: 0.5364\n",
      "Epoch: 331\tTrain Loss: 0.5365\tVal Loss: 0.5359\n",
      "Epoch: 332\tTrain Loss: 0.5360\tVal Loss: 0.5354\n",
      "Epoch: 333\tTrain Loss: 0.5355\tVal Loss: 0.5348\n",
      "Epoch: 334\tTrain Loss: 0.5350\tVal Loss: 0.5343\n",
      "Epoch: 335\tTrain Loss: 0.5345\tVal Loss: 0.5338\n",
      "Epoch: 336\tTrain Loss: 0.5339\tVal Loss: 0.5333\n",
      "Epoch: 337\tTrain Loss: 0.5334\tVal Loss: 0.5328\n",
      "Epoch: 338\tTrain Loss: 0.5329\tVal Loss: 0.5323\n",
      "Epoch: 339\tTrain Loss: 0.5324\tVal Loss: 0.5318\n",
      "Epoch: 340\tTrain Loss: 0.5319\tVal Loss: 0.5313\n",
      "Epoch: 341\tTrain Loss: 0.5314\tVal Loss: 0.5308\n",
      "Epoch: 342\tTrain Loss: 0.5309\tVal Loss: 0.5303\n",
      "Epoch: 343\tTrain Loss: 0.5305\tVal Loss: 0.5298\n",
      "Epoch: 344\tTrain Loss: 0.5300\tVal Loss: 0.5294\n",
      "Epoch: 345\tTrain Loss: 0.5295\tVal Loss: 0.5289\n",
      "Epoch: 346\tTrain Loss: 0.5290\tVal Loss: 0.5284\n",
      "Epoch: 347\tTrain Loss: 0.5285\tVal Loss: 0.5279\n",
      "Epoch: 348\tTrain Loss: 0.5280\tVal Loss: 0.5274\n",
      "Epoch: 349\tTrain Loss: 0.5276\tVal Loss: 0.5270\n",
      "Epoch: 350\tTrain Loss: 0.5271\tVal Loss: 0.5265\n",
      "Epoch: 351\tTrain Loss: 0.5266\tVal Loss: 0.5260\n",
      "Epoch: 352\tTrain Loss: 0.5261\tVal Loss: 0.5256\n",
      "Epoch: 353\tTrain Loss: 0.5257\tVal Loss: 0.5251\n",
      "Epoch: 354\tTrain Loss: 0.5252\tVal Loss: 0.5246\n",
      "Epoch: 355\tTrain Loss: 0.5248\tVal Loss: 0.5242\n",
      "Epoch: 356\tTrain Loss: 0.5243\tVal Loss: 0.5237\n",
      "Epoch: 357\tTrain Loss: 0.5238\tVal Loss: 0.5233\n",
      "Epoch: 358\tTrain Loss: 0.5234\tVal Loss: 0.5228\n",
      "Epoch: 359\tTrain Loss: 0.5229\tVal Loss: 0.5224\n",
      "Epoch: 360\tTrain Loss: 0.5225\tVal Loss: 0.5219\n",
      "Epoch: 361\tTrain Loss: 0.5220\tVal Loss: 0.5215\n",
      "Epoch: 362\tTrain Loss: 0.5216\tVal Loss: 0.5210\n",
      "Epoch: 363\tTrain Loss: 0.5212\tVal Loss: 0.5206\n",
      "Epoch: 364\tTrain Loss: 0.5207\tVal Loss: 0.5202\n",
      "Epoch: 365\tTrain Loss: 0.5203\tVal Loss: 0.5197\n",
      "Epoch: 366\tTrain Loss: 0.5198\tVal Loss: 0.5193\n",
      "Epoch: 367\tTrain Loss: 0.5194\tVal Loss: 0.5189\n",
      "Epoch: 368\tTrain Loss: 0.5190\tVal Loss: 0.5184\n",
      "Epoch: 369\tTrain Loss: 0.5185\tVal Loss: 0.5180\n",
      "Epoch: 370\tTrain Loss: 0.5181\tVal Loss: 0.5176\n",
      "Epoch: 371\tTrain Loss: 0.5177\tVal Loss: 0.5171\n",
      "Epoch: 372\tTrain Loss: 0.5173\tVal Loss: 0.5167\n",
      "Epoch: 373\tTrain Loss: 0.5168\tVal Loss: 0.5163\n",
      "Epoch: 374\tTrain Loss: 0.5164\tVal Loss: 0.5159\n",
      "Epoch: 375\tTrain Loss: 0.5160\tVal Loss: 0.5155\n",
      "Epoch: 376\tTrain Loss: 0.5156\tVal Loss: 0.5151\n",
      "Epoch: 377\tTrain Loss: 0.5152\tVal Loss: 0.5146\n",
      "Epoch: 378\tTrain Loss: 0.5148\tVal Loss: 0.5142\n",
      "Epoch: 379\tTrain Loss: 0.5143\tVal Loss: 0.5138\n",
      "Epoch: 380\tTrain Loss: 0.5139\tVal Loss: 0.5134\n",
      "Epoch: 381\tTrain Loss: 0.5135\tVal Loss: 0.5130\n",
      "Epoch: 382\tTrain Loss: 0.5131\tVal Loss: 0.5126\n",
      "Epoch: 383\tTrain Loss: 0.5127\tVal Loss: 0.5122\n",
      "Epoch: 384\tTrain Loss: 0.5123\tVal Loss: 0.5118\n",
      "Epoch: 385\tTrain Loss: 0.5119\tVal Loss: 0.5114\n",
      "Epoch: 386\tTrain Loss: 0.5115\tVal Loss: 0.5110\n",
      "Epoch: 387\tTrain Loss: 0.5111\tVal Loss: 0.5106\n",
      "Epoch: 388\tTrain Loss: 0.5107\tVal Loss: 0.5102\n",
      "Epoch: 389\tTrain Loss: 0.5103\tVal Loss: 0.5099\n",
      "Epoch: 390\tTrain Loss: 0.5100\tVal Loss: 0.5095\n",
      "Epoch: 391\tTrain Loss: 0.5096\tVal Loss: 0.5091\n",
      "Epoch: 392\tTrain Loss: 0.5092\tVal Loss: 0.5087\n",
      "Epoch: 393\tTrain Loss: 0.5088\tVal Loss: 0.5083\n",
      "Epoch: 394\tTrain Loss: 0.5084\tVal Loss: 0.5079\n",
      "Epoch: 395\tTrain Loss: 0.5080\tVal Loss: 0.5075\n",
      "Epoch: 396\tTrain Loss: 0.5077\tVal Loss: 0.5072\n",
      "Epoch: 397\tTrain Loss: 0.5073\tVal Loss: 0.5068\n",
      "Epoch: 398\tTrain Loss: 0.5069\tVal Loss: 0.5064\n",
      "Epoch: 399\tTrain Loss: 0.5065\tVal Loss: 0.5060\n",
      "Epoch: 400\tTrain Loss: 0.5061\tVal Loss: 0.5057\n",
      "Epoch: 401\tTrain Loss: 0.5058\tVal Loss: 0.5053\n",
      "Epoch: 402\tTrain Loss: 0.5054\tVal Loss: 0.5049\n",
      "Epoch: 403\tTrain Loss: 0.5050\tVal Loss: 0.5046\n",
      "Epoch: 404\tTrain Loss: 0.5047\tVal Loss: 0.5042\n",
      "Epoch: 405\tTrain Loss: 0.5043\tVal Loss: 0.5038\n",
      "Epoch: 406\tTrain Loss: 0.5039\tVal Loss: 0.5035\n",
      "Epoch: 407\tTrain Loss: 0.5036\tVal Loss: 0.5031\n",
      "Epoch: 408\tTrain Loss: 0.5032\tVal Loss: 0.5028\n",
      "Epoch: 409\tTrain Loss: 0.5029\tVal Loss: 0.5024\n",
      "Epoch: 410\tTrain Loss: 0.5025\tVal Loss: 0.5020\n",
      "Epoch: 411\tTrain Loss: 0.5021\tVal Loss: 0.5017\n",
      "Epoch: 412\tTrain Loss: 0.5018\tVal Loss: 0.5013\n",
      "Epoch: 413\tTrain Loss: 0.5014\tVal Loss: 0.5010\n",
      "Epoch: 414\tTrain Loss: 0.5011\tVal Loss: 0.5006\n",
      "Epoch: 415\tTrain Loss: 0.5007\tVal Loss: 0.5003\n",
      "Epoch: 416\tTrain Loss: 0.5004\tVal Loss: 0.4999\n",
      "Epoch: 417\tTrain Loss: 0.5000\tVal Loss: 0.4996\n",
      "Epoch: 418\tTrain Loss: 0.4997\tVal Loss: 0.4992\n",
      "Epoch: 419\tTrain Loss: 0.4993\tVal Loss: 0.4989\n",
      "Epoch: 420\tTrain Loss: 0.4990\tVal Loss: 0.4986\n",
      "Epoch: 421\tTrain Loss: 0.4987\tVal Loss: 0.4982\n",
      "Epoch: 422\tTrain Loss: 0.4983\tVal Loss: 0.4979\n",
      "Epoch: 423\tTrain Loss: 0.4980\tVal Loss: 0.4975\n",
      "Epoch: 424\tTrain Loss: 0.4976\tVal Loss: 0.4972\n",
      "Epoch: 425\tTrain Loss: 0.4973\tVal Loss: 0.4969\n",
      "Epoch: 426\tTrain Loss: 0.4970\tVal Loss: 0.4965\n",
      "Epoch: 427\tTrain Loss: 0.4966\tVal Loss: 0.4962\n",
      "Epoch: 428\tTrain Loss: 0.4963\tVal Loss: 0.4959\n",
      "Epoch: 429\tTrain Loss: 0.4960\tVal Loss: 0.4955\n",
      "Epoch: 430\tTrain Loss: 0.4956\tVal Loss: 0.4952\n",
      "Epoch: 431\tTrain Loss: 0.4953\tVal Loss: 0.4949\n",
      "Epoch: 432\tTrain Loss: 0.4950\tVal Loss: 0.4946\n",
      "Epoch: 433\tTrain Loss: 0.4947\tVal Loss: 0.4942\n",
      "Epoch: 434\tTrain Loss: 0.4943\tVal Loss: 0.4939\n",
      "Epoch: 435\tTrain Loss: 0.4940\tVal Loss: 0.4936\n",
      "Epoch: 436\tTrain Loss: 0.4937\tVal Loss: 0.4933\n",
      "Epoch: 437\tTrain Loss: 0.4934\tVal Loss: 0.4930\n",
      "Epoch: 438\tTrain Loss: 0.4930\tVal Loss: 0.4926\n",
      "Epoch: 439\tTrain Loss: 0.4927\tVal Loss: 0.4923\n",
      "Epoch: 440\tTrain Loss: 0.4924\tVal Loss: 0.4920\n",
      "Epoch: 441\tTrain Loss: 0.4921\tVal Loss: 0.4917\n",
      "Epoch: 442\tTrain Loss: 0.4918\tVal Loss: 0.4914\n",
      "Epoch: 443\tTrain Loss: 0.4915\tVal Loss: 0.4911\n",
      "Epoch: 444\tTrain Loss: 0.4912\tVal Loss: 0.4907\n",
      "Epoch: 445\tTrain Loss: 0.4908\tVal Loss: 0.4904\n",
      "Epoch: 446\tTrain Loss: 0.4905\tVal Loss: 0.4901\n",
      "Epoch: 447\tTrain Loss: 0.4902\tVal Loss: 0.4898\n",
      "Epoch: 448\tTrain Loss: 0.4899\tVal Loss: 0.4895\n",
      "Epoch: 449\tTrain Loss: 0.4896\tVal Loss: 0.4892\n",
      "Epoch: 450\tTrain Loss: 0.4893\tVal Loss: 0.4889\n",
      "Epoch: 451\tTrain Loss: 0.4890\tVal Loss: 0.4886\n",
      "Epoch: 452\tTrain Loss: 0.4887\tVal Loss: 0.4883\n",
      "Epoch: 453\tTrain Loss: 0.4884\tVal Loss: 0.4880\n",
      "Epoch: 454\tTrain Loss: 0.4881\tVal Loss: 0.4877\n",
      "Epoch: 455\tTrain Loss: 0.4878\tVal Loss: 0.4874\n",
      "Epoch: 456\tTrain Loss: 0.4875\tVal Loss: 0.4871\n",
      "Epoch: 457\tTrain Loss: 0.4872\tVal Loss: 0.4868\n",
      "Epoch: 458\tTrain Loss: 0.4869\tVal Loss: 0.4865\n",
      "Epoch: 459\tTrain Loss: 0.4866\tVal Loss: 0.4862\n",
      "Epoch: 460\tTrain Loss: 0.4863\tVal Loss: 0.4859\n",
      "Epoch: 461\tTrain Loss: 0.4860\tVal Loss: 0.4856\n",
      "Epoch: 462\tTrain Loss: 0.4857\tVal Loss: 0.4853\n",
      "Epoch: 463\tTrain Loss: 0.4854\tVal Loss: 0.4850\n",
      "Epoch: 464\tTrain Loss: 0.4851\tVal Loss: 0.4848\n",
      "Epoch: 465\tTrain Loss: 0.4849\tVal Loss: 0.4845\n",
      "Epoch: 466\tTrain Loss: 0.4846\tVal Loss: 0.4842\n",
      "Epoch: 467\tTrain Loss: 0.4843\tVal Loss: 0.4839\n",
      "Epoch: 468\tTrain Loss: 0.4840\tVal Loss: 0.4836\n",
      "Epoch: 469\tTrain Loss: 0.4837\tVal Loss: 0.4833\n",
      "Epoch: 470\tTrain Loss: 0.4834\tVal Loss: 0.4830\n",
      "Epoch: 471\tTrain Loss: 0.4831\tVal Loss: 0.4828\n",
      "Epoch: 472\tTrain Loss: 0.4829\tVal Loss: 0.4825\n",
      "Epoch: 473\tTrain Loss: 0.4826\tVal Loss: 0.4822\n",
      "Epoch: 474\tTrain Loss: 0.4823\tVal Loss: 0.4819\n",
      "Epoch: 475\tTrain Loss: 0.4820\tVal Loss: 0.4816\n",
      "Epoch: 476\tTrain Loss: 0.4817\tVal Loss: 0.4814\n",
      "Epoch: 477\tTrain Loss: 0.4815\tVal Loss: 0.4811\n",
      "Epoch: 478\tTrain Loss: 0.4812\tVal Loss: 0.4808\n",
      "Epoch: 479\tTrain Loss: 0.4809\tVal Loss: 0.4805\n",
      "Epoch: 480\tTrain Loss: 0.4806\tVal Loss: 0.4803\n",
      "Epoch: 481\tTrain Loss: 0.4804\tVal Loss: 0.4800\n",
      "Epoch: 482\tTrain Loss: 0.4801\tVal Loss: 0.4797\n",
      "Epoch: 483\tTrain Loss: 0.4798\tVal Loss: 0.4795\n",
      "Epoch: 484\tTrain Loss: 0.4795\tVal Loss: 0.4792\n",
      "Epoch: 485\tTrain Loss: 0.4793\tVal Loss: 0.4789\n",
      "Epoch: 486\tTrain Loss: 0.4790\tVal Loss: 0.4786\n",
      "Epoch: 487\tTrain Loss: 0.4787\tVal Loss: 0.4784\n",
      "Epoch: 488\tTrain Loss: 0.4785\tVal Loss: 0.4781\n",
      "Epoch: 489\tTrain Loss: 0.4782\tVal Loss: 0.4779\n",
      "Epoch: 490\tTrain Loss: 0.4779\tVal Loss: 0.4776\n",
      "Epoch: 491\tTrain Loss: 0.4777\tVal Loss: 0.4773\n",
      "Epoch: 492\tTrain Loss: 0.4774\tVal Loss: 0.4771\n",
      "Epoch: 493\tTrain Loss: 0.4772\tVal Loss: 0.4768\n",
      "Epoch: 494\tTrain Loss: 0.4769\tVal Loss: 0.4765\n",
      "Epoch: 495\tTrain Loss: 0.4766\tVal Loss: 0.4763\n",
      "Epoch: 496\tTrain Loss: 0.4764\tVal Loss: 0.4760\n",
      "Epoch: 497\tTrain Loss: 0.4761\tVal Loss: 0.4758\n",
      "Epoch: 498\tTrain Loss: 0.4759\tVal Loss: 0.4755\n",
      "Epoch: 499\tTrain Loss: 0.4756\tVal Loss: 0.4753\n",
      "Epoch: 500\tTrain Loss: 0.4753\tVal Loss: 0.4750\n",
      "Epoch: 501\tTrain Loss: 0.4751\tVal Loss: 0.4747\n",
      "Epoch: 502\tTrain Loss: 0.4748\tVal Loss: 0.4745\n",
      "Epoch: 503\tTrain Loss: 0.4746\tVal Loss: 0.4742\n",
      "Epoch: 504\tTrain Loss: 0.4743\tVal Loss: 0.4740\n",
      "Epoch: 505\tTrain Loss: 0.4741\tVal Loss: 0.4737\n",
      "Epoch: 506\tTrain Loss: 0.4738\tVal Loss: 0.4735\n",
      "Epoch: 507\tTrain Loss: 0.4736\tVal Loss: 0.4732\n",
      "Epoch: 508\tTrain Loss: 0.4733\tVal Loss: 0.4730\n",
      "Epoch: 509\tTrain Loss: 0.4731\tVal Loss: 0.4727\n",
      "Epoch: 510\tTrain Loss: 0.4728\tVal Loss: 0.4725\n",
      "Epoch: 511\tTrain Loss: 0.4726\tVal Loss: 0.4723\n",
      "Epoch: 512\tTrain Loss: 0.4723\tVal Loss: 0.4720\n",
      "Epoch: 513\tTrain Loss: 0.4721\tVal Loss: 0.4718\n",
      "Epoch: 514\tTrain Loss: 0.4718\tVal Loss: 0.4715\n",
      "Epoch: 515\tTrain Loss: 0.4716\tVal Loss: 0.4713\n",
      "Epoch: 516\tTrain Loss: 0.4714\tVal Loss: 0.4710\n",
      "Epoch: 517\tTrain Loss: 0.4711\tVal Loss: 0.4708\n",
      "Epoch: 518\tTrain Loss: 0.4709\tVal Loss: 0.4706\n",
      "Epoch: 519\tTrain Loss: 0.4706\tVal Loss: 0.4703\n",
      "Epoch: 520\tTrain Loss: 0.4704\tVal Loss: 0.4701\n",
      "Epoch: 521\tTrain Loss: 0.4702\tVal Loss: 0.4698\n",
      "Epoch: 522\tTrain Loss: 0.4699\tVal Loss: 0.4696\n",
      "Epoch: 523\tTrain Loss: 0.4697\tVal Loss: 0.4694\n",
      "Epoch: 524\tTrain Loss: 0.4695\tVal Loss: 0.4691\n",
      "Epoch: 525\tTrain Loss: 0.4692\tVal Loss: 0.4689\n",
      "Epoch: 526\tTrain Loss: 0.4690\tVal Loss: 0.4687\n",
      "Epoch: 527\tTrain Loss: 0.4687\tVal Loss: 0.4684\n",
      "Epoch: 528\tTrain Loss: 0.4685\tVal Loss: 0.4682\n",
      "Epoch: 529\tTrain Loss: 0.4683\tVal Loss: 0.4680\n",
      "Epoch: 530\tTrain Loss: 0.4680\tVal Loss: 0.4677\n",
      "Epoch: 531\tTrain Loss: 0.4678\tVal Loss: 0.4675\n",
      "Epoch: 532\tTrain Loss: 0.4676\tVal Loss: 0.4673\n",
      "Epoch: 533\tTrain Loss: 0.4674\tVal Loss: 0.4670\n",
      "Epoch: 534\tTrain Loss: 0.4671\tVal Loss: 0.4668\n",
      "Epoch: 535\tTrain Loss: 0.4669\tVal Loss: 0.4666\n",
      "Epoch: 536\tTrain Loss: 0.4667\tVal Loss: 0.4664\n",
      "Epoch: 537\tTrain Loss: 0.4664\tVal Loss: 0.4661\n",
      "Epoch: 538\tTrain Loss: 0.4662\tVal Loss: 0.4659\n",
      "Epoch: 539\tTrain Loss: 0.4660\tVal Loss: 0.4657\n",
      "Epoch: 540\tTrain Loss: 0.4658\tVal Loss: 0.4655\n",
      "Epoch: 541\tTrain Loss: 0.4655\tVal Loss: 0.4652\n",
      "Epoch: 542\tTrain Loss: 0.4653\tVal Loss: 0.4650\n",
      "Epoch: 543\tTrain Loss: 0.4651\tVal Loss: 0.4648\n",
      "Epoch: 544\tTrain Loss: 0.4649\tVal Loss: 0.4646\n",
      "Epoch: 545\tTrain Loss: 0.4647\tVal Loss: 0.4644\n",
      "Epoch: 546\tTrain Loss: 0.4644\tVal Loss: 0.4641\n",
      "Epoch: 547\tTrain Loss: 0.4642\tVal Loss: 0.4639\n",
      "Epoch: 548\tTrain Loss: 0.4640\tVal Loss: 0.4637\n",
      "Epoch: 549\tTrain Loss: 0.4638\tVal Loss: 0.4635\n",
      "Epoch: 550\tTrain Loss: 0.4636\tVal Loss: 0.4633\n",
      "Epoch: 551\tTrain Loss: 0.4633\tVal Loss: 0.4630\n",
      "Epoch: 552\tTrain Loss: 0.4631\tVal Loss: 0.4628\n",
      "Epoch: 553\tTrain Loss: 0.4629\tVal Loss: 0.4626\n",
      "Epoch: 554\tTrain Loss: 0.4627\tVal Loss: 0.4624\n",
      "Epoch: 555\tTrain Loss: 0.4625\tVal Loss: 0.4622\n",
      "Epoch: 556\tTrain Loss: 0.4623\tVal Loss: 0.4620\n",
      "Epoch: 557\tTrain Loss: 0.4621\tVal Loss: 0.4618\n",
      "Epoch: 558\tTrain Loss: 0.4618\tVal Loss: 0.4616\n",
      "Epoch: 559\tTrain Loss: 0.4616\tVal Loss: 0.4613\n",
      "Epoch: 560\tTrain Loss: 0.4614\tVal Loss: 0.4611\n",
      "Epoch: 561\tTrain Loss: 0.4612\tVal Loss: 0.4609\n",
      "Epoch: 562\tTrain Loss: 0.4610\tVal Loss: 0.4607\n",
      "Epoch: 563\tTrain Loss: 0.4608\tVal Loss: 0.4605\n",
      "Epoch: 564\tTrain Loss: 0.4606\tVal Loss: 0.4603\n",
      "Epoch: 565\tTrain Loss: 0.4604\tVal Loss: 0.4601\n",
      "Epoch: 566\tTrain Loss: 0.4602\tVal Loss: 0.4599\n",
      "Epoch: 567\tTrain Loss: 0.4600\tVal Loss: 0.4597\n",
      "Epoch: 568\tTrain Loss: 0.4598\tVal Loss: 0.4595\n",
      "Epoch: 569\tTrain Loss: 0.4595\tVal Loss: 0.4593\n",
      "Epoch: 570\tTrain Loss: 0.4593\tVal Loss: 0.4591\n",
      "Epoch: 571\tTrain Loss: 0.4591\tVal Loss: 0.4589\n",
      "Epoch: 572\tTrain Loss: 0.4589\tVal Loss: 0.4587\n",
      "Epoch: 573\tTrain Loss: 0.4587\tVal Loss: 0.4584\n",
      "Epoch: 574\tTrain Loss: 0.4585\tVal Loss: 0.4582\n",
      "Epoch: 575\tTrain Loss: 0.4583\tVal Loss: 0.4580\n",
      "Epoch: 576\tTrain Loss: 0.4581\tVal Loss: 0.4578\n",
      "Epoch: 577\tTrain Loss: 0.4579\tVal Loss: 0.4576\n",
      "Epoch: 578\tTrain Loss: 0.4577\tVal Loss: 0.4574\n",
      "Epoch: 579\tTrain Loss: 0.4575\tVal Loss: 0.4572\n",
      "Epoch: 580\tTrain Loss: 0.4573\tVal Loss: 0.4570\n",
      "Epoch: 581\tTrain Loss: 0.4571\tVal Loss: 0.4569\n",
      "Epoch: 582\tTrain Loss: 0.4569\tVal Loss: 0.4567\n",
      "Epoch: 583\tTrain Loss: 0.4567\tVal Loss: 0.4565\n",
      "Epoch: 584\tTrain Loss: 0.4565\tVal Loss: 0.4563\n",
      "Epoch: 585\tTrain Loss: 0.4563\tVal Loss: 0.4561\n",
      "Epoch: 586\tTrain Loss: 0.4561\tVal Loss: 0.4559\n",
      "Epoch: 587\tTrain Loss: 0.4559\tVal Loss: 0.4557\n",
      "Epoch: 588\tTrain Loss: 0.4558\tVal Loss: 0.4555\n",
      "Epoch: 589\tTrain Loss: 0.4556\tVal Loss: 0.4553\n",
      "Epoch: 590\tTrain Loss: 0.4554\tVal Loss: 0.4551\n",
      "Epoch: 591\tTrain Loss: 0.4552\tVal Loss: 0.4549\n",
      "Epoch: 592\tTrain Loss: 0.4550\tVal Loss: 0.4547\n",
      "Epoch: 593\tTrain Loss: 0.4548\tVal Loss: 0.4545\n",
      "Epoch: 594\tTrain Loss: 0.4546\tVal Loss: 0.4543\n",
      "Epoch: 595\tTrain Loss: 0.4544\tVal Loss: 0.4541\n",
      "Epoch: 596\tTrain Loss: 0.4542\tVal Loss: 0.4540\n",
      "Epoch: 597\tTrain Loss: 0.4540\tVal Loss: 0.4538\n",
      "Epoch: 598\tTrain Loss: 0.4538\tVal Loss: 0.4536\n",
      "Epoch: 599\tTrain Loss: 0.4536\tVal Loss: 0.4534\n",
      "Epoch: 600\tTrain Loss: 0.4535\tVal Loss: 0.4532\n",
      "Epoch: 601\tTrain Loss: 0.4533\tVal Loss: 0.4530\n",
      "Epoch: 602\tTrain Loss: 0.4531\tVal Loss: 0.4528\n",
      "Epoch: 603\tTrain Loss: 0.4529\tVal Loss: 0.4526\n",
      "Epoch: 604\tTrain Loss: 0.4527\tVal Loss: 0.4525\n",
      "Epoch: 605\tTrain Loss: 0.4525\tVal Loss: 0.4523\n",
      "Epoch: 606\tTrain Loss: 0.4523\tVal Loss: 0.4521\n",
      "Epoch: 607\tTrain Loss: 0.4522\tVal Loss: 0.4519\n",
      "Epoch: 608\tTrain Loss: 0.4520\tVal Loss: 0.4517\n",
      "Epoch: 609\tTrain Loss: 0.4518\tVal Loss: 0.4515\n",
      "Epoch: 610\tTrain Loss: 0.4516\tVal Loss: 0.4514\n",
      "Epoch: 611\tTrain Loss: 0.4514\tVal Loss: 0.4512\n",
      "Epoch: 612\tTrain Loss: 0.4512\tVal Loss: 0.4510\n",
      "Epoch: 613\tTrain Loss: 0.4511\tVal Loss: 0.4508\n",
      "Epoch: 614\tTrain Loss: 0.4509\tVal Loss: 0.4506\n",
      "Epoch: 615\tTrain Loss: 0.4507\tVal Loss: 0.4504\n",
      "Epoch: 616\tTrain Loss: 0.4505\tVal Loss: 0.4503\n",
      "Epoch: 617\tTrain Loss: 0.4503\tVal Loss: 0.4501\n",
      "Epoch: 618\tTrain Loss: 0.4502\tVal Loss: 0.4499\n",
      "Epoch: 619\tTrain Loss: 0.4500\tVal Loss: 0.4497\n",
      "Epoch: 620\tTrain Loss: 0.4498\tVal Loss: 0.4496\n",
      "Epoch: 621\tTrain Loss: 0.4496\tVal Loss: 0.4494\n",
      "Epoch: 622\tTrain Loss: 0.4495\tVal Loss: 0.4492\n",
      "Epoch: 623\tTrain Loss: 0.4493\tVal Loss: 0.4490\n",
      "Epoch: 624\tTrain Loss: 0.4491\tVal Loss: 0.4488\n",
      "Epoch: 625\tTrain Loss: 0.4489\tVal Loss: 0.4487\n",
      "Epoch: 626\tTrain Loss: 0.4487\tVal Loss: 0.4485\n",
      "Epoch: 627\tTrain Loss: 0.4486\tVal Loss: 0.4483\n",
      "Epoch: 628\tTrain Loss: 0.4484\tVal Loss: 0.4482\n",
      "Epoch: 629\tTrain Loss: 0.4482\tVal Loss: 0.4480\n",
      "Epoch: 630\tTrain Loss: 0.4481\tVal Loss: 0.4478\n",
      "Epoch: 631\tTrain Loss: 0.4479\tVal Loss: 0.4476\n",
      "Epoch: 632\tTrain Loss: 0.4477\tVal Loss: 0.4475\n",
      "Epoch: 633\tTrain Loss: 0.4475\tVal Loss: 0.4473\n",
      "Epoch: 634\tTrain Loss: 0.4474\tVal Loss: 0.4471\n",
      "Epoch: 635\tTrain Loss: 0.4472\tVal Loss: 0.4469\n",
      "Epoch: 636\tTrain Loss: 0.4470\tVal Loss: 0.4468\n",
      "Epoch: 637\tTrain Loss: 0.4469\tVal Loss: 0.4466\n",
      "Epoch: 638\tTrain Loss: 0.4467\tVal Loss: 0.4464\n",
      "Epoch: 639\tTrain Loss: 0.4465\tVal Loss: 0.4463\n",
      "Epoch: 640\tTrain Loss: 0.4463\tVal Loss: 0.4461\n",
      "Epoch: 641\tTrain Loss: 0.4462\tVal Loss: 0.4459\n",
      "Epoch: 642\tTrain Loss: 0.4460\tVal Loss: 0.4458\n",
      "Epoch: 643\tTrain Loss: 0.4458\tVal Loss: 0.4456\n",
      "Epoch: 644\tTrain Loss: 0.4457\tVal Loss: 0.4454\n",
      "Epoch: 645\tTrain Loss: 0.4455\tVal Loss: 0.4453\n",
      "Epoch: 646\tTrain Loss: 0.4453\tVal Loss: 0.4451\n",
      "Epoch: 647\tTrain Loss: 0.4452\tVal Loss: 0.4449\n",
      "Epoch: 648\tTrain Loss: 0.4450\tVal Loss: 0.4448\n",
      "Epoch: 649\tTrain Loss: 0.4448\tVal Loss: 0.4446\n",
      "Epoch: 650\tTrain Loss: 0.4447\tVal Loss: 0.4444\n",
      "Epoch: 651\tTrain Loss: 0.4445\tVal Loss: 0.4443\n",
      "Epoch: 652\tTrain Loss: 0.4444\tVal Loss: 0.4441\n",
      "Epoch: 653\tTrain Loss: 0.4442\tVal Loss: 0.4440\n",
      "Epoch: 654\tTrain Loss: 0.4440\tVal Loss: 0.4438\n",
      "Epoch: 655\tTrain Loss: 0.4439\tVal Loss: 0.4436\n",
      "Epoch: 656\tTrain Loss: 0.4437\tVal Loss: 0.4435\n",
      "Epoch: 657\tTrain Loss: 0.4435\tVal Loss: 0.4433\n",
      "Epoch: 658\tTrain Loss: 0.4434\tVal Loss: 0.4431\n",
      "Epoch: 659\tTrain Loss: 0.4432\tVal Loss: 0.4430\n",
      "Epoch: 660\tTrain Loss: 0.4431\tVal Loss: 0.4428\n",
      "Epoch: 661\tTrain Loss: 0.4429\tVal Loss: 0.4427\n",
      "Epoch: 662\tTrain Loss: 0.4427\tVal Loss: 0.4425\n",
      "Epoch: 663\tTrain Loss: 0.4426\tVal Loss: 0.4423\n",
      "Epoch: 664\tTrain Loss: 0.4424\tVal Loss: 0.4422\n",
      "Epoch: 665\tTrain Loss: 0.4423\tVal Loss: 0.4420\n",
      "Epoch: 666\tTrain Loss: 0.4421\tVal Loss: 0.4419\n",
      "Epoch: 667\tTrain Loss: 0.4419\tVal Loss: 0.4417\n",
      "Epoch: 668\tTrain Loss: 0.4418\tVal Loss: 0.4416\n",
      "Epoch: 669\tTrain Loss: 0.4416\tVal Loss: 0.4414\n",
      "Epoch: 670\tTrain Loss: 0.4415\tVal Loss: 0.4412\n",
      "Epoch: 671\tTrain Loss: 0.4413\tVal Loss: 0.4411\n",
      "Epoch: 672\tTrain Loss: 0.4412\tVal Loss: 0.4409\n",
      "Epoch: 673\tTrain Loss: 0.4410\tVal Loss: 0.4408\n",
      "Epoch: 674\tTrain Loss: 0.4408\tVal Loss: 0.4406\n",
      "Epoch: 675\tTrain Loss: 0.4407\tVal Loss: 0.4405\n",
      "Epoch: 676\tTrain Loss: 0.4405\tVal Loss: 0.4403\n",
      "Epoch: 677\tTrain Loss: 0.4404\tVal Loss: 0.4402\n",
      "Epoch: 678\tTrain Loss: 0.4402\tVal Loss: 0.4400\n",
      "Epoch: 679\tTrain Loss: 0.4401\tVal Loss: 0.4399\n",
      "Epoch: 680\tTrain Loss: 0.4399\tVal Loss: 0.4397\n",
      "Epoch: 681\tTrain Loss: 0.4398\tVal Loss: 0.4396\n",
      "Epoch: 682\tTrain Loss: 0.4396\tVal Loss: 0.4394\n",
      "Epoch: 683\tTrain Loss: 0.4395\tVal Loss: 0.4393\n",
      "Epoch: 684\tTrain Loss: 0.4393\tVal Loss: 0.4391\n",
      "Epoch: 685\tTrain Loss: 0.4392\tVal Loss: 0.4389\n",
      "Epoch: 686\tTrain Loss: 0.4390\tVal Loss: 0.4388\n",
      "Epoch: 687\tTrain Loss: 0.4389\tVal Loss: 0.4386\n",
      "Epoch: 688\tTrain Loss: 0.4387\tVal Loss: 0.4385\n",
      "Epoch: 689\tTrain Loss: 0.4386\tVal Loss: 0.4384\n",
      "Epoch: 690\tTrain Loss: 0.4384\tVal Loss: 0.4382\n",
      "Epoch: 691\tTrain Loss: 0.4383\tVal Loss: 0.4381\n",
      "Epoch: 692\tTrain Loss: 0.4381\tVal Loss: 0.4379\n",
      "Epoch: 693\tTrain Loss: 0.4380\tVal Loss: 0.4378\n",
      "Epoch: 694\tTrain Loss: 0.4378\tVal Loss: 0.4376\n",
      "Epoch: 695\tTrain Loss: 0.4377\tVal Loss: 0.4375\n",
      "Epoch: 696\tTrain Loss: 0.4375\tVal Loss: 0.4373\n",
      "Epoch: 697\tTrain Loss: 0.4374\tVal Loss: 0.4372\n",
      "Epoch: 698\tTrain Loss: 0.4372\tVal Loss: 0.4370\n",
      "Epoch: 699\tTrain Loss: 0.4371\tVal Loss: 0.4369\n",
      "Epoch: 700\tTrain Loss: 0.4369\tVal Loss: 0.4367\n",
      "Epoch: 701\tTrain Loss: 0.4368\tVal Loss: 0.4366\n",
      "Epoch: 702\tTrain Loss: 0.4367\tVal Loss: 0.4364\n",
      "Epoch: 703\tTrain Loss: 0.4365\tVal Loss: 0.4363\n",
      "Epoch: 704\tTrain Loss: 0.4364\tVal Loss: 0.4362\n",
      "Epoch: 705\tTrain Loss: 0.4362\tVal Loss: 0.4360\n",
      "Epoch: 706\tTrain Loss: 0.4361\tVal Loss: 0.4359\n",
      "Epoch: 707\tTrain Loss: 0.4359\tVal Loss: 0.4357\n",
      "Epoch: 708\tTrain Loss: 0.4358\tVal Loss: 0.4356\n",
      "Epoch: 709\tTrain Loss: 0.4356\tVal Loss: 0.4354\n",
      "Epoch: 710\tTrain Loss: 0.4355\tVal Loss: 0.4353\n",
      "Epoch: 711\tTrain Loss: 0.4354\tVal Loss: 0.4352\n",
      "Epoch: 712\tTrain Loss: 0.4352\tVal Loss: 0.4350\n",
      "Epoch: 713\tTrain Loss: 0.4351\tVal Loss: 0.4349\n",
      "Epoch: 714\tTrain Loss: 0.4349\tVal Loss: 0.4347\n",
      "Epoch: 715\tTrain Loss: 0.4348\tVal Loss: 0.4346\n",
      "Epoch: 716\tTrain Loss: 0.4347\tVal Loss: 0.4345\n",
      "Epoch: 717\tTrain Loss: 0.4345\tVal Loss: 0.4343\n",
      "Epoch: 718\tTrain Loss: 0.4344\tVal Loss: 0.4342\n",
      "Epoch: 719\tTrain Loss: 0.4342\tVal Loss: 0.4340\n",
      "Epoch: 720\tTrain Loss: 0.4341\tVal Loss: 0.4339\n",
      "Epoch: 721\tTrain Loss: 0.4340\tVal Loss: 0.4338\n",
      "Epoch: 722\tTrain Loss: 0.4338\tVal Loss: 0.4336\n",
      "Epoch: 723\tTrain Loss: 0.4337\tVal Loss: 0.4335\n",
      "Epoch: 724\tTrain Loss: 0.4335\tVal Loss: 0.4333\n",
      "Epoch: 725\tTrain Loss: 0.4334\tVal Loss: 0.4332\n",
      "Epoch: 726\tTrain Loss: 0.4333\tVal Loss: 0.4331\n",
      "Epoch: 727\tTrain Loss: 0.4331\tVal Loss: 0.4329\n",
      "Epoch: 728\tTrain Loss: 0.4330\tVal Loss: 0.4328\n",
      "Epoch: 729\tTrain Loss: 0.4329\tVal Loss: 0.4327\n",
      "Epoch: 730\tTrain Loss: 0.4327\tVal Loss: 0.4325\n",
      "Epoch: 731\tTrain Loss: 0.4326\tVal Loss: 0.4324\n",
      "Epoch: 732\tTrain Loss: 0.4325\tVal Loss: 0.4323\n",
      "Epoch: 733\tTrain Loss: 0.4323\tVal Loss: 0.4321\n",
      "Epoch: 734\tTrain Loss: 0.4322\tVal Loss: 0.4320\n",
      "Epoch: 735\tTrain Loss: 0.4321\tVal Loss: 0.4319\n",
      "Epoch: 736\tTrain Loss: 0.4319\tVal Loss: 0.4317\n",
      "Epoch: 737\tTrain Loss: 0.4318\tVal Loss: 0.4316\n",
      "Epoch: 738\tTrain Loss: 0.4316\tVal Loss: 0.4315\n",
      "Epoch: 739\tTrain Loss: 0.4315\tVal Loss: 0.4313\n",
      "Epoch: 740\tTrain Loss: 0.4314\tVal Loss: 0.4312\n",
      "Epoch: 741\tTrain Loss: 0.4313\tVal Loss: 0.4311\n",
      "Epoch: 742\tTrain Loss: 0.4311\tVal Loss: 0.4309\n",
      "Epoch: 743\tTrain Loss: 0.4310\tVal Loss: 0.4308\n",
      "Epoch: 744\tTrain Loss: 0.4309\tVal Loss: 0.4307\n",
      "Epoch: 745\tTrain Loss: 0.4307\tVal Loss: 0.4305\n",
      "Epoch: 746\tTrain Loss: 0.4306\tVal Loss: 0.4304\n",
      "Epoch: 747\tTrain Loss: 0.4305\tVal Loss: 0.4303\n",
      "Epoch: 748\tTrain Loss: 0.4303\tVal Loss: 0.4301\n",
      "Epoch: 749\tTrain Loss: 0.4302\tVal Loss: 0.4300\n",
      "Epoch: 750\tTrain Loss: 0.4301\tVal Loss: 0.4299\n",
      "Epoch: 751\tTrain Loss: 0.4299\tVal Loss: 0.4297\n",
      "Epoch: 752\tTrain Loss: 0.4298\tVal Loss: 0.4296\n",
      "Epoch: 753\tTrain Loss: 0.4297\tVal Loss: 0.4295\n",
      "Epoch: 754\tTrain Loss: 0.4296\tVal Loss: 0.4294\n",
      "Epoch: 755\tTrain Loss: 0.4294\tVal Loss: 0.4292\n",
      "Epoch: 756\tTrain Loss: 0.4293\tVal Loss: 0.4291\n",
      "Epoch: 757\tTrain Loss: 0.4292\tVal Loss: 0.4290\n",
      "Epoch: 758\tTrain Loss: 0.4290\tVal Loss: 0.4288\n",
      "Epoch: 759\tTrain Loss: 0.4289\tVal Loss: 0.4287\n",
      "Epoch: 760\tTrain Loss: 0.4288\tVal Loss: 0.4286\n",
      "Epoch: 761\tTrain Loss: 0.4287\tVal Loss: 0.4285\n",
      "Epoch: 762\tTrain Loss: 0.4285\tVal Loss: 0.4283\n",
      "Epoch: 763\tTrain Loss: 0.4284\tVal Loss: 0.4282\n",
      "Epoch: 764\tTrain Loss: 0.4283\tVal Loss: 0.4281\n",
      "Epoch: 765\tTrain Loss: 0.4281\tVal Loss: 0.4280\n",
      "Epoch: 766\tTrain Loss: 0.4280\tVal Loss: 0.4278\n",
      "Epoch: 767\tTrain Loss: 0.4279\tVal Loss: 0.4277\n",
      "Epoch: 768\tTrain Loss: 0.4278\tVal Loss: 0.4276\n",
      "Epoch: 769\tTrain Loss: 0.4276\tVal Loss: 0.4275\n",
      "Epoch: 770\tTrain Loss: 0.4275\tVal Loss: 0.4273\n",
      "Epoch: 771\tTrain Loss: 0.4274\tVal Loss: 0.4272\n",
      "Epoch: 772\tTrain Loss: 0.4273\tVal Loss: 0.4271\n",
      "Epoch: 773\tTrain Loss: 0.4271\tVal Loss: 0.4270\n",
      "Epoch: 774\tTrain Loss: 0.4270\tVal Loss: 0.4268\n",
      "Epoch: 775\tTrain Loss: 0.4269\tVal Loss: 0.4267\n",
      "Epoch: 776\tTrain Loss: 0.4268\tVal Loss: 0.4266\n",
      "Epoch: 777\tTrain Loss: 0.4267\tVal Loss: 0.4265\n",
      "Epoch: 778\tTrain Loss: 0.4265\tVal Loss: 0.4263\n",
      "Epoch: 779\tTrain Loss: 0.4264\tVal Loss: 0.4262\n",
      "Epoch: 780\tTrain Loss: 0.4263\tVal Loss: 0.4261\n",
      "Epoch: 781\tTrain Loss: 0.4262\tVal Loss: 0.4260\n",
      "Epoch: 782\tTrain Loss: 0.4260\tVal Loss: 0.4259\n",
      "Epoch: 783\tTrain Loss: 0.4259\tVal Loss: 0.4257\n",
      "Epoch: 784\tTrain Loss: 0.4258\tVal Loss: 0.4256\n",
      "Epoch: 785\tTrain Loss: 0.4257\tVal Loss: 0.4255\n",
      "Epoch: 786\tTrain Loss: 0.4256\tVal Loss: 0.4254\n",
      "Epoch: 787\tTrain Loss: 0.4254\tVal Loss: 0.4253\n",
      "Epoch: 788\tTrain Loss: 0.4253\tVal Loss: 0.4251\n",
      "Epoch: 789\tTrain Loss: 0.4252\tVal Loss: 0.4250\n",
      "Epoch: 790\tTrain Loss: 0.4251\tVal Loss: 0.4249\n",
      "Epoch: 791\tTrain Loss: 0.4250\tVal Loss: 0.4248\n",
      "Epoch: 792\tTrain Loss: 0.4248\tVal Loss: 0.4247\n",
      "Epoch: 793\tTrain Loss: 0.4247\tVal Loss: 0.4245\n",
      "Epoch: 794\tTrain Loss: 0.4246\tVal Loss: 0.4244\n",
      "Epoch: 795\tTrain Loss: 0.4245\tVal Loss: 0.4243\n",
      "Epoch: 796\tTrain Loss: 0.4244\tVal Loss: 0.4242\n",
      "Epoch: 797\tTrain Loss: 0.4242\tVal Loss: 0.4241\n",
      "Epoch: 798\tTrain Loss: 0.4241\tVal Loss: 0.4240\n",
      "Epoch: 799\tTrain Loss: 0.4240\tVal Loss: 0.4238\n",
      "Epoch: 800\tTrain Loss: 0.4239\tVal Loss: 0.4237\n",
      "Epoch: 801\tTrain Loss: 0.4238\tVal Loss: 0.4236\n",
      "Epoch: 802\tTrain Loss: 0.4237\tVal Loss: 0.4235\n",
      "Epoch: 803\tTrain Loss: 0.4235\tVal Loss: 0.4234\n",
      "Epoch: 804\tTrain Loss: 0.4234\tVal Loss: 0.4233\n",
      "Epoch: 805\tTrain Loss: 0.4233\tVal Loss: 0.4231\n",
      "Epoch: 806\tTrain Loss: 0.4232\tVal Loss: 0.4230\n",
      "Epoch: 807\tTrain Loss: 0.4231\tVal Loss: 0.4229\n",
      "Epoch: 808\tTrain Loss: 0.4230\tVal Loss: 0.4228\n",
      "Epoch: 809\tTrain Loss: 0.4229\tVal Loss: 0.4227\n",
      "Epoch: 810\tTrain Loss: 0.4227\tVal Loss: 0.4226\n",
      "Epoch: 811\tTrain Loss: 0.4226\tVal Loss: 0.4224\n",
      "Epoch: 812\tTrain Loss: 0.4225\tVal Loss: 0.4223\n",
      "Epoch: 813\tTrain Loss: 0.4224\tVal Loss: 0.4222\n",
      "Epoch: 814\tTrain Loss: 0.4223\tVal Loss: 0.4221\n",
      "Epoch: 815\tTrain Loss: 0.4222\tVal Loss: 0.4220\n",
      "Epoch: 816\tTrain Loss: 0.4221\tVal Loss: 0.4219\n",
      "Epoch: 817\tTrain Loss: 0.4219\tVal Loss: 0.4218\n",
      "Epoch: 818\tTrain Loss: 0.4218\tVal Loss: 0.4217\n",
      "Epoch: 819\tTrain Loss: 0.4217\tVal Loss: 0.4215\n",
      "Epoch: 820\tTrain Loss: 0.4216\tVal Loss: 0.4214\n",
      "Epoch: 821\tTrain Loss: 0.4215\tVal Loss: 0.4213\n",
      "Epoch: 822\tTrain Loss: 0.4214\tVal Loss: 0.4212\n",
      "Epoch: 823\tTrain Loss: 0.4213\tVal Loss: 0.4211\n",
      "Epoch: 824\tTrain Loss: 0.4211\tVal Loss: 0.4210\n",
      "Epoch: 825\tTrain Loss: 0.4210\tVal Loss: 0.4209\n",
      "Epoch: 826\tTrain Loss: 0.4209\tVal Loss: 0.4208\n",
      "Epoch: 827\tTrain Loss: 0.4208\tVal Loss: 0.4206\n",
      "Epoch: 828\tTrain Loss: 0.4207\tVal Loss: 0.4205\n",
      "Epoch: 829\tTrain Loss: 0.4206\tVal Loss: 0.4204\n",
      "Epoch: 830\tTrain Loss: 0.4205\tVal Loss: 0.4203\n",
      "Epoch: 831\tTrain Loss: 0.4204\tVal Loss: 0.4202\n",
      "Epoch: 832\tTrain Loss: 0.4203\tVal Loss: 0.4201\n",
      "Epoch: 833\tTrain Loss: 0.4202\tVal Loss: 0.4200\n",
      "Epoch: 834\tTrain Loss: 0.4200\tVal Loss: 0.4199\n",
      "Epoch: 835\tTrain Loss: 0.4199\tVal Loss: 0.4198\n",
      "Epoch: 836\tTrain Loss: 0.4198\tVal Loss: 0.4197\n",
      "Epoch: 837\tTrain Loss: 0.4197\tVal Loss: 0.4195\n",
      "Epoch: 838\tTrain Loss: 0.4196\tVal Loss: 0.4194\n",
      "Epoch: 839\tTrain Loss: 0.4195\tVal Loss: 0.4193\n",
      "Epoch: 840\tTrain Loss: 0.4194\tVal Loss: 0.4192\n",
      "Epoch: 841\tTrain Loss: 0.4193\tVal Loss: 0.4191\n",
      "Epoch: 842\tTrain Loss: 0.4192\tVal Loss: 0.4190\n",
      "Epoch: 843\tTrain Loss: 0.4191\tVal Loss: 0.4189\n",
      "Epoch: 844\tTrain Loss: 0.4190\tVal Loss: 0.4188\n",
      "Epoch: 845\tTrain Loss: 0.4188\tVal Loss: 0.4187\n",
      "Epoch: 846\tTrain Loss: 0.4187\tVal Loss: 0.4186\n",
      "Epoch: 847\tTrain Loss: 0.4186\tVal Loss: 0.4185\n",
      "Epoch: 848\tTrain Loss: 0.4185\tVal Loss: 0.4184\n",
      "Epoch: 849\tTrain Loss: 0.4184\tVal Loss: 0.4183\n",
      "Epoch: 850\tTrain Loss: 0.4183\tVal Loss: 0.4182\n",
      "Epoch: 851\tTrain Loss: 0.4182\tVal Loss: 0.4180\n",
      "Epoch: 852\tTrain Loss: 0.4181\tVal Loss: 0.4179\n",
      "Epoch: 853\tTrain Loss: 0.4180\tVal Loss: 0.4178\n",
      "Epoch: 854\tTrain Loss: 0.4179\tVal Loss: 0.4177\n",
      "Epoch: 855\tTrain Loss: 0.4178\tVal Loss: 0.4176\n",
      "Epoch: 856\tTrain Loss: 0.4177\tVal Loss: 0.4175\n",
      "Epoch: 857\tTrain Loss: 0.4176\tVal Loss: 0.4174\n",
      "Epoch: 858\tTrain Loss: 0.4175\tVal Loss: 0.4173\n",
      "Epoch: 859\tTrain Loss: 0.4174\tVal Loss: 0.4172\n",
      "Epoch: 860\tTrain Loss: 0.4173\tVal Loss: 0.4171\n",
      "Epoch: 861\tTrain Loss: 0.4172\tVal Loss: 0.4170\n",
      "Epoch: 862\tTrain Loss: 0.4171\tVal Loss: 0.4169\n",
      "Epoch: 863\tTrain Loss: 0.4170\tVal Loss: 0.4168\n",
      "Epoch: 864\tTrain Loss: 0.4168\tVal Loss: 0.4167\n",
      "Epoch: 865\tTrain Loss: 0.4167\tVal Loss: 0.4166\n",
      "Epoch: 866\tTrain Loss: 0.4166\tVal Loss: 0.4165\n",
      "Epoch: 867\tTrain Loss: 0.4165\tVal Loss: 0.4164\n",
      "Epoch: 868\tTrain Loss: 0.4164\tVal Loss: 0.4163\n",
      "Epoch: 869\tTrain Loss: 0.4163\tVal Loss: 0.4162\n",
      "Epoch: 870\tTrain Loss: 0.4162\tVal Loss: 0.4161\n",
      "Epoch: 871\tTrain Loss: 0.4161\tVal Loss: 0.4160\n",
      "Epoch: 872\tTrain Loss: 0.4160\tVal Loss: 0.4159\n",
      "Epoch: 873\tTrain Loss: 0.4159\tVal Loss: 0.4158\n",
      "Epoch: 874\tTrain Loss: 0.4158\tVal Loss: 0.4157\n",
      "Epoch: 875\tTrain Loss: 0.4157\tVal Loss: 0.4156\n",
      "Epoch: 876\tTrain Loss: 0.4156\tVal Loss: 0.4155\n",
      "Epoch: 877\tTrain Loss: 0.4155\tVal Loss: 0.4154\n",
      "Epoch: 878\tTrain Loss: 0.4154\tVal Loss: 0.4153\n",
      "Epoch: 879\tTrain Loss: 0.4153\tVal Loss: 0.4152\n",
      "Epoch: 880\tTrain Loss: 0.4152\tVal Loss: 0.4151\n",
      "Epoch: 881\tTrain Loss: 0.4151\tVal Loss: 0.4150\n",
      "Epoch: 882\tTrain Loss: 0.4150\tVal Loss: 0.4149\n",
      "Epoch: 883\tTrain Loss: 0.4149\tVal Loss: 0.4148\n",
      "Epoch: 884\tTrain Loss: 0.4148\tVal Loss: 0.4147\n",
      "Epoch: 885\tTrain Loss: 0.4147\tVal Loss: 0.4146\n",
      "Epoch: 886\tTrain Loss: 0.4146\tVal Loss: 0.4145\n",
      "Epoch: 887\tTrain Loss: 0.4145\tVal Loss: 0.4144\n",
      "Epoch: 888\tTrain Loss: 0.4144\tVal Loss: 0.4143\n",
      "Epoch: 889\tTrain Loss: 0.4143\tVal Loss: 0.4142\n",
      "Epoch: 890\tTrain Loss: 0.4142\tVal Loss: 0.4141\n",
      "Epoch: 891\tTrain Loss: 0.4141\tVal Loss: 0.4140\n",
      "Epoch: 892\tTrain Loss: 0.4140\tVal Loss: 0.4139\n",
      "Epoch: 893\tTrain Loss: 0.4139\tVal Loss: 0.4138\n",
      "Epoch: 894\tTrain Loss: 0.4138\tVal Loss: 0.4137\n",
      "Epoch: 895\tTrain Loss: 0.4137\tVal Loss: 0.4136\n",
      "Epoch: 896\tTrain Loss: 0.4136\tVal Loss: 0.4135\n",
      "Epoch: 897\tTrain Loss: 0.4135\tVal Loss: 0.4134\n",
      "Epoch: 898\tTrain Loss: 0.4134\tVal Loss: 0.4133\n",
      "Epoch: 899\tTrain Loss: 0.4133\tVal Loss: 0.4132\n",
      "Epoch: 900\tTrain Loss: 0.4132\tVal Loss: 0.4131\n",
      "Epoch: 901\tTrain Loss: 0.4131\tVal Loss: 0.4130\n",
      "Epoch: 902\tTrain Loss: 0.4130\tVal Loss: 0.4129\n",
      "Epoch: 903\tTrain Loss: 0.4129\tVal Loss: 0.4128\n",
      "Epoch: 904\tTrain Loss: 0.4129\tVal Loss: 0.4127\n",
      "Epoch: 905\tTrain Loss: 0.4128\tVal Loss: 0.4126\n",
      "Epoch: 906\tTrain Loss: 0.4127\tVal Loss: 0.4125\n",
      "Epoch: 907\tTrain Loss: 0.4126\tVal Loss: 0.4124\n",
      "Epoch: 908\tTrain Loss: 0.4125\tVal Loss: 0.4123\n",
      "Epoch: 909\tTrain Loss: 0.4124\tVal Loss: 0.4122\n",
      "Epoch: 910\tTrain Loss: 0.4123\tVal Loss: 0.4121\n",
      "Epoch: 911\tTrain Loss: 0.4122\tVal Loss: 0.4120\n",
      "Epoch: 912\tTrain Loss: 0.4121\tVal Loss: 0.4119\n",
      "Epoch: 913\tTrain Loss: 0.4120\tVal Loss: 0.4118\n",
      "Epoch: 914\tTrain Loss: 0.4119\tVal Loss: 0.4118\n",
      "Epoch: 915\tTrain Loss: 0.4118\tVal Loss: 0.4117\n",
      "Epoch: 916\tTrain Loss: 0.4117\tVal Loss: 0.4116\n",
      "Epoch: 917\tTrain Loss: 0.4116\tVal Loss: 0.4115\n",
      "Epoch: 918\tTrain Loss: 0.4115\tVal Loss: 0.4114\n",
      "Epoch: 919\tTrain Loss: 0.4114\tVal Loss: 0.4113\n",
      "Epoch: 920\tTrain Loss: 0.4113\tVal Loss: 0.4112\n",
      "Epoch: 921\tTrain Loss: 0.4112\tVal Loss: 0.4111\n",
      "Epoch: 922\tTrain Loss: 0.4111\tVal Loss: 0.4110\n",
      "Epoch: 923\tTrain Loss: 0.4111\tVal Loss: 0.4109\n",
      "Epoch: 924\tTrain Loss: 0.4110\tVal Loss: 0.4108\n",
      "Epoch: 925\tTrain Loss: 0.4109\tVal Loss: 0.4107\n",
      "Epoch: 926\tTrain Loss: 0.4108\tVal Loss: 0.4106\n",
      "Epoch: 927\tTrain Loss: 0.4107\tVal Loss: 0.4105\n",
      "Epoch: 928\tTrain Loss: 0.4106\tVal Loss: 0.4104\n",
      "Epoch: 929\tTrain Loss: 0.4105\tVal Loss: 0.4104\n",
      "Epoch: 930\tTrain Loss: 0.4104\tVal Loss: 0.4103\n",
      "Epoch: 931\tTrain Loss: 0.4103\tVal Loss: 0.4102\n",
      "Epoch: 932\tTrain Loss: 0.4102\tVal Loss: 0.4101\n",
      "Epoch: 933\tTrain Loss: 0.4101\tVal Loss: 0.4100\n",
      "Epoch: 934\tTrain Loss: 0.4100\tVal Loss: 0.4099\n",
      "Epoch: 935\tTrain Loss: 0.4099\tVal Loss: 0.4098\n",
      "Epoch: 936\tTrain Loss: 0.4099\tVal Loss: 0.4097\n",
      "Epoch: 937\tTrain Loss: 0.4098\tVal Loss: 0.4096\n",
      "Epoch: 938\tTrain Loss: 0.4097\tVal Loss: 0.4095\n",
      "Epoch: 939\tTrain Loss: 0.4096\tVal Loss: 0.4094\n",
      "Epoch: 940\tTrain Loss: 0.4095\tVal Loss: 0.4094\n",
      "Epoch: 941\tTrain Loss: 0.4094\tVal Loss: 0.4093\n",
      "Epoch: 942\tTrain Loss: 0.4093\tVal Loss: 0.4092\n",
      "Epoch: 943\tTrain Loss: 0.4092\tVal Loss: 0.4091\n",
      "Epoch: 944\tTrain Loss: 0.4091\tVal Loss: 0.4090\n",
      "Epoch: 945\tTrain Loss: 0.4090\tVal Loss: 0.4089\n",
      "Epoch: 946\tTrain Loss: 0.4090\tVal Loss: 0.4088\n",
      "Epoch: 947\tTrain Loss: 0.4089\tVal Loss: 0.4087\n",
      "Epoch: 948\tTrain Loss: 0.4088\tVal Loss: 0.4086\n",
      "Epoch: 949\tTrain Loss: 0.4087\tVal Loss: 0.4085\n",
      "Epoch: 950\tTrain Loss: 0.4086\tVal Loss: 0.4085\n",
      "Epoch: 951\tTrain Loss: 0.4085\tVal Loss: 0.4084\n",
      "Epoch: 952\tTrain Loss: 0.4084\tVal Loss: 0.4083\n",
      "Epoch: 953\tTrain Loss: 0.4083\tVal Loss: 0.4082\n",
      "Epoch: 954\tTrain Loss: 0.4082\tVal Loss: 0.4081\n",
      "Epoch: 955\tTrain Loss: 0.4082\tVal Loss: 0.4080\n",
      "Epoch: 956\tTrain Loss: 0.4081\tVal Loss: 0.4079\n",
      "Epoch: 957\tTrain Loss: 0.4080\tVal Loss: 0.4078\n",
      "Epoch: 958\tTrain Loss: 0.4079\tVal Loss: 0.4078\n",
      "Epoch: 959\tTrain Loss: 0.4078\tVal Loss: 0.4077\n",
      "Epoch: 960\tTrain Loss: 0.4077\tVal Loss: 0.4076\n",
      "Epoch: 961\tTrain Loss: 0.4076\tVal Loss: 0.4075\n",
      "Epoch: 962\tTrain Loss: 0.4075\tVal Loss: 0.4074\n",
      "Epoch: 963\tTrain Loss: 0.4075\tVal Loss: 0.4073\n",
      "Epoch: 964\tTrain Loss: 0.4074\tVal Loss: 0.4072\n",
      "Epoch: 965\tTrain Loss: 0.4073\tVal Loss: 0.4071\n",
      "Epoch: 966\tTrain Loss: 0.4072\tVal Loss: 0.4071\n",
      "Epoch: 967\tTrain Loss: 0.4071\tVal Loss: 0.4070\n",
      "Epoch: 968\tTrain Loss: 0.4070\tVal Loss: 0.4069\n",
      "Epoch: 969\tTrain Loss: 0.4069\tVal Loss: 0.4068\n",
      "Epoch: 970\tTrain Loss: 0.4069\tVal Loss: 0.4067\n",
      "Epoch: 971\tTrain Loss: 0.4068\tVal Loss: 0.4066\n",
      "Epoch: 972\tTrain Loss: 0.4067\tVal Loss: 0.4065\n",
      "Epoch: 973\tTrain Loss: 0.4066\tVal Loss: 0.4065\n",
      "Epoch: 974\tTrain Loss: 0.4065\tVal Loss: 0.4064\n",
      "Epoch: 975\tTrain Loss: 0.4064\tVal Loss: 0.4063\n",
      "Epoch: 976\tTrain Loss: 0.4063\tVal Loss: 0.4062\n",
      "Epoch: 977\tTrain Loss: 0.4063\tVal Loss: 0.4061\n",
      "Epoch: 978\tTrain Loss: 0.4062\tVal Loss: 0.4060\n",
      "Epoch: 979\tTrain Loss: 0.4061\tVal Loss: 0.4060\n",
      "Epoch: 980\tTrain Loss: 0.4060\tVal Loss: 0.4059\n",
      "Epoch: 981\tTrain Loss: 0.4059\tVal Loss: 0.4058\n",
      "Epoch: 982\tTrain Loss: 0.4058\tVal Loss: 0.4057\n",
      "Epoch: 983\tTrain Loss: 0.4058\tVal Loss: 0.4056\n",
      "Epoch: 984\tTrain Loss: 0.4057\tVal Loss: 0.4055\n",
      "Epoch: 985\tTrain Loss: 0.4056\tVal Loss: 0.4055\n",
      "Epoch: 986\tTrain Loss: 0.4055\tVal Loss: 0.4054\n",
      "Epoch: 987\tTrain Loss: 0.4054\tVal Loss: 0.4053\n",
      "Epoch: 988\tTrain Loss: 0.4053\tVal Loss: 0.4052\n",
      "Epoch: 989\tTrain Loss: 0.4053\tVal Loss: 0.4051\n",
      "Epoch: 990\tTrain Loss: 0.4052\tVal Loss: 0.4050\n",
      "Epoch: 991\tTrain Loss: 0.4051\tVal Loss: 0.4050\n",
      "Epoch: 992\tTrain Loss: 0.4050\tVal Loss: 0.4049\n",
      "Epoch: 993\tTrain Loss: 0.4049\tVal Loss: 0.4048\n",
      "Epoch: 994\tTrain Loss: 0.4048\tVal Loss: 0.4047\n",
      "Epoch: 995\tTrain Loss: 0.4048\tVal Loss: 0.4046\n",
      "Epoch: 996\tTrain Loss: 0.4047\tVal Loss: 0.4045\n",
      "Epoch: 997\tTrain Loss: 0.4046\tVal Loss: 0.4045\n",
      "Epoch: 998\tTrain Loss: 0.4045\tVal Loss: 0.4044\n",
      "Epoch: 999\tTrain Loss: 0.4044\tVal Loss: 0.4043\n",
      "Epoch: 1000\tTrain Loss: 0.4043\tVal Loss: 0.4042\n",
      "Epoch: 1001\tTrain Loss: 0.4043\tVal Loss: 0.4041\n",
      "Epoch: 1002\tTrain Loss: 0.4042\tVal Loss: 0.4041\n",
      "Epoch: 1003\tTrain Loss: 0.4041\tVal Loss: 0.4040\n",
      "Epoch: 1004\tTrain Loss: 0.4040\tVal Loss: 0.4039\n",
      "Epoch: 1005\tTrain Loss: 0.4039\tVal Loss: 0.4038\n",
      "Epoch: 1006\tTrain Loss: 0.4039\tVal Loss: 0.4037\n",
      "Epoch: 1007\tTrain Loss: 0.4038\tVal Loss: 0.4036\n",
      "Epoch: 1008\tTrain Loss: 0.4037\tVal Loss: 0.4036\n",
      "Epoch: 1009\tTrain Loss: 0.4036\tVal Loss: 0.4035\n",
      "Epoch: 1010\tTrain Loss: 0.4035\tVal Loss: 0.4034\n",
      "Epoch: 1011\tTrain Loss: 0.4035\tVal Loss: 0.4033\n",
      "Epoch: 1012\tTrain Loss: 0.4034\tVal Loss: 0.4032\n",
      "Epoch: 1013\tTrain Loss: 0.4033\tVal Loss: 0.4032\n",
      "Epoch: 1014\tTrain Loss: 0.4032\tVal Loss: 0.4031\n",
      "Epoch: 1015\tTrain Loss: 0.4031\tVal Loss: 0.4030\n",
      "Epoch: 1016\tTrain Loss: 0.4031\tVal Loss: 0.4029\n",
      "Epoch: 1017\tTrain Loss: 0.4030\tVal Loss: 0.4029\n",
      "Epoch: 1018\tTrain Loss: 0.4029\tVal Loss: 0.4028\n",
      "Epoch: 1019\tTrain Loss: 0.4028\tVal Loss: 0.4027\n",
      "Epoch: 1020\tTrain Loss: 0.4027\tVal Loss: 0.4026\n",
      "Epoch: 1021\tTrain Loss: 0.4027\tVal Loss: 0.4025\n",
      "Epoch: 1022\tTrain Loss: 0.4026\tVal Loss: 0.4025\n",
      "Epoch: 1023\tTrain Loss: 0.4025\tVal Loss: 0.4024\n",
      "Epoch: 1024\tTrain Loss: 0.4024\tVal Loss: 0.4023\n",
      "Epoch: 1025\tTrain Loss: 0.4023\tVal Loss: 0.4022\n",
      "Epoch: 1026\tTrain Loss: 0.4023\tVal Loss: 0.4021\n",
      "Epoch: 1027\tTrain Loss: 0.4022\tVal Loss: 0.4021\n",
      "Epoch: 1028\tTrain Loss: 0.4021\tVal Loss: 0.4020\n",
      "Epoch: 1029\tTrain Loss: 0.4020\tVal Loss: 0.4019\n",
      "Epoch: 1030\tTrain Loss: 0.4020\tVal Loss: 0.4018\n",
      "Epoch: 1031\tTrain Loss: 0.4019\tVal Loss: 0.4018\n",
      "Epoch: 1032\tTrain Loss: 0.4018\tVal Loss: 0.4017\n",
      "Epoch: 1033\tTrain Loss: 0.4017\tVal Loss: 0.4016\n",
      "Epoch: 1034\tTrain Loss: 0.4017\tVal Loss: 0.4015\n",
      "Epoch: 1035\tTrain Loss: 0.4016\tVal Loss: 0.4014\n",
      "Epoch: 1036\tTrain Loss: 0.4015\tVal Loss: 0.4014\n",
      "Epoch: 1037\tTrain Loss: 0.4014\tVal Loss: 0.4013\n",
      "Epoch: 1038\tTrain Loss: 0.4013\tVal Loss: 0.4012\n",
      "Epoch: 1039\tTrain Loss: 0.4013\tVal Loss: 0.4011\n",
      "Epoch: 1040\tTrain Loss: 0.4012\tVal Loss: 0.4011\n",
      "Epoch: 1041\tTrain Loss: 0.4011\tVal Loss: 0.4010\n",
      "Epoch: 1042\tTrain Loss: 0.4010\tVal Loss: 0.4009\n",
      "Epoch: 1043\tTrain Loss: 0.4010\tVal Loss: 0.4008\n",
      "Epoch: 1044\tTrain Loss: 0.4009\tVal Loss: 0.4008\n",
      "Epoch: 1045\tTrain Loss: 0.4008\tVal Loss: 0.4007\n",
      "Epoch: 1046\tTrain Loss: 0.4007\tVal Loss: 0.4006\n",
      "Epoch: 1047\tTrain Loss: 0.4007\tVal Loss: 0.4005\n",
      "Epoch: 1048\tTrain Loss: 0.4006\tVal Loss: 0.4005\n",
      "Epoch: 1049\tTrain Loss: 0.4005\tVal Loss: 0.4004\n",
      "Epoch: 1050\tTrain Loss: 0.4004\tVal Loss: 0.4003\n",
      "Epoch: 1051\tTrain Loss: 0.4004\tVal Loss: 0.4002\n",
      "Epoch: 1052\tTrain Loss: 0.4003\tVal Loss: 0.4002\n",
      "Epoch: 1053\tTrain Loss: 0.4002\tVal Loss: 0.4001\n",
      "Epoch: 1054\tTrain Loss: 0.4001\tVal Loss: 0.4000\n",
      "Epoch: 1055\tTrain Loss: 0.4001\tVal Loss: 0.3999\n",
      "Epoch: 1056\tTrain Loss: 0.4000\tVal Loss: 0.3999\n",
      "Epoch: 1057\tTrain Loss: 0.3999\tVal Loss: 0.3998\n",
      "Epoch: 1058\tTrain Loss: 0.3998\tVal Loss: 0.3997\n",
      "Epoch: 1059\tTrain Loss: 0.3998\tVal Loss: 0.3996\n",
      "Epoch: 1060\tTrain Loss: 0.3997\tVal Loss: 0.3996\n",
      "Epoch: 1061\tTrain Loss: 0.3996\tVal Loss: 0.3995\n",
      "Epoch: 1062\tTrain Loss: 0.3995\tVal Loss: 0.3994\n",
      "Epoch: 1063\tTrain Loss: 0.3995\tVal Loss: 0.3993\n",
      "Epoch: 1064\tTrain Loss: 0.3994\tVal Loss: 0.3993\n",
      "Epoch: 1065\tTrain Loss: 0.3993\tVal Loss: 0.3992\n",
      "Epoch: 1066\tTrain Loss: 0.3992\tVal Loss: 0.3991\n",
      "Epoch: 1067\tTrain Loss: 0.3992\tVal Loss: 0.3991\n",
      "Epoch: 1068\tTrain Loss: 0.3991\tVal Loss: 0.3990\n",
      "Epoch: 1069\tTrain Loss: 0.3990\tVal Loss: 0.3989\n",
      "Epoch: 1070\tTrain Loss: 0.3990\tVal Loss: 0.3988\n",
      "Epoch: 1071\tTrain Loss: 0.3989\tVal Loss: 0.3988\n",
      "Epoch: 1072\tTrain Loss: 0.3988\tVal Loss: 0.3987\n",
      "Epoch: 1073\tTrain Loss: 0.3987\tVal Loss: 0.3986\n",
      "Epoch: 1074\tTrain Loss: 0.3987\tVal Loss: 0.3985\n",
      "Epoch: 1075\tTrain Loss: 0.3986\tVal Loss: 0.3985\n",
      "Epoch: 1076\tTrain Loss: 0.3985\tVal Loss: 0.3984\n",
      "Epoch: 1077\tTrain Loss: 0.3984\tVal Loss: 0.3983\n",
      "Epoch: 1078\tTrain Loss: 0.3984\tVal Loss: 0.3983\n",
      "Epoch: 1079\tTrain Loss: 0.3983\tVal Loss: 0.3982\n",
      "Epoch: 1080\tTrain Loss: 0.3982\tVal Loss: 0.3981\n",
      "Epoch: 1081\tTrain Loss: 0.3982\tVal Loss: 0.3980\n",
      "Epoch: 1082\tTrain Loss: 0.3981\tVal Loss: 0.3980\n",
      "Epoch: 1083\tTrain Loss: 0.3980\tVal Loss: 0.3979\n",
      "Epoch: 1084\tTrain Loss: 0.3979\tVal Loss: 0.3978\n",
      "Epoch: 1085\tTrain Loss: 0.3979\tVal Loss: 0.3978\n",
      "Epoch: 1086\tTrain Loss: 0.3978\tVal Loss: 0.3977\n",
      "Epoch: 1087\tTrain Loss: 0.3977\tVal Loss: 0.3976\n",
      "Epoch: 1088\tTrain Loss: 0.3977\tVal Loss: 0.3975\n",
      "Epoch: 1089\tTrain Loss: 0.3976\tVal Loss: 0.3975\n",
      "Epoch: 1090\tTrain Loss: 0.3975\tVal Loss: 0.3974\n",
      "Epoch: 1091\tTrain Loss: 0.3975\tVal Loss: 0.3973\n",
      "Epoch: 1092\tTrain Loss: 0.3974\tVal Loss: 0.3973\n",
      "Epoch: 1093\tTrain Loss: 0.3973\tVal Loss: 0.3972\n",
      "Epoch: 1094\tTrain Loss: 0.3972\tVal Loss: 0.3971\n",
      "Epoch: 1095\tTrain Loss: 0.3972\tVal Loss: 0.3971\n",
      "Epoch: 1096\tTrain Loss: 0.3971\tVal Loss: 0.3970\n",
      "Epoch: 1097\tTrain Loss: 0.3970\tVal Loss: 0.3969\n",
      "Epoch: 1098\tTrain Loss: 0.3970\tVal Loss: 0.3968\n",
      "Epoch: 1099\tTrain Loss: 0.3969\tVal Loss: 0.3968\n",
      "Epoch: 1100\tTrain Loss: 0.3968\tVal Loss: 0.3967\n",
      "Epoch: 1101\tTrain Loss: 0.3968\tVal Loss: 0.3966\n",
      "Epoch: 1102\tTrain Loss: 0.3967\tVal Loss: 0.3966\n",
      "Epoch: 1103\tTrain Loss: 0.3966\tVal Loss: 0.3965\n",
      "Epoch: 1104\tTrain Loss: 0.3965\tVal Loss: 0.3964\n",
      "Epoch: 1105\tTrain Loss: 0.3965\tVal Loss: 0.3964\n",
      "Epoch: 1106\tTrain Loss: 0.3964\tVal Loss: 0.3963\n",
      "Epoch: 1107\tTrain Loss: 0.3963\tVal Loss: 0.3962\n",
      "Epoch: 1108\tTrain Loss: 0.3963\tVal Loss: 0.3962\n",
      "Epoch: 1109\tTrain Loss: 0.3962\tVal Loss: 0.3961\n",
      "Epoch: 1110\tTrain Loss: 0.3961\tVal Loss: 0.3960\n",
      "Epoch: 1111\tTrain Loss: 0.3961\tVal Loss: 0.3960\n",
      "Epoch: 1112\tTrain Loss: 0.3960\tVal Loss: 0.3959\n",
      "Epoch: 1113\tTrain Loss: 0.3959\tVal Loss: 0.3958\n",
      "Epoch: 1114\tTrain Loss: 0.3959\tVal Loss: 0.3958\n",
      "Epoch: 1115\tTrain Loss: 0.3958\tVal Loss: 0.3957\n",
      "Epoch: 1116\tTrain Loss: 0.3957\tVal Loss: 0.3956\n",
      "Epoch: 1117\tTrain Loss: 0.3957\tVal Loss: 0.3955\n",
      "Epoch: 1118\tTrain Loss: 0.3956\tVal Loss: 0.3955\n",
      "Epoch: 1119\tTrain Loss: 0.3955\tVal Loss: 0.3954\n",
      "Epoch: 1120\tTrain Loss: 0.3955\tVal Loss: 0.3953\n",
      "Epoch: 1121\tTrain Loss: 0.3954\tVal Loss: 0.3953\n",
      "Epoch: 1122\tTrain Loss: 0.3953\tVal Loss: 0.3952\n",
      "Epoch: 1123\tTrain Loss: 0.3953\tVal Loss: 0.3951\n",
      "Epoch: 1124\tTrain Loss: 0.3952\tVal Loss: 0.3951\n",
      "Epoch: 1125\tTrain Loss: 0.3951\tVal Loss: 0.3950\n",
      "Epoch: 1126\tTrain Loss: 0.3951\tVal Loss: 0.3949\n",
      "Epoch: 1127\tTrain Loss: 0.3950\tVal Loss: 0.3949\n",
      "Epoch: 1128\tTrain Loss: 0.3949\tVal Loss: 0.3948\n",
      "Epoch: 1129\tTrain Loss: 0.3949\tVal Loss: 0.3947\n",
      "Epoch: 1130\tTrain Loss: 0.3948\tVal Loss: 0.3947\n",
      "Epoch: 1131\tTrain Loss: 0.3947\tVal Loss: 0.3946\n",
      "Epoch: 1132\tTrain Loss: 0.3947\tVal Loss: 0.3946\n",
      "Epoch: 1133\tTrain Loss: 0.3946\tVal Loss: 0.3945\n",
      "Epoch: 1134\tTrain Loss: 0.3945\tVal Loss: 0.3944\n",
      "Epoch: 1135\tTrain Loss: 0.3945\tVal Loss: 0.3944\n",
      "Epoch: 1136\tTrain Loss: 0.3944\tVal Loss: 0.3943\n",
      "Epoch: 1137\tTrain Loss: 0.3943\tVal Loss: 0.3942\n",
      "Epoch: 1138\tTrain Loss: 0.3943\tVal Loss: 0.3942\n",
      "Epoch: 1139\tTrain Loss: 0.3942\tVal Loss: 0.3941\n",
      "Epoch: 1140\tTrain Loss: 0.3941\tVal Loss: 0.3940\n",
      "Epoch: 1141\tTrain Loss: 0.3941\tVal Loss: 0.3940\n",
      "Epoch: 1142\tTrain Loss: 0.3940\tVal Loss: 0.3939\n",
      "Epoch: 1143\tTrain Loss: 0.3939\tVal Loss: 0.3938\n",
      "Epoch: 1144\tTrain Loss: 0.3939\tVal Loss: 0.3938\n",
      "Epoch: 1145\tTrain Loss: 0.3938\tVal Loss: 0.3937\n",
      "Epoch: 1146\tTrain Loss: 0.3937\tVal Loss: 0.3936\n",
      "Epoch: 1147\tTrain Loss: 0.3937\tVal Loss: 0.3936\n",
      "Epoch: 1148\tTrain Loss: 0.3936\tVal Loss: 0.3935\n",
      "Epoch: 1149\tTrain Loss: 0.3936\tVal Loss: 0.3934\n",
      "Epoch: 1150\tTrain Loss: 0.3935\tVal Loss: 0.3934\n",
      "Epoch: 1151\tTrain Loss: 0.3934\tVal Loss: 0.3933\n",
      "Epoch: 1152\tTrain Loss: 0.3934\tVal Loss: 0.3933\n",
      "Epoch: 1153\tTrain Loss: 0.3933\tVal Loss: 0.3932\n",
      "Epoch: 1154\tTrain Loss: 0.3932\tVal Loss: 0.3931\n",
      "Epoch: 1155\tTrain Loss: 0.3932\tVal Loss: 0.3931\n",
      "Epoch: 1156\tTrain Loss: 0.3931\tVal Loss: 0.3930\n",
      "Epoch: 1157\tTrain Loss: 0.3930\tVal Loss: 0.3929\n",
      "Epoch: 1158\tTrain Loss: 0.3930\tVal Loss: 0.3929\n",
      "Epoch: 1159\tTrain Loss: 0.3929\tVal Loss: 0.3928\n",
      "Epoch: 1160\tTrain Loss: 0.3929\tVal Loss: 0.3927\n",
      "Epoch: 1161\tTrain Loss: 0.3928\tVal Loss: 0.3927\n",
      "Epoch: 1162\tTrain Loss: 0.3927\tVal Loss: 0.3926\n",
      "Epoch: 1163\tTrain Loss: 0.3927\tVal Loss: 0.3926\n",
      "Epoch: 1164\tTrain Loss: 0.3926\tVal Loss: 0.3925\n",
      "Epoch: 1165\tTrain Loss: 0.3925\tVal Loss: 0.3924\n",
      "Epoch: 1166\tTrain Loss: 0.3925\tVal Loss: 0.3924\n",
      "Epoch: 1167\tTrain Loss: 0.3924\tVal Loss: 0.3923\n",
      "Epoch: 1168\tTrain Loss: 0.3923\tVal Loss: 0.3922\n",
      "Epoch: 1169\tTrain Loss: 0.3923\tVal Loss: 0.3922\n",
      "Epoch: 1170\tTrain Loss: 0.3922\tVal Loss: 0.3921\n",
      "Epoch: 1171\tTrain Loss: 0.3922\tVal Loss: 0.3921\n",
      "Epoch: 1172\tTrain Loss: 0.3921\tVal Loss: 0.3920\n",
      "Epoch: 1173\tTrain Loss: 0.3920\tVal Loss: 0.3919\n",
      "Epoch: 1174\tTrain Loss: 0.3920\tVal Loss: 0.3919\n",
      "Epoch: 1175\tTrain Loss: 0.3919\tVal Loss: 0.3918\n",
      "Epoch: 1176\tTrain Loss: 0.3919\tVal Loss: 0.3917\n",
      "Epoch: 1177\tTrain Loss: 0.3918\tVal Loss: 0.3917\n",
      "Epoch: 1178\tTrain Loss: 0.3917\tVal Loss: 0.3916\n",
      "Epoch: 1179\tTrain Loss: 0.3917\tVal Loss: 0.3916\n",
      "Epoch: 1180\tTrain Loss: 0.3916\tVal Loss: 0.3915\n",
      "Epoch: 1181\tTrain Loss: 0.3915\tVal Loss: 0.3914\n",
      "Epoch: 1182\tTrain Loss: 0.3915\tVal Loss: 0.3914\n",
      "Epoch: 1183\tTrain Loss: 0.3914\tVal Loss: 0.3913\n",
      "Epoch: 1184\tTrain Loss: 0.3914\tVal Loss: 0.3913\n",
      "Epoch: 1185\tTrain Loss: 0.3913\tVal Loss: 0.3912\n",
      "Epoch: 1186\tTrain Loss: 0.3912\tVal Loss: 0.3911\n",
      "Epoch: 1187\tTrain Loss: 0.3912\tVal Loss: 0.3911\n",
      "Epoch: 1188\tTrain Loss: 0.3911\tVal Loss: 0.3910\n",
      "Epoch: 1189\tTrain Loss: 0.3911\tVal Loss: 0.3910\n",
      "Epoch: 1190\tTrain Loss: 0.3910\tVal Loss: 0.3909\n",
      "Epoch: 1191\tTrain Loss: 0.3909\tVal Loss: 0.3908\n",
      "Epoch: 1192\tTrain Loss: 0.3909\tVal Loss: 0.3908\n",
      "Epoch: 1193\tTrain Loss: 0.3908\tVal Loss: 0.3907\n",
      "Epoch: 1194\tTrain Loss: 0.3908\tVal Loss: 0.3907\n",
      "Epoch: 1195\tTrain Loss: 0.3907\tVal Loss: 0.3906\n",
      "Epoch: 1196\tTrain Loss: 0.3906\tVal Loss: 0.3905\n",
      "Epoch: 1197\tTrain Loss: 0.3906\tVal Loss: 0.3905\n",
      "Epoch: 1198\tTrain Loss: 0.3905\tVal Loss: 0.3904\n",
      "Epoch: 1199\tTrain Loss: 0.3905\tVal Loss: 0.3904\n",
      "Epoch: 1200\tTrain Loss: 0.3904\tVal Loss: 0.3903\n",
      "Epoch: 1201\tTrain Loss: 0.3903\tVal Loss: 0.3902\n",
      "Epoch: 1202\tTrain Loss: 0.3903\tVal Loss: 0.3902\n",
      "Epoch: 1203\tTrain Loss: 0.3902\tVal Loss: 0.3901\n",
      "Epoch: 1204\tTrain Loss: 0.3902\tVal Loss: 0.3901\n",
      "Epoch: 1205\tTrain Loss: 0.3901\tVal Loss: 0.3900\n",
      "Epoch: 1206\tTrain Loss: 0.3900\tVal Loss: 0.3899\n",
      "Epoch: 1207\tTrain Loss: 0.3900\tVal Loss: 0.3899\n",
      "Epoch: 1208\tTrain Loss: 0.3899\tVal Loss: 0.3898\n",
      "Epoch: 1209\tTrain Loss: 0.3899\tVal Loss: 0.3898\n",
      "Epoch: 1210\tTrain Loss: 0.3898\tVal Loss: 0.3897\n",
      "Epoch: 1211\tTrain Loss: 0.3898\tVal Loss: 0.3897\n",
      "Epoch: 1212\tTrain Loss: 0.3897\tVal Loss: 0.3896\n",
      "Epoch: 1213\tTrain Loss: 0.3896\tVal Loss: 0.3895\n",
      "Epoch: 1214\tTrain Loss: 0.3896\tVal Loss: 0.3895\n",
      "Epoch: 1215\tTrain Loss: 0.3895\tVal Loss: 0.3894\n",
      "Epoch: 1216\tTrain Loss: 0.3895\tVal Loss: 0.3894\n",
      "Epoch: 1217\tTrain Loss: 0.3894\tVal Loss: 0.3893\n",
      "Epoch: 1218\tTrain Loss: 0.3893\tVal Loss: 0.3892\n",
      "Epoch: 1219\tTrain Loss: 0.3893\tVal Loss: 0.3892\n",
      "Epoch: 1220\tTrain Loss: 0.3892\tVal Loss: 0.3891\n",
      "Epoch: 1221\tTrain Loss: 0.3892\tVal Loss: 0.3891\n",
      "Epoch: 1222\tTrain Loss: 0.3891\tVal Loss: 0.3890\n",
      "Epoch: 1223\tTrain Loss: 0.3891\tVal Loss: 0.3890\n",
      "Epoch: 1224\tTrain Loss: 0.3890\tVal Loss: 0.3889\n",
      "Epoch: 1225\tTrain Loss: 0.3889\tVal Loss: 0.3888\n",
      "Epoch: 1226\tTrain Loss: 0.3889\tVal Loss: 0.3888\n",
      "Epoch: 1227\tTrain Loss: 0.3888\tVal Loss: 0.3887\n",
      "Epoch: 1228\tTrain Loss: 0.3888\tVal Loss: 0.3887\n",
      "Epoch: 1229\tTrain Loss: 0.3887\tVal Loss: 0.3886\n",
      "Epoch: 1230\tTrain Loss: 0.3887\tVal Loss: 0.3886\n",
      "Epoch: 1231\tTrain Loss: 0.3886\tVal Loss: 0.3885\n",
      "Epoch: 1232\tTrain Loss: 0.3885\tVal Loss: 0.3884\n",
      "Epoch: 1233\tTrain Loss: 0.3885\tVal Loss: 0.3884\n",
      "Epoch: 1234\tTrain Loss: 0.3884\tVal Loss: 0.3883\n",
      "Epoch: 1235\tTrain Loss: 0.3884\tVal Loss: 0.3883\n",
      "Epoch: 1236\tTrain Loss: 0.3883\tVal Loss: 0.3882\n",
      "Epoch: 1237\tTrain Loss: 0.3883\tVal Loss: 0.3882\n",
      "Epoch: 1238\tTrain Loss: 0.3882\tVal Loss: 0.3881\n",
      "Epoch: 1239\tTrain Loss: 0.3881\tVal Loss: 0.3881\n",
      "Epoch: 1240\tTrain Loss: 0.3881\tVal Loss: 0.3880\n",
      "Epoch: 1241\tTrain Loss: 0.3880\tVal Loss: 0.3879\n",
      "Epoch: 1242\tTrain Loss: 0.3880\tVal Loss: 0.3879\n",
      "Epoch: 1243\tTrain Loss: 0.3879\tVal Loss: 0.3878\n",
      "Epoch: 1244\tTrain Loss: 0.3879\tVal Loss: 0.3878\n",
      "Epoch: 1245\tTrain Loss: 0.3878\tVal Loss: 0.3877\n",
      "Epoch: 1246\tTrain Loss: 0.3878\tVal Loss: 0.3877\n",
      "Epoch: 1247\tTrain Loss: 0.3877\tVal Loss: 0.3876\n",
      "Epoch: 1248\tTrain Loss: 0.3876\tVal Loss: 0.3876\n",
      "Epoch: 1249\tTrain Loss: 0.3876\tVal Loss: 0.3875\n",
      "Epoch: 1250\tTrain Loss: 0.3875\tVal Loss: 0.3874\n",
      "Epoch: 1251\tTrain Loss: 0.3875\tVal Loss: 0.3874\n",
      "Epoch: 1252\tTrain Loss: 0.3874\tVal Loss: 0.3873\n",
      "Epoch: 1253\tTrain Loss: 0.3874\tVal Loss: 0.3873\n",
      "Epoch: 1254\tTrain Loss: 0.3873\tVal Loss: 0.3872\n",
      "Epoch: 1255\tTrain Loss: 0.3873\tVal Loss: 0.3872\n",
      "Epoch: 1256\tTrain Loss: 0.3872\tVal Loss: 0.3871\n",
      "Epoch: 1257\tTrain Loss: 0.3872\tVal Loss: 0.3871\n",
      "Epoch: 1258\tTrain Loss: 0.3871\tVal Loss: 0.3870\n",
      "Epoch: 1259\tTrain Loss: 0.3870\tVal Loss: 0.3870\n",
      "Epoch: 1260\tTrain Loss: 0.3870\tVal Loss: 0.3869\n",
      "Epoch: 1261\tTrain Loss: 0.3869\tVal Loss: 0.3868\n",
      "Epoch: 1262\tTrain Loss: 0.3869\tVal Loss: 0.3868\n",
      "Epoch: 1263\tTrain Loss: 0.3868\tVal Loss: 0.3867\n",
      "Epoch: 1264\tTrain Loss: 0.3868\tVal Loss: 0.3867\n",
      "Epoch: 1265\tTrain Loss: 0.3867\tVal Loss: 0.3866\n",
      "Epoch: 1266\tTrain Loss: 0.3867\tVal Loss: 0.3866\n",
      "Epoch: 1267\tTrain Loss: 0.3866\tVal Loss: 0.3865\n",
      "Epoch: 1268\tTrain Loss: 0.3866\tVal Loss: 0.3865\n",
      "Epoch: 1269\tTrain Loss: 0.3865\tVal Loss: 0.3864\n",
      "Epoch: 1270\tTrain Loss: 0.3865\tVal Loss: 0.3864\n",
      "Epoch: 1271\tTrain Loss: 0.3864\tVal Loss: 0.3863\n",
      "Epoch: 1272\tTrain Loss: 0.3863\tVal Loss: 0.3863\n",
      "Epoch: 1273\tTrain Loss: 0.3863\tVal Loss: 0.3862\n",
      "Epoch: 1274\tTrain Loss: 0.3862\tVal Loss: 0.3861\n",
      "Epoch: 1275\tTrain Loss: 0.3862\tVal Loss: 0.3861\n",
      "Epoch: 1276\tTrain Loss: 0.3861\tVal Loss: 0.3860\n",
      "Epoch: 1277\tTrain Loss: 0.3861\tVal Loss: 0.3860\n",
      "Epoch: 1278\tTrain Loss: 0.3860\tVal Loss: 0.3859\n",
      "Epoch: 1279\tTrain Loss: 0.3860\tVal Loss: 0.3859\n",
      "Epoch: 1280\tTrain Loss: 0.3859\tVal Loss: 0.3858\n",
      "Epoch: 1281\tTrain Loss: 0.3859\tVal Loss: 0.3858\n",
      "Epoch: 1282\tTrain Loss: 0.3858\tVal Loss: 0.3857\n",
      "Epoch: 1283\tTrain Loss: 0.3858\tVal Loss: 0.3857\n",
      "Epoch: 1284\tTrain Loss: 0.3857\tVal Loss: 0.3856\n",
      "Epoch: 1285\tTrain Loss: 0.3857\tVal Loss: 0.3856\n",
      "Epoch: 1286\tTrain Loss: 0.3856\tVal Loss: 0.3855\n",
      "Epoch: 1287\tTrain Loss: 0.3856\tVal Loss: 0.3855\n",
      "Epoch: 1288\tTrain Loss: 0.3855\tVal Loss: 0.3854\n",
      "Epoch: 1289\tTrain Loss: 0.3854\tVal Loss: 0.3854\n",
      "Epoch: 1290\tTrain Loss: 0.3854\tVal Loss: 0.3853\n",
      "Epoch: 1291\tTrain Loss: 0.3853\tVal Loss: 0.3853\n",
      "Epoch: 1292\tTrain Loss: 0.3853\tVal Loss: 0.3852\n",
      "Epoch: 1293\tTrain Loss: 0.3852\tVal Loss: 0.3852\n",
      "Epoch: 1294\tTrain Loss: 0.3852\tVal Loss: 0.3851\n",
      "Epoch: 1295\tTrain Loss: 0.3851\tVal Loss: 0.3850\n",
      "Epoch: 1296\tTrain Loss: 0.3851\tVal Loss: 0.3850\n",
      "Epoch: 1297\tTrain Loss: 0.3850\tVal Loss: 0.3849\n",
      "Epoch: 1298\tTrain Loss: 0.3850\tVal Loss: 0.3849\n",
      "Epoch: 1299\tTrain Loss: 0.3849\tVal Loss: 0.3848\n",
      "Epoch: 1300\tTrain Loss: 0.3849\tVal Loss: 0.3848\n",
      "Epoch: 1301\tTrain Loss: 0.3848\tVal Loss: 0.3847\n",
      "Epoch: 1302\tTrain Loss: 0.3848\tVal Loss: 0.3847\n",
      "Epoch: 1303\tTrain Loss: 0.3847\tVal Loss: 0.3846\n",
      "Epoch: 1304\tTrain Loss: 0.3847\tVal Loss: 0.3846\n",
      "Epoch: 1305\tTrain Loss: 0.3846\tVal Loss: 0.3845\n",
      "Epoch: 1306\tTrain Loss: 0.3846\tVal Loss: 0.3845\n",
      "Epoch: 1307\tTrain Loss: 0.3845\tVal Loss: 0.3844\n",
      "Epoch: 1308\tTrain Loss: 0.3845\tVal Loss: 0.3844\n",
      "Epoch: 1309\tTrain Loss: 0.3844\tVal Loss: 0.3843\n",
      "Epoch: 1310\tTrain Loss: 0.3844\tVal Loss: 0.3843\n",
      "Epoch: 1311\tTrain Loss: 0.3843\tVal Loss: 0.3842\n",
      "Epoch: 1312\tTrain Loss: 0.3843\tVal Loss: 0.3842\n",
      "Epoch: 1313\tTrain Loss: 0.3842\tVal Loss: 0.3841\n",
      "Epoch: 1314\tTrain Loss: 0.3842\tVal Loss: 0.3841\n",
      "Epoch: 1315\tTrain Loss: 0.3841\tVal Loss: 0.3840\n",
      "Epoch: 1316\tTrain Loss: 0.3841\tVal Loss: 0.3840\n",
      "Epoch: 1317\tTrain Loss: 0.3840\tVal Loss: 0.3839\n",
      "Epoch: 1318\tTrain Loss: 0.3840\tVal Loss: 0.3839\n",
      "Epoch: 1319\tTrain Loss: 0.3839\tVal Loss: 0.3838\n",
      "Epoch: 1320\tTrain Loss: 0.3839\tVal Loss: 0.3838\n",
      "Epoch: 1321\tTrain Loss: 0.3838\tVal Loss: 0.3837\n",
      "Epoch: 1322\tTrain Loss: 0.3838\tVal Loss: 0.3837\n",
      "Epoch: 1323\tTrain Loss: 0.3837\tVal Loss: 0.3836\n",
      "Epoch: 1324\tTrain Loss: 0.3837\tVal Loss: 0.3836\n",
      "Epoch: 1325\tTrain Loss: 0.3836\tVal Loss: 0.3835\n",
      "Epoch: 1326\tTrain Loss: 0.3836\tVal Loss: 0.3835\n",
      "Epoch: 1327\tTrain Loss: 0.3835\tVal Loss: 0.3834\n",
      "Epoch: 1328\tTrain Loss: 0.3835\tVal Loss: 0.3834\n",
      "Epoch: 1329\tTrain Loss: 0.3834\tVal Loss: 0.3833\n",
      "Epoch: 1330\tTrain Loss: 0.3834\tVal Loss: 0.3833\n",
      "Epoch: 1331\tTrain Loss: 0.3833\tVal Loss: 0.3832\n",
      "Epoch: 1332\tTrain Loss: 0.3833\tVal Loss: 0.3832\n",
      "Epoch: 1333\tTrain Loss: 0.3832\tVal Loss: 0.3831\n",
      "Epoch: 1334\tTrain Loss: 0.3832\tVal Loss: 0.3831\n",
      "Epoch: 1335\tTrain Loss: 0.3831\tVal Loss: 0.3830\n",
      "Epoch: 1336\tTrain Loss: 0.3831\tVal Loss: 0.3830\n",
      "Epoch: 1337\tTrain Loss: 0.3830\tVal Loss: 0.3830\n",
      "Epoch: 1338\tTrain Loss: 0.3830\tVal Loss: 0.3829\n",
      "Epoch: 1339\tTrain Loss: 0.3829\tVal Loss: 0.3829\n",
      "Epoch: 1340\tTrain Loss: 0.3829\tVal Loss: 0.3828\n",
      "Epoch: 1341\tTrain Loss: 0.3828\tVal Loss: 0.3828\n",
      "Epoch: 1342\tTrain Loss: 0.3828\tVal Loss: 0.3827\n",
      "Epoch: 1343\tTrain Loss: 0.3827\tVal Loss: 0.3827\n",
      "Epoch: 1344\tTrain Loss: 0.3827\tVal Loss: 0.3826\n",
      "Epoch: 1345\tTrain Loss: 0.3827\tVal Loss: 0.3826\n",
      "Epoch: 1346\tTrain Loss: 0.3826\tVal Loss: 0.3825\n",
      "Epoch: 1347\tTrain Loss: 0.3826\tVal Loss: 0.3825\n",
      "Epoch: 1348\tTrain Loss: 0.3825\tVal Loss: 0.3824\n",
      "Epoch: 1349\tTrain Loss: 0.3825\tVal Loss: 0.3824\n",
      "Epoch: 1350\tTrain Loss: 0.3824\tVal Loss: 0.3823\n",
      "Epoch: 1351\tTrain Loss: 0.3824\tVal Loss: 0.3823\n",
      "Epoch: 1352\tTrain Loss: 0.3823\tVal Loss: 0.3822\n",
      "Epoch: 1353\tTrain Loss: 0.3823\tVal Loss: 0.3822\n",
      "Epoch: 1354\tTrain Loss: 0.3822\tVal Loss: 0.3821\n",
      "Epoch: 1355\tTrain Loss: 0.3822\tVal Loss: 0.3821\n",
      "Epoch: 1356\tTrain Loss: 0.3821\tVal Loss: 0.3820\n",
      "Epoch: 1357\tTrain Loss: 0.3821\tVal Loss: 0.3820\n",
      "Epoch: 1358\tTrain Loss: 0.3820\tVal Loss: 0.3820\n",
      "Epoch: 1359\tTrain Loss: 0.3820\tVal Loss: 0.3819\n",
      "Epoch: 1360\tTrain Loss: 0.3819\tVal Loss: 0.3819\n",
      "Epoch: 1361\tTrain Loss: 0.3819\tVal Loss: 0.3818\n",
      "Epoch: 1362\tTrain Loss: 0.3818\tVal Loss: 0.3818\n",
      "Epoch: 1363\tTrain Loss: 0.3818\tVal Loss: 0.3817\n",
      "Epoch: 1364\tTrain Loss: 0.3818\tVal Loss: 0.3817\n",
      "Epoch: 1365\tTrain Loss: 0.3817\tVal Loss: 0.3816\n",
      "Epoch: 1366\tTrain Loss: 0.3817\tVal Loss: 0.3816\n",
      "Epoch: 1367\tTrain Loss: 0.3816\tVal Loss: 0.3815\n",
      "Epoch: 1368\tTrain Loss: 0.3816\tVal Loss: 0.3815\n",
      "Epoch: 1369\tTrain Loss: 0.3815\tVal Loss: 0.3814\n",
      "Epoch: 1370\tTrain Loss: 0.3815\tVal Loss: 0.3814\n",
      "Epoch: 1371\tTrain Loss: 0.3814\tVal Loss: 0.3813\n",
      "Epoch: 1372\tTrain Loss: 0.3814\tVal Loss: 0.3813\n",
      "Epoch: 1373\tTrain Loss: 0.3813\tVal Loss: 0.3813\n",
      "Epoch: 1374\tTrain Loss: 0.3813\tVal Loss: 0.3812\n",
      "Epoch: 1375\tTrain Loss: 0.3812\tVal Loss: 0.3812\n",
      "Epoch: 1376\tTrain Loss: 0.3812\tVal Loss: 0.3811\n",
      "Epoch: 1377\tTrain Loss: 0.3812\tVal Loss: 0.3811\n",
      "Epoch: 1378\tTrain Loss: 0.3811\tVal Loss: 0.3810\n",
      "Epoch: 1379\tTrain Loss: 0.3811\tVal Loss: 0.3810\n",
      "Epoch: 1380\tTrain Loss: 0.3810\tVal Loss: 0.3809\n",
      "Epoch: 1381\tTrain Loss: 0.3810\tVal Loss: 0.3809\n",
      "Epoch: 1382\tTrain Loss: 0.3809\tVal Loss: 0.3808\n",
      "Epoch: 1383\tTrain Loss: 0.3809\tVal Loss: 0.3808\n",
      "Epoch: 1384\tTrain Loss: 0.3808\tVal Loss: 0.3808\n",
      "Epoch: 1385\tTrain Loss: 0.3808\tVal Loss: 0.3807\n",
      "Epoch: 1386\tTrain Loss: 0.3807\tVal Loss: 0.3807\n",
      "Epoch: 1387\tTrain Loss: 0.3807\tVal Loss: 0.3806\n",
      "Epoch: 1388\tTrain Loss: 0.3807\tVal Loss: 0.3806\n",
      "Epoch: 1389\tTrain Loss: 0.3806\tVal Loss: 0.3805\n",
      "Epoch: 1390\tTrain Loss: 0.3806\tVal Loss: 0.3805\n",
      "Epoch: 1391\tTrain Loss: 0.3805\tVal Loss: 0.3804\n",
      "Epoch: 1392\tTrain Loss: 0.3805\tVal Loss: 0.3804\n",
      "Epoch: 1393\tTrain Loss: 0.3804\tVal Loss: 0.3803\n",
      "Epoch: 1394\tTrain Loss: 0.3804\tVal Loss: 0.3803\n",
      "Epoch: 1395\tTrain Loss: 0.3803\tVal Loss: 0.3803\n",
      "Epoch: 1396\tTrain Loss: 0.3803\tVal Loss: 0.3802\n",
      "Epoch: 1397\tTrain Loss: 0.3803\tVal Loss: 0.3802\n",
      "Epoch: 1398\tTrain Loss: 0.3802\tVal Loss: 0.3801\n",
      "Epoch: 1399\tTrain Loss: 0.3802\tVal Loss: 0.3801\n",
      "Epoch: 1400\tTrain Loss: 0.3801\tVal Loss: 0.3800\n",
      "Epoch: 1401\tTrain Loss: 0.3801\tVal Loss: 0.3800\n",
      "Epoch: 1402\tTrain Loss: 0.3800\tVal Loss: 0.3800\n",
      "Epoch: 1403\tTrain Loss: 0.3800\tVal Loss: 0.3799\n",
      "Epoch: 1404\tTrain Loss: 0.3799\tVal Loss: 0.3799\n",
      "Epoch: 1405\tTrain Loss: 0.3799\tVal Loss: 0.3798\n",
      "Epoch: 1406\tTrain Loss: 0.3799\tVal Loss: 0.3798\n",
      "Epoch: 1407\tTrain Loss: 0.3798\tVal Loss: 0.3797\n",
      "Epoch: 1408\tTrain Loss: 0.3798\tVal Loss: 0.3797\n",
      "Epoch: 1409\tTrain Loss: 0.3797\tVal Loss: 0.3796\n",
      "Epoch: 1410\tTrain Loss: 0.3797\tVal Loss: 0.3796\n",
      "Epoch: 1411\tTrain Loss: 0.3796\tVal Loss: 0.3796\n",
      "Epoch: 1412\tTrain Loss: 0.3796\tVal Loss: 0.3795\n",
      "Epoch: 1413\tTrain Loss: 0.3795\tVal Loss: 0.3795\n",
      "Epoch: 1414\tTrain Loss: 0.3795\tVal Loss: 0.3794\n",
      "Epoch: 1415\tTrain Loss: 0.3795\tVal Loss: 0.3794\n",
      "Epoch: 1416\tTrain Loss: 0.3794\tVal Loss: 0.3793\n",
      "Epoch: 1417\tTrain Loss: 0.3794\tVal Loss: 0.3793\n",
      "Epoch: 1418\tTrain Loss: 0.3793\tVal Loss: 0.3793\n",
      "Epoch: 1419\tTrain Loss: 0.3793\tVal Loss: 0.3792\n",
      "Epoch: 1420\tTrain Loss: 0.3792\tVal Loss: 0.3792\n",
      "Epoch: 1421\tTrain Loss: 0.3792\tVal Loss: 0.3791\n",
      "Epoch: 1422\tTrain Loss: 0.3792\tVal Loss: 0.3791\n",
      "Epoch: 1423\tTrain Loss: 0.3791\tVal Loss: 0.3790\n",
      "Epoch: 1424\tTrain Loss: 0.3791\tVal Loss: 0.3790\n",
      "Epoch: 1425\tTrain Loss: 0.3790\tVal Loss: 0.3790\n",
      "Epoch: 1426\tTrain Loss: 0.3790\tVal Loss: 0.3789\n",
      "Epoch: 1427\tTrain Loss: 0.3789\tVal Loss: 0.3789\n",
      "Epoch: 1428\tTrain Loss: 0.3789\tVal Loss: 0.3788\n",
      "Epoch: 1429\tTrain Loss: 0.3789\tVal Loss: 0.3788\n",
      "Epoch: 1430\tTrain Loss: 0.3788\tVal Loss: 0.3787\n",
      "Epoch: 1431\tTrain Loss: 0.3788\tVal Loss: 0.3787\n",
      "Epoch: 1432\tTrain Loss: 0.3787\tVal Loss: 0.3787\n",
      "Epoch: 1433\tTrain Loss: 0.3787\tVal Loss: 0.3786\n",
      "Epoch: 1434\tTrain Loss: 0.3786\tVal Loss: 0.3786\n",
      "Epoch: 1435\tTrain Loss: 0.3786\tVal Loss: 0.3785\n",
      "Epoch: 1436\tTrain Loss: 0.3786\tVal Loss: 0.3785\n",
      "Epoch: 1437\tTrain Loss: 0.3785\tVal Loss: 0.3784\n",
      "Epoch: 1438\tTrain Loss: 0.3785\tVal Loss: 0.3784\n",
      "Epoch: 1439\tTrain Loss: 0.3784\tVal Loss: 0.3784\n",
      "Epoch: 1440\tTrain Loss: 0.3784\tVal Loss: 0.3783\n",
      "Epoch: 1441\tTrain Loss: 0.3784\tVal Loss: 0.3783\n",
      "Epoch: 1442\tTrain Loss: 0.3783\tVal Loss: 0.3782\n",
      "Epoch: 1443\tTrain Loss: 0.3783\tVal Loss: 0.3782\n",
      "Epoch: 1444\tTrain Loss: 0.3782\tVal Loss: 0.3782\n",
      "Epoch: 1445\tTrain Loss: 0.3782\tVal Loss: 0.3781\n",
      "Epoch: 1446\tTrain Loss: 0.3781\tVal Loss: 0.3781\n",
      "Epoch: 1447\tTrain Loss: 0.3781\tVal Loss: 0.3780\n",
      "Epoch: 1448\tTrain Loss: 0.3781\tVal Loss: 0.3780\n",
      "Epoch: 1449\tTrain Loss: 0.3780\tVal Loss: 0.3779\n",
      "Epoch: 1450\tTrain Loss: 0.3780\tVal Loss: 0.3779\n",
      "Epoch: 1451\tTrain Loss: 0.3779\tVal Loss: 0.3779\n",
      "Epoch: 1452\tTrain Loss: 0.3779\tVal Loss: 0.3778\n",
      "Epoch: 1453\tTrain Loss: 0.3779\tVal Loss: 0.3778\n",
      "Epoch: 1454\tTrain Loss: 0.3778\tVal Loss: 0.3777\n",
      "Epoch: 1455\tTrain Loss: 0.3778\tVal Loss: 0.3777\n",
      "Epoch: 1456\tTrain Loss: 0.3777\tVal Loss: 0.3777\n",
      "Epoch: 1457\tTrain Loss: 0.3777\tVal Loss: 0.3776\n",
      "Epoch: 1458\tTrain Loss: 0.3777\tVal Loss: 0.3776\n",
      "Epoch: 1459\tTrain Loss: 0.3776\tVal Loss: 0.3775\n",
      "Epoch: 1460\tTrain Loss: 0.3776\tVal Loss: 0.3775\n",
      "Epoch: 1461\tTrain Loss: 0.3775\tVal Loss: 0.3775\n",
      "Epoch: 1462\tTrain Loss: 0.3775\tVal Loss: 0.3774\n",
      "Epoch: 1463\tTrain Loss: 0.3774\tVal Loss: 0.3774\n",
      "Epoch: 1464\tTrain Loss: 0.3774\tVal Loss: 0.3773\n",
      "Epoch: 1465\tTrain Loss: 0.3774\tVal Loss: 0.3773\n",
      "Epoch: 1466\tTrain Loss: 0.3773\tVal Loss: 0.3773\n",
      "Epoch: 1467\tTrain Loss: 0.3773\tVal Loss: 0.3772\n",
      "Epoch: 1468\tTrain Loss: 0.3772\tVal Loss: 0.3772\n",
      "Epoch: 1469\tTrain Loss: 0.3772\tVal Loss: 0.3771\n",
      "Epoch: 1470\tTrain Loss: 0.3772\tVal Loss: 0.3771\n",
      "Epoch: 1471\tTrain Loss: 0.3771\tVal Loss: 0.3771\n",
      "Epoch: 1472\tTrain Loss: 0.3771\tVal Loss: 0.3770\n",
      "Epoch: 1473\tTrain Loss: 0.3770\tVal Loss: 0.3770\n",
      "Epoch: 1474\tTrain Loss: 0.3770\tVal Loss: 0.3769\n",
      "Epoch: 1475\tTrain Loss: 0.3770\tVal Loss: 0.3769\n",
      "Epoch: 1476\tTrain Loss: 0.3769\tVal Loss: 0.3769\n",
      "Epoch: 1477\tTrain Loss: 0.3769\tVal Loss: 0.3768\n",
      "Epoch: 1478\tTrain Loss: 0.3768\tVal Loss: 0.3768\n",
      "Epoch: 1479\tTrain Loss: 0.3768\tVal Loss: 0.3767\n",
      "Epoch: 1480\tTrain Loss: 0.3768\tVal Loss: 0.3767\n",
      "Epoch: 1481\tTrain Loss: 0.3767\tVal Loss: 0.3767\n",
      "Epoch: 1482\tTrain Loss: 0.3767\tVal Loss: 0.3766\n",
      "Epoch: 1483\tTrain Loss: 0.3766\tVal Loss: 0.3766\n",
      "Epoch: 1484\tTrain Loss: 0.3766\tVal Loss: 0.3765\n",
      "Epoch: 1485\tTrain Loss: 0.3766\tVal Loss: 0.3765\n",
      "Epoch: 1486\tTrain Loss: 0.3765\tVal Loss: 0.3765\n",
      "Epoch: 1487\tTrain Loss: 0.3765\tVal Loss: 0.3764\n",
      "Epoch: 1488\tTrain Loss: 0.3765\tVal Loss: 0.3764\n",
      "Epoch: 1489\tTrain Loss: 0.3764\tVal Loss: 0.3763\n",
      "Epoch: 1490\tTrain Loss: 0.3764\tVal Loss: 0.3763\n",
      "Epoch: 1491\tTrain Loss: 0.3763\tVal Loss: 0.3763\n",
      "Epoch: 1492\tTrain Loss: 0.3763\tVal Loss: 0.3762\n",
      "Epoch: 1493\tTrain Loss: 0.3763\tVal Loss: 0.3762\n",
      "Epoch: 1494\tTrain Loss: 0.3762\tVal Loss: 0.3761\n",
      "Epoch: 1495\tTrain Loss: 0.3762\tVal Loss: 0.3761\n",
      "Epoch: 1496\tTrain Loss: 0.3761\tVal Loss: 0.3761\n",
      "Epoch: 1497\tTrain Loss: 0.3761\tVal Loss: 0.3760\n",
      "Epoch: 1498\tTrain Loss: 0.3761\tVal Loss: 0.3760\n",
      "Epoch: 1499\tTrain Loss: 0.3760\tVal Loss: 0.3760\n"
     ]
    }
   ],
   "source": [
    "from BackwardPropagation import BackPropModel, regularized_loss, regularized_gradient\n",
    "model = BackPropModel()\n",
    "model.init_parameters(400, 10)\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "epochs = 1500\n",
    "lr = 2\n",
    "scale = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    output = model(train_x)\n",
    "    g_1, g_2 = regularized_gradient(model, output, train_y, scale=scale)\n",
    "    model.optimize([g_1, g_2],lr=lr)\n",
    "    loss = regularized_loss(output, train_y, [model.theta1, model.theta2], scale=scale)\n",
    "    train_loss.append(loss)\n",
    "    _loss = regularized_loss(model(val_x), val_y, [model.theta1,model.theta2], scale=scale)\n",
    "    val_loss.append(_loss)\n",
    "    print(\"Epoch: {}\\tTrain Loss: {:.4f}\\tVal Loss: {:.4f}\".format(epoch, loss, _loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练过程可视化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x800 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9wAAAK9CAYAAADWj2RWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAABk30lEQVR4nO3dd5xU1f3/8fedsrO9sLB0liK9BQENYo0oIBIFjYZgITHxS8QoRv2hiRqsWKIxasQSI2pErBhjVMSGDRVBEBAQ6UiTsjtbp97fH7M7sIJI2bl3Zu7r+Xjcx51bduYzexR433PuuYZpmqYAAAAAAECjctldAAAAAAAA6YjADQAAAABAAhC4AQAAAABIAAI3AAAAAAAJQOAGAAAAACABCNwAAAAAACQAgRsAAAAAgAQgcAMAAAAAkAAEbgAAAAAAEoDADQBAChg3bpzat29vdxkAAOAgELgBADgMhmEc0PLee+/ZXeo+bd26VVdddZW6deum7Oxs5eTkqH///rrllltUVlZmd3kAAKQ0wzRN0+4iAABIVf/+978bbD/55JOaPXu2nnrqqQb7TznlFDVv3vyQPycUCikajcrn8x3ye3zfvHnzdNppp6myslLnnXee+vfvL0n6/PPPNWPGDB1zzDF68803G+3zAABwGgI3AACN6NJLL9U//vEP/dhfr9XV1crOzraoqr2VlZWpV69eCofDeu+999StW7cGx7du3apHH31U11133WF/VlVVlXJycg77fQAASDUMKQcAIMFOPPFE9erVS/Pnz9fxxx+v7Oxs/elPf5Ik/ec//9GIESPUqlUr+Xw+derUSTfffLMikUiD9/j+Pdxr166VYRj661//qkceeUSdOnWSz+fTwIEDNW/evB+t6eGHH9a3336re+65Z6+wLUnNmzdvELYNw9DkyZP3Oq99+/YaN25cfHvatGkyDENz5szRJZdcopKSErVp00YvvPBCfP++ajEMQ0uWLInvW758uc4++2w1adJEmZmZGjBggF555ZUf/V4AACQTj90FAADgBDt27NDw4cP1y1/+Uuedd158ePm0adOUm5urP/7xj8rNzdU777yjG264QX6/X3fdddePvu/06dNVUVGh//u//5NhGLrzzjs1evRorV69Wl6v9wd/7pVXXlFWVpbOPvvsRvuOe7rkkkvUrFkz3XDDDaqqqtKIESOUm5ur5557TieccEKDc5999ln17NlTvXr1kiQtXbpUgwcPVuvWrXXNNdcoJydHzz33nM4880y9+OKLGjVqVEJqBgCgsRG4AQCwwJYtW/TQQw/p//7v/xrsnz59urKysuLb48eP1/jx4/Xggw/qlltu+dF7ttevX6+VK1eqqKhIktS1a1edccYZmjVrlk4//fQf/Llly5apS5cuysjIOIxv9cOaNGmit99+W263O75v5MiReuGFF3TffffF92/ZskVz5sxp0Ht++eWXq127dpo3b178+19yySU69thjNWnSJAI3ACBlMKQcAAAL+Hw+/frXv95r/55hu6KiQtu3b9dxxx2n6upqLV++/Eff99xzz42HbUk67rjjJEmrV6/e78/5/X7l5eUdaPkH7Xe/+12DsC3Fat22bVuDGdtfeOEFRaNRnXvuuZKknTt36p133tE555wT/31s375dO3bs0NChQ7Vy5Up9++23CasbAIDGRA83AAAWaN269T57k5cuXarrrrtO77zzjvx+f4Nj5eXlP/q+7dq1a7BdH7537dq135/Lz89XRUXFj77/oerQocNe+4YNG6aCggI9++yzOvnkkyXFhpP/5Cc/UZcuXSRJ33zzjUzT1PXXX6/rr79+n++9bds2tW7dOmG1AwDQWAjcAABYYM+e7HplZWU64YQTlJ+fr5tuukmdOnVSZmamFixYoEmTJikajf7o+36/F7nej82S3q1bNy1cuFDBYPCwhpV/f3K3evv6vj6fT2eeeaZmzpypBx98UFu3btVHH32k2267LX5O/Xe+6qqrNHTo0H2+9xFHHHHI9QIAYCUCNwAANnnvvfe0Y8cOvfTSSzr++OPj+9esWZPwzx45cqTmzp2rF198UWPGjPnR84uKilRWVtZgXzAY1ObNmw/qc88991w98cQTevvtt7Vs2TKZphkfTi5JHTt2lCR5vV4NGTLkoN4bAIBkwz3cAADYpL53es/e6GAwqAcffDDhnz1+/Hi1bNlSV155pb7++uu9jm/btk233HJLfLtTp056//33G5zzyCOP/GAP9w8ZMmSImjRpomeffVbPPvusjjrqqAbDz0tKSnTiiSfq4Ycf3meY/+677w7q8wAAsBM93AAA2OSYY45RUVGRLrzwQl122WUyDENPPfXUjw4HbwxFRUWaOXOmTjvtNP3kJz/Reeedp/79+0uSFixYoGeeeUaDBg2Kn//b3/5W48eP11lnnaVTTjlFixYt0qxZs9S0adOD+lyv16vRo0drxowZqqqq0l//+te9zvnHP/6hY489Vr1799bvfvc7dezYUVu3btXcuXO1ceNGLVq06PC+PAAAFiFwAwBgk+LiYr366qu68sordd1116moqEjnnXeeTj755B+8f7kxHX300VqyZInuuusu/e9//9NTTz0ll8ul7t2765prrtGll14aP/d3v/ud1qxZo8cee0xvvPGGjjvuOM2ePTs++dnBOPfcc/XPf/5ThmHonHPO2et4jx499Pnnn+vGG2/UtGnTtGPHDpWUlKhfv3664YYbDus7AwBgJcO04jI6AAAAAAAOwz3cAAAAAAAkAIEbAAAAAIAEIHADAAAAAJAABG4AAAAAABKAwA0AAAAAQAIQuAEAAAAASICUfg53NBrVpk2blJeXJ8Mw7C4HAAAAAJDmTNNURUWFWrVqJZdr/33YKR24N23apLZt29pdBgAAAADAYTZs2KA2bdrs95yUDtx5eXmSYl80Pz/f5moAAAAAAOnO7/erbdu28Ty6PykduOuHkefn5xO4AQAAAACWOZDbmpk0DQAAAACABCBwAwAAAACQAARuAAAAAAASIKXv4T4QpmkqHA4rEonYXQoOk9vtlsfj4RFwAAAAAFJCWgfuYDCozZs3q7q62u5S0Eiys7PVsmVLZWRk2F0KAAAAAOxX2gbuaDSqNWvWyO12q1WrVsrIyKBnNIWZpqlgMKjvvvtOa9asUefOnX/0IfMAAAAAYKe0DdzBYFDRaFRt27ZVdna23eWgEWRlZcnr9WrdunUKBoPKzMy0uyQAAAAA+EFp30VIL2h6oT0BAAAApArSCwAAAAAACUDgBgAAAAAgAQjcDtG+fXvde++9dpcBAAAAAI5B4E4yhmHsd5k8efIhve+8efN08cUXH1ZtJ554oiZOnHhY7wEAAAAATpG2s5Snqs2bN8dfP/vss7rhhhu0YsWK+L7c3Nz4a9M0FYlE5PH8eDM2a9ascQsFAAAAAOyXs3q4TVOqqrJ+Mc0DLrFFixbxpaCgQIZhxLeXL1+uvLw8vf766+rfv798Pp8+/PBDrVq1SmeccYaaN2+u3NxcDRw4UG+99VaD9/3+kHLDMPTPf/5To0aNUnZ2tjp37qxXXnnlsH69L774onr27Cmfz6f27dvr7rvvbnD8wQcfVOfOnZWZmanmzZvr7LPPjh974YUX1Lt3b2VlZam4uFhDhgxRVVXVYdUDAAAAAHZyVg93dbW0Rw+xZSorpZycRnu7a665Rn/961/VsWNHFRUVacOGDTrttNN06623yufz6cknn9TIkSO1YsUKtWvX7gff58Ybb9Sdd96pu+66S/fff7/Gjh2rdevWqUmTJgdd0/z583XOOedo8uTJOvfcc/Xxxx/rkksuUXFxscaNG6fPP/9cl112mZ566ikdc8wx2rlzpz744ANJsV79MWPG6M4779SoUaNUUVGhDz74QOZBXKgAAAAAgGTjrMCdJm666Sadcsop8e0mTZqob9++8e2bb75ZM2fO1CuvvKJLL730B99n3LhxGjNmjCTptttu03333afPPvtMw4YNO+ia7rnnHp188sm6/vrrJUldunTRV199pbvuukvjxo3T+vXrlZOTo9NPP115eXkqLS1Vv379JMUCdzgc1ujRo1VaWipJ6t2790HXAAAAAADJxFmBOzs71ttsx+c2ogEDBjTYrqys1OTJk/W///0vHl5ramq0fv36/b5Pnz594q9zcnKUn5+vbdu2HVJNy5Yt0xlnnNFg3+DBg3XvvfcqEonolFNOUWlpqTp27Khhw4Zp2LBh8eHsffv21cknn6zevXtr6NChOvXUU3X22WerqKjokGoBAAAAgGTgrHu4DSM2tNvqxTAa9WvkfG94+lVXXaWZM2fqtttu0wcffKCFCxeqd+/eCgaD+30fr9f7vV+PoWg02qi11svLy9OCBQv0zDPPqGXLlrrhhhvUt29flZWVye12a/bs2Xr99dfVo0cP3X///eratavWrFmTkFoAAAAAwArOCtxp6qOPPtK4ceM0atQo9e7dWy1atNDatWstraF79+766KOP9qqrS5cucrvdkiSPx6MhQ4bozjvv1Jdffqm1a9fqnXfekRQL+4MHD9aNN96oL774QhkZGZo5c6al3wEAAAAAGpOzhpSnqc6dO+ull17SyJEjZRiGrr/++oT1VH/33XdauHBhg30tW7bUlVdeqYEDB+rmm2/Wueeeq7lz5+qBBx7Qgw8+KEl69dVXtXr1ah1//PEqKirSa6+9pmg0qq5du+rTTz/V22+/rVNPPVUlJSX69NNP9d1336l79+4J+Q4AAAAAYAUCdxq455579Jvf/EbHHHOMmjZtqkmTJsnv9yfks6ZPn67p06c32HfzzTfruuuu03PPPacbbrhBN998s1q2bKmbbrpJ48aNkyQVFhbqpZde0uTJk1VbW6vOnTvrmWeeUc+ePbVs2TK9//77uvfee+X3+1VaWqq7775bw4cPT8h3AAAAAAArGGYKP3vJ7/eroKBA5eXlys/Pb3CstrZWa9asUYcOHZSZmWlThWhstCsAAAAAO+0vh34f93ADAAAAAJAADCm3QlWVFAxKWVkSvbIAAAAA4Aj0cFth2zZp1Spp1y67KwEAAAAAWITAbYVGfg43AAAAACD5EbgBAAAAAEgAAreVUndCeAAAAADAQSJwW4Eh5QAAAADgOARuAAAAAAASgMBthfoeboaUAwAAAIBjELjT1IknnqiJEyfaXQYAAAAAOBaBO8mMHDlSw4YN2+exDz74QIZh6Msvvzzsz5k2bZoKCwsP+30AAAAAAPtG4LbCQQwpv+iiizR79mxt3Lhxr2OPP/64BgwYoD59+jR2hQAAAACARuaowG2aUlWV9cvB3Lp9+umnq1mzZpo2bVqD/ZWVlXr++ed10UUXaceOHRozZoxat26t7Oxs9e7dW88880yj/q7Wr1+vM844Q7m5ucrPz9c555yjrVu3xo8vWrRIJ510kvLy8pSfn6/+/fvr888/lyStW7dOI0eOVFFRkXJyctSzZ0+99tprjVofAAAAACQ7j90FWKm6WsrNtf5zK5cbypEOKHl7PB5dcMEFmjZtmv785z/LqOsdf/755xWJRDRmzBhVVlaqf//+mjRpkvLz8/W///1P559/vjp16qSjjjrqsOuNRqPxsD1nzhyFw2FNmDBB5557rt577z1J0tixY9WvXz9NnTpVbrdbCxculNfrlSRNmDBBwWBQ77//vnJycvTVV18p145fPAAAAADYyFGB2zYH+Rzu3/zmN7rrrrs0Z84cnXjiiZJiw8nPOussFRQUqKCgQFdddVX8/D/84Q+aNWuWnnvuuUYJ3G+//bYWL16sNWvWqG3btpKkJ598Uj179tS8efM0cOBArV+/XldffbW6desmSercuXP859evX6+zzjpLvXv3liR17NjxsGsCAAAAgFTjqMCdnS1VVtrwuTtNyX/g53fr1k3HHHOM/vWvf+nEE0/UN998ow8++EA33XSTJCkSiei2227Tc889p2+//VbBYFCBQEDZ2dmNUu+yZcvUtm3beNiWpB49eqiwsFDLli3TwIED9cc//lG//e1v9dRTT2nIkCH6xS9+oU6dOkmSLrvsMv3+97/Xm2++qSFDhuiss87ivnMAAAAAjuOoe7gNQ8rJsX6Jd3AfxM3cF110kV588UVVVFTo8ccfV6dOnXTCCSdIku666y79/e9/16RJk/Tuu+9q4cKFGjp0qILBYAJ+a/s2efJkLV26VCNGjNA777yjHj16aObMmZKk3/72t1q9erXOP/98LV68WAMGDND9999vWW0AAAAAkAwcFbhtc5BDyiXpnHPOkcvl0vTp0/Xkk0/qN7/5Tfx+7o8++khnnHGGzjvvPPXt21cdO3bU119/3Wjldu/eXRs2bNCGDRvi+7766iuVlZWpR48e8X1dunTRFVdcoTfffFOjR4/W448/Hj/Wtm1bjR8/Xi+99JKuvPJKPfroo41WHwAAAACkAkcNKU8lubm5Ovfcc3XttdfK7/dr3Lhx8WOdO3fWCy+8oI8//lhFRUW65557tHXr1gZh+EBEIhEtXLiwwT6fz6chQ4aod+/eGjt2rO69916Fw2FdcsklOuGEEzRgwADV1NTo6quv1tlnn60OHTpo48aNmjdvns466yxJ0sSJEzV8+HB16dJFu3bt0rvvvqvu3bsf7q8EAAAAAFIKgdtKB/N8MMWGlT/22GM67bTT1KpVq/j+6667TqtXr9bQoUOVnZ2tiy++WGeeeabKy8sP6v0rKyvVr1+/Bvs6deqkb775Rv/5z3/0hz/8Qccff7xcLpeGDRsWHxbudru1Y8cOXXDBBdq6dauaNm2q0aNH68Ybb5QUC/ITJkzQxo0blZ+fr2HDhulvf/vbQdUGAAAAAKnOMM2DTIFJxO/3q6CgQOXl5crPz29wrLa2VmvWrFGHDh2UmZlpU4V1Nm2KLc2aSaWl9taS4pKqXQEAAAA4zv5y6PdxDzcAAAAAAAlA4LZC/aRpqTuYAAAAAABwkAjcAAAAAAAkAIEbAAAAAIAESPvAnRRzwjGkvNEkRXsCAAAAwAFI28Dt9XolSdXV1TZXgsZU35717QsAAAAAySptn8PtdrtVWFiobdu2SZKys7Nl1Pc0Wy0Uiq3DYam21p4aUpxpmqqurta2bdtUWFgot9ttd0kAAAAAsF9pG7glqUWLFpIUD9228fulXbukmppY6MYhKywsjLcrAAAAACSztA7chmGoZcuWKikpUai+l9kOTz4p3XabNGKEdPfd9tWR4rxeLz3bAAAAAFJGWgfuem63296gFghI69ZJ27dLmZn21QEAAAAAsEzaTpqWVFx1v2Zm2AYAAAAAxyBwW6F+srZo1N46AAAAAACWIXBbob6Hm8ANAAAAAI5B4LYCgRsAAAAAHIfAbQUCNwAAAAA4DoHbCkyaBgAAAACOQ+C2ApOmAQAAAIDjELitwJByAAAAAHAcArcVCNwAAAAA4DgEbitwDzcAAAAAOA6B2wrcww0AAAAAjkPgtgJDygEAAADAcQjcViBwAwAAAIDjELitQOAGAAAAAMchcFuBSdMAAAAAwHEI3FZg0jQAAAAAcBwCtxUYUg4AAAAAjkPgtgKBGwAAAAAch8BtBe7hBgAAAADHIXBbgR5uAAAAAHAcArcVmDQNAAAAAByHwG0FergBAAAAwHEI3FbgHm4AAAAAcBwCtxXo4QYAAAAAxyFwW4F7uAEAAADAcQjcVqCHGwAAAAAch8BtBQI3AAAAADiO7YH722+/1Xnnnafi4mJlZWWpd+/e+vzzz+0uq3ExaRoAAAAAOI7Hzg/ftWuXBg8erJNOOkmvv/66mjVrppUrV6qoqMjOshofPdwAAAAA4Di2Bu477rhDbdu21eOPPx7f16FDBxsrShAmTQMAAAAAx7F1SPkrr7yiAQMG6Be/+IVKSkrUr18/Pfrooz94fiAQkN/vb7CkBHq4AQAAAMBxbA3cq1ev1tSpU9W5c2fNmjVLv//973XZZZfpiSee2Of5U6ZMUUFBQXxp27atxRUfIu7hBgAAAADHMUzTvhSYkZGhAQMG6OOPP47vu+yyyzRv3jzNnTt3r/MDgYACgUB82+/3q23btiovL1d+fr4lNR+SBQuk/v2lNm2kDRvsrgYAAAAAcIj8fr8KCgoOKIfa2sPdsmVL9ejRo8G+7t27a/369fs83+fzKT8/v8GSEriHGwAAAAAcx9bAPXjwYK1YsaLBvq+//lqlpaU2VZQg3MMNAAAAAI5ja+C+4oor9Mknn+i2227TN998o+nTp+uRRx7RhAkT7Cyr8RG4AQAAAMBxbA3cAwcO1MyZM/XMM8+oV69euvnmm3Xvvfdq7NixdpbV+Jg0DQAAAAAcx9bncEvS6aefrtNPP93uMhKLHm4AAAAAcBxbe7gdg0nTAAAAAMBxCNxWoIcbAAAAAByHwG0F7uEGAAAAAMchcFuBHm4AAAAAcBwCtxUI3AAAAADgOARuKzBpGgAAAAA4DoHbCtzDDQAAAACOQ+C2AkPKAQAAAMBxCNxWIHADAAAAgOMQuK3APdwAAAAA4DgEbitwDzcAAAAAOA6B2wquPX7NhG4AAAAAcAQCtxX2DNwMKwcAAAAARyBwW6A26FKZClQrH4EbAAAAAByCwG2B8X/MVpHKdJ8uI3ADAAAAgEMQuC3g9sRmKQ/LI0UiNlcDAAAAALACgdsCnoxY4I7ITeAGAAAAAIcgcFvA4439msPySOGwzdUAAAAAAKxA4LaAx7vHkHICNwAAAAA4AoHbAgRuAAAAAHAeArcFPJ7YmsANAAAAAM5B4LaA2x1bM2kaAAAAADgHgdsC9HADAAAAgPMQuC1A4AYAAAAA5yFwW4DADQAAAADOQ+C2AIEbAAAAAJyHwG0BAjcAAAAAOA+B2wLMUg4AAAAAzkPgtgA93AAAAADgPARuCxC4AQAAAMB5CNwWIHADAAAAgPMQuC1A4AYAAAAA5yFwW6A+cDNpGgAAAAA4B4HbAvWzlNPDDQAAAADOQeC2AEPKAQAAAMB5CNwWIHADAAAAgPMQuC1A4AYAAAAA5yFwW6DBpGkEbgAAAABwBAK3BRr0cDNLOQAAAAA4AoHbAgwpBwAAAADnIXBbgMeCAQAAAIDzELgtQA83AAAAADgPgdsCBG4AAAAAcB4CtwWYpRwAAAAAnIfAbQFmKQcAAAAA5yFwW4Ah5QAAAADgPARuCzBLOQAAAAA4D4HbAvRwAwAAAIDzELgtwKRpAAAAAOA8BG4L0MMNAAAAAM5D4LbA7sDtlRlmlnIAAAAAcAICtwXqA7ckRUMEbgAAAABwAgK3BepnKZekcDBqXyEAAAAAAMsQuC3g2uO3HCVvAwAAAIAjELgtYBi7X5umfXUAAAAAAKxD4LZAg8AdJXEDAAAAgBMQuC1ADzcAAAAAOA+B2wIEbgAAAABwHgK3BRhSDgAAAADOQ+C2AD3cAAAAAOA8BG4LELgBAAAAwHkI3BZgSDkAAAAAOA+B2wL0cAMAAACA8xC4LUDgBgAAAADnIXBbgCHlAAAAAOA8BG4L0MMNAAAAAM5D4LYAPdwAAAAA4DwEbovRww0AAAAAzkDgtohhxJI2PdwAAAAA4AwEbou46gJ31DR+5EwAAAAAQDogcFvEUF0PNx3cAAAAAOAIBG6L1E+cxpByAAAAAHAGArdF4oGbvA0AAAAAjkDgtghDygEAAADAWQjcFqGHGwAAAACchcBtkfq5yQncAAAAAOAMBG6L8BxuAAAAAHAWArdFGFIOAAAAAM5ia+CePHmyDMNosHTr1s3OkhKGSdMAAAAAwFk8dhfQs2dPvfXWW/Ftj8f2khKCHm4AAAAAcBbb063H41GLFi3sLiPhCNwAAAAA4Cy238O9cuVKtWrVSh07dtTYsWO1fv36Hzw3EAjI7/c3WFIFs5QDAAAAgLPYGriPPvpoTZs2TW+88YamTp2qNWvW6LjjjlNFRcU+z58yZYoKCgriS9u2bS2u+NAxSzkAAAAAOIthmsnT51pWVqbS0lLdc889uuiii/Y6HggEFAgE4tt+v19t27ZVeXm58vPzrSz1oDXJDWhXlU/LBv9W3T78p93lAAAAAAAOgd/vV0FBwQHlUNvv4d5TYWGhunTpom+++Wafx30+n3w+n8VVNQ6GlAMAAACAs9h+D/eeKisrtWrVKrVs2dLuUhodQ8oBAAAAwFlsDdxXXXWV5syZo7Vr1+rjjz/WqFGj5Ha7NWbMGDvLSghmKQcAAAAAZ7F1SPnGjRs1ZswY7dixQ82aNdOxxx6rTz75RM2aNbOzrIQgcAMAAACAs9gauGfMmGHnx1uKe7gBAAAAwFmS6h7udBa/h5vADQAAAACOQOC2CEPKAQAAAMBZCNwWcdUF7qhp7P9EAAAAAEBaIHBbhCHlAAAAAOAsBG6L7B5STuIGAAAAACcgcFskHrij9tYBAAAAALAGgdsiPBYMAAAAAJyFwG0RZikHAAAAAGchcFuESdMAAAAAwFkI3BahhxsAAAAAnIXAbRECNwAAAAA4C4HbIgRuAAAAAHAWArdFmKUcAAAAAJyFwG2ReA+3vWUAAAAAACxC4LYIs5QDAAAAgLMQuC0S7+GO2lsHAAAAAMAaBG6LMGkaAAAAADgLgdsiBG4AAAAAcBYCt0UI3AAAAADgLARuixC4AQAAAMBZCNwW4TncAAAAAOAsBG6L0MMNAAAAAM5C4LYIgRsAAAAAnIXAbZF44La3DAAAAACARQjcFnG5YlE7aho/ciYAAAAAIB0QuC2ye0g5gRsAAAAAnIDAbRHu4QYAAAAAZyFwW4TADQAAAADOQuC2CIEbAAAAAJyFwG0RAjcAAAAAOAuB2yIEbgAAAABwFgK3RXY/h5tZygEAAADACQjcFqGHGwAAAACchcBtEQI3AAAAADgLgdsiuwM3iRsAAAAAnIDAbZHdgZt7uAEAAADACQjcFmFIOQAAAAA4C4HbIgRuAAAAAHAWArdFjLrETeAGAAAAAGcgcFuE53ADAAAAgLMQuC3CkHIAAAAAcBYCt0UI3AAAAADgLARuixC4AQAAAMBZCNwW4R5uAAAAAHAWArdFjLrfND3cAAAAAOAMBG6LMKQcAAAAAJyFwG0RV91vOsqQcgAAAABwBAK3RYy6Lm7TJHADAAAAgBMQuC3CkHIAAAAAcBYCt0V2z1IOAAAAAHACArdFdvdwM6QcAAAAAJyAwG0RHgsGAAAAAM5C4LYIQ8oBAAAAwFkI3BaJz1IetbkQAAAAAIAlCNwW2d3DzT3cAAAAAOAEBG6L8FgwAAAAAHAWArdFmDQNAAAAAJyFwG0RhpQDAAAAgLMQuC0SnzSNHm4AAAAAcAQCt0V4LBgAAAAAOAuB2yK7J01jSDkAAAAAOAGB2yLxSdPsLQMAAAAAYBECt0Xi93AzaRoAAAAAOAKB2yINZiln5jQAAAAASHsEbosQuAEAAADAWQjcFtl9DzeBGwAAAACcgMBtkQb3cBO4AQAAACDtEbgt4nLH1gRuAAAAAHAGArdF6nu4o3IRuAEAAADAAQjcFmHSNAAAAABwFgK3RZg0DQAAAACchcBtEXq4AQAAAMBZCNwWYZZyAAAAAHAWArdFGgwpj0btLQYAAAAAkHAEbovQww0AAAAAzkLgtgj3cAMAAACAsxC4LcIs5QAAAADgLARuizCkHAAAAACcJWkC9+233y7DMDRx4kS7S0kIhpQDAAAAgLMkReCeN2+eHn74YfXp08fuUhKGIeUAAAAA4Cy2B+7KykqNHTtWjz76qIqKiuwuJ2EYUg4AAAAAzmJ74J4wYYJGjBihIUOG/Oi5gUBAfr+/wZIq6OEGAAAAAGfx2PnhM2bM0IIFCzRv3rwDOn/KlCm68cYbE1xVYtDDDQAAAADOYlsP94YNG3T55Zfr6aefVmZm5gH9zLXXXqvy8vL4smHDhgRX2XiYNA0AAAAAnMW2Hu758+dr27ZtOvLII+P7IpGI3n//fT3wwAMKBAJyu90Nfsbn88nn81ldaqMwXPRwAwAAAICT2Ba4Tz75ZC1evLjBvl//+tfq1q2bJk2atFfYTnX0cAMAAACAs9gWuPPy8tSrV68G+3JyclRcXLzX/nRA4AYAAAAAZ7F9lnKnIHADAAAAgLPYOkv597333nt2l5AwBG4AAAAAcBZ6uC3i4jncAAAAAOAoBG6L1PdwR+UicAMAAACAAxC4LcKQcgAAAABwFgK3RRoE7mjU3mIAAAAAAAlH4LYIPdwAAAAA4CwEbosQuAEAAADAWQjcFiFwAwAAAICzELgtQuAGAAAAAGchcFuEwA0AAAAAzkLgtgiBGwAAAACchcBtEQI3AAAAADgLgdsiBG4AAAAAcBYCt0Vcdb/pqFxSNGpvMQAAAACAhCNwW8Tjia0jckuRiL3FAAAAAAASjsBtkfrAHZJXCoXsLQYAAAAAkHAEbot4vbF1WB4pHLa3GAAAAABAwhG4LdKgh5vADQAAAABpj8BtkfrAHZaHIeUAAAAA4AAEboswpBwAAAAAnIXAbREmTQMAAAAAZyFwW4QebgAAAABwFgK3RZg0DQAAAACchcBtkQY93AwpBwAAAIC0R+C2CD3cAAAAAOAshxS4N2zYoI0bN8a3P/vsM02cOFGPPPJIoxWWbngsGAAAAAA4yyEF7l/96ld69913JUlbtmzRKaecos8++0x//vOfddNNNzVqgemCSdMAAAAAwFkOKXAvWbJERx11lCTpueeeU69evfTxxx/r6aef1rRp0xqzvrTBkHIAAAAAcJZDCtyhUEg+n0+S9NZbb+nnP/+5JKlbt27avHlz41WXRpg0DQAAAACc5ZACd8+ePfXQQw/pgw8+0OzZszVs2DBJ0qZNm1RcXNyoBaYLergBAAAAwFkOKXDfcccdevjhh3XiiSdqzJgx6tu3ryTplVdeiQ81R0NMmgYAAAAAzuI5lB868cQTtX37dvn9fhUVFcX3X3zxxcrOzm604tIJk6YBAAAAgLMcUg93TU2NAoFAPGyvW7dO9957r1asWKGSkpJGLTBdMKQcAAAAAJzlkAL3GWecoSeffFKSVFZWpqOPPlp33323zjzzTE2dOrVRC0wXTJoGAAAAAM5ySIF7wYIFOu644yRJL7zwgpo3b65169bpySef1H333deoBaYLergBAAAAwFkOKXBXV1crLy9PkvTmm29q9OjRcrlc+ulPf6p169Y1aoHposGkaQRuAAAAAEh7hxS4jzjiCL388svasGGDZs2apVNPPVWStG3bNuXn5zdqgemifkh5SF6GlAMAAACAAxxS4L7hhht01VVXqX379jrqqKM0aNAgSbHe7n79+jVqgemivofblEvRUMTeYgAAAAAACXdIjwU7++yzdeyxx2rz5s3xZ3BL0sknn6xRo0Y1WnHppL6HW5LCgYgy7CsFAAAAAGCBQwrcktSiRQu1aNFCGzdulCS1adNGRx11VKMVlm48e/ymQ0GTwA0AAAAAae6QhpRHo1HddNNNKigoUGlpqUpLS1VYWKibb75Z0Wi0sWtMC3sG7nDItK8QAAAAAIAlDqmH+89//rMee+wx3X777Ro8eLAk6cMPP9TkyZNVW1urW2+9tVGLTAd7DikPBbgoAQAAAADp7pAC9xNPPKF//vOf+vnPfx7f16dPH7Vu3VqXXHIJgXsfXC7JMEyZpkEPNwAAAAA4wCENKd+5c6e6deu21/5u3bpp586dh11UuvK6Yz3b4SA93AAAAACQ7g4pcPft21cPPPDAXvsfeOAB9enT57CLSlceVyxo8xhuAAAAAEh/hzSk/M4779SIESP01ltvxZ/BPXfuXG3YsEGvvfZaoxaYTrzu2FDycNjmQgAAAAAACXdIPdwnnHCCvv76a40aNUplZWUqKyvT6NGjtXTpUj311FONXWPayPDGerhra7iHGwAAAADSnWGaZqOlv0WLFunII49UJBJprLfcL7/fr4KCApWXlys/P9+SzzwcR7Sq0qrNOfqw60UavPwxu8sBAAAAABykg8mhh9TDjUNTkBfr4fZXGDZXAgAAAABINAK3hfLzY0G7vMptcyUAAAAAgEQjcFuooCj26y6v8tpcCQAAAAAg0Q5qlvLRo0fv93hZWdnh1JL28pvEerb94SwpGJQyMmyuCAAAAACQKAcVuAsKCn70+AUXXHBYBaWzgqaxnu1yFUh+v9S0qc0VAQAAAAAS5aAC9+OPP56oOhwhvyA2pNyvfAI3AAAAAKQ57uG2UP0AgXIVSOXl9hYDAAAAAEgoAreFGgRuv9/eYgAAAAAACUXgtlCzZrH1ZrWUNm2ytxgAAAAAQEIRuC3UpUtsvUJdZS5fYW8xAAAAAICEInBbqFMnyTBMlatQ33252e5yAAAAAAAJROC2UFaWVNqsWpK0cH5YMk2bKwIAAAAAJAqB22JDTo39ym/fMFbmf16xuRoAAAAAQKIQuC127Y1ZyvSE9K5+psfGfSCVldldEgAAAAAgAQjcFuvYUbr5xthQ8j+W36D1F99ic0UAAAAAgEQgcNvgikkZGtTLrwrl6+Lnh8h87XW7SwIAAAAANDICtw3cbunxF/Llc4c0S8P07qUvMoEaAAAAAKQZArdNunaVfvvriCTp7jWjpNmzba4IAAAAANCYCNw2uuKaTBmK6jWN0Ipbnre7HAAAAABAIyJw26hTJ2nocTWSpJc+KmHGcgAAAABIIwRum406L0eS9HL059Jrr9lcDQAAAACgsRC4bTZyZGz9mY7Wpmfm2FsMAAAAAKDRELht1rKldHTPSknSG295pEjE5ooAAAAAAI2BwJ0ETjwtW5I0t/Yn0rJl9hYDAAAAAGgUBO4kMGhwrBnmapD0ySc2VwMAAAAAaAwE7iTw05/G1l+ph8rnLLS1FgAAAABA4yBwJ4HmzaUOzatkyqXP5tTYXQ4AAAAAoBEQuJPETwe7JUmfbmgp+f02VwMAAAAAOFwE7iTxk6MzJUlL1Ev66iubqwEAAAAAHC4Cd5Lo1Su2XqJe0pIl9hYDAAAAADhsBO4k0bNnbL1CXRVcxKPBAAAAACDV2Rq4p06dqj59+ig/P1/5+fkaNGiQXn/9dTtLsk27dlKuL6SwvFo5r8zucgAAAAAAh8nWwN2mTRvdfvvtmj9/vj7//HP97Gc/0xlnnKGlS5faWZYtDEPq1TkgSVq63G1zNQAAAACAw2Vr4B45cqROO+00de7cWV26dNGtt96q3NxcffLJJ3aWZZueR/okSUvK20jbt9tcDQAAAADgcCTNPdyRSEQzZsxQVVWVBg0atM9zAoGA/H5/gyWd9OjrlSQtU3dp+XKbqwEAAAAAHA7bA/fixYuVm5srn8+n8ePHa+bMmerRo8c+z50yZYoKCgriS9u2bS2uNrE6d46tV6mT9M039hYDAAAAADgstgfurl27auHChfr000/1+9//XhdeeKG++oHnUF977bUqLy+PLxs2bLC42sSqD9wr1VnmSgI3AAAAAKQyj90FZGRk6IgjjpAk9e/fX/PmzdPf//53Pfzww3ud6/P55PP5rC7RMh06SIZhqtLM07al36m53QUBAAAAAA6Z7T3c3xeNRhUIBOwuwxY+n9SuWY0k6ZtlIZurAQAAAAAcDlt7uK+99loNHz5c7dq1U0VFhaZPn6733ntPs2bNsrMsW3XuFNW6bdLK9T4NNs3Y88IAAAAAACnH1sC9bds2XXDBBdq8ebMKCgrUp08fzZo1S6eccoqdZdnqiF6Zemuu9E1t69ijwZo1s7skAAAAAMAhsDVwP/bYY3Z+fFLq3C3WJN/oCGnlSgI3AAAAAKSopLuH2+nq5o+LBW4eDQYAAAAAKYvAnWTqAzePBgMAAACA1EbgTjIdO8YeDeZXgbYv2WJ3OQAAAACAQ0TgTjKZmVLbpnWPBlsetrkaAAAAAMChInAnoU7to5KkVRsybK4EAAAAAHCoCNxJqH3XWNBeV1UsVVTYXA0AAAAA4FAQuJNQ6RF1gVul0rp1NlcDAAAAADgUBO4k1L59bL1W7QncAAAAAJCiCNxJqLQ0tl6nUmntWltrAQAAAAAcGgJ3Eqrv4V6nUkXXrre1FgAAAADAoSFwJ6HWrSWXEVVAmdq2Yqfd5QAAAAAADgGBOwl5vVLr4lpJ0tpVUZurAQAAAAAcCgJ3kmrfNha0121021wJAAAAAOBQELiTVOkRXknSWn+RVFNjczUAAAAAgINF4E5S7bvwLG4AAAAASGUE7iRV2t6QxLO4AQAAACBVEbiT1J6PBuNZ3AAAAACQegjcSaq0NLZeq/Yy19LDDQAAAACphsCdpNq1i62rlaMdX++wtxgAAAAAwEEjcCcpn09qWRSbnXzdNyGbqwEAAAAAHCwCdxIrbR2RJK3dwLO4AQAAACDVELiTWPsjPJKkdbvypUDA5moAAAAAAAeDwJ3ESrv4JElrVSpt2GBzNQAAAACAg0HgTmLtO8Sexb1OpTyLGwAAAABSDIE7ie35aDCexQ0AAAAAqYXAncTat4+t6eEGAAAAgNRD4E5i9c/iLlehyr7eZm8xAAAAAICDQuBOYjk5UrP8WknSupVBm6sBAAAAABwMAneSK20VliStXU9TAQAAAEAqIcUlufad3JKktTvypHDY5moAAAAAAAeKwJ3kSrtmSpLWmW2ljRttrgYAAAAAcKAI3Emu/lnca9WemcoBAAAAIIUQuJNc/bO416mUZ3EDAAAAQAohcCe5+mdx08MNAAAAAKmFwJ3k6nu4d6pYFSu32FsMAAAAAOCAEbiTXH6+VJQTkCStW1FrczUAAAAAgANF4E4B7VuFJEnrNhg2VwIAAAAAOFAE7hRQ2jHWTGu35UiRiM3VAAAAAAAOBIE7BbSvfxZ3tI20ebPN1QAAAAAADgSBOwWUdqjr4VZ7Hg0GAAAAACmCwJ0C6h8Ntk6lPBoMAAAAAFIEgTsF1D8ajB5uAAAAAEgdBO4UUN/DvU3NVbNqk621AAAAAAAODIE7BRQWSnmZQUk8ixsAAAAAUgWBOwUYhtS+VSxwr13Hs7gBAAAAIBUQuFNEaftYU63bmilFozZXAwAAAAD4MQTuFNG+m0+StDbcWtq61eZqAAAAAAA/hsCdIko7uCXxaDAAAAAASBUE7hRRP1M5jwYDAAAAgNRA4E4RPIsbAAAAAFILgTtFdOgQW29WK9Ws3GhvMQAAAACAH0XgThHFxVJ+VuzRYGuWVttcDQAAAADgxxC4U4RhSJ3axgL3N6tpNgAAAABIdiS3FHJE19hM5au250vBoM3VAAAAAAD2h8CdQjr1yJQkrTI7MnEaAAAAACQ5AncK6XSEIUn6RkdIq1bZXA0AAAAAYH8I3CmkU6fYepU6EbgBAAAAIMkRuFPIEUfE1mvVXuGvV9tbDAAAAABgvwjcKaR1a8nnCSssrzYs9dtdDgAAAABgPwjcKcTlkjq0rJUkfbPStLkaAAAAAMD+ELhTzBGdY022anOWFI3aXA0AAAAA4IcQuFNMp551jwYLl0qbNtlcDQAAAADghxC4U8wRXWJN9rW6SF9/bXM1AAAAAIAfQuBOMd26xdYr1FVavtzeYgAAAAAAP4jAnWLqA/cqdVJoKT3cAAAAAJCsCNwppnVrKccXUlheffNFhd3lAAAAAAB+AIE7xRiG1K1DQJK0/GuaDwAAAACSFYktBXXv7ZUkLd/RVKqstLkaAAAAAMC+ELhTULe+PknScnWTVqywuRoAAAAAwL4QuFNQ/cRpy9WNmcoBAAAAIEkRuFNQfeBepu4ylxG4AQAAACAZEbhT0BFHSG5XVBXK1+aFW+0uBwAAAACwDwTuFOTzSR1b1kiSli+N2FwNAAAAAGBfCNwpqls3Q5K0fEOOFA7bXA0AAAAA4PsI3CmqW78sSdLyyBHS2rX2FgMAAAAA2AuBO0V16x7r4V6m7sxUDgAAAABJiMCdorp3j62Xqbv01Vf2FgMAAAAA2AuBO0X16BFbf6s22jV/tb3FAAAAAAD2YmvgnjJligYOHKi8vDyVlJTozDPP1IoVK+wsKWUUFEilJdWSpMXzgzZXAwAAAAD4PlsD95w5czRhwgR98sknmj17tkKhkE499VRVVVXZWVbK6N0rtl68Nk8KhewtBgAAAADQgMfOD3/jjTcabE+bNk0lJSWaP3++jj/+eJuqSh19js7Sq+9IX0Z6SF9/LfXsaXdJAAAAAIA6tgbu7ysvL5ckNWnSZJ/HA4GAAoFAfNvv91tSV7Lq3Sc2U/li9ZYWLyZwAwAAAEASSZpJ06LRqCZOnKjBgwerV69e+zxnypQpKigoiC9t27a1uMrk0qdPbL1YvRVdtNjeYgAAAAAADSRN4J4wYYKWLFmiGTNm/OA51157rcrLy+PLhg0bLKww+XTuLGV4IqpUntZ9usXucgAAAAAAe0iKIeWXXnqpXn31Vb3//vtq06bND57n8/nk8/ksrCy5eb1Sjw41WrgyV18uNtTB7oIAAAAAAHG29nCbpqlLL71UM2fO1DvvvKMOHYiMB6v3kRmSpMXbW0hlZfYWAwAAAACIszVwT5gwQf/+9781ffp05eXlacuWLdqyZYtqamrsLCul9BkQC9xfqo+0ZInN1QAAAAAA6tkauKdOnary8nKdeOKJatmyZXx59tln7SwrpfTuHVsvVm/pyy/tLQYAAAAAEGfrPdymadr58Wmhfqbyr9VFNV9MVZa95QAAAAAA6iTNLOU4NC1aSE3zahWVW0s/qbC7HAAAAABAHQJ3ijMM6ci+EUnS/OU5Ujhsc0UAAAAAAInAnRb6H5stSfo83Ff66iubqwEAAAAASATutNB/gCFJmq/+0vz5NlcDAAAAAJAI3Gmhf//Yeol6KfDZInuLAQAAAABIInCnhdJSqUluQCFlaPGH5XaXAwAAAAAQgTstGIbUv09ssrT5K3KZOA0AAAAAkgCBO030Py42cdr8UG9p2TKbqwEAAAAAELjTxICBsYnTPtcA6fPPba4GAAAAAEDgThNHHRVbf6k+qvpggb3FAAAAAAAI3OmibVupdZMaReTR5+9V2l0OAAAAADgegTuNDDomNqz84zUtJb/f5moAAAAAwNkI3GnkmJ9lSpLm6qfSp5/aXA0AAAAAOBuBO40MGhRbz9UgmR/PtbcYAAAAAHA4Anca6ddPyvBEtF3N9M3b6+wuBwAAAAAcjcCdRnw+aUCPGknS3PleKRq1uSIAAAAAcC4Cd5o5ZkiWJOnD6iOlpUttrgYAAAAAnIvAnWZOOMktSXpXJ0nvvmtzNQAAAADgXATuNHPccZLLiOobddbG1760uxwAAAAAcCwCd5opKJD6d6uWJL37gYf7uAEAAADAJgTuNHTSabH7uN+tPkpatMjmagAAAADAmQjcaeikIdzHDQAAAAB2I3CnoWOPlTyuiNaqg9b+j5nKAQAAAMAOBO40lJsrDewZex73ux/7pHDY5ooAAAAAwHkI3GnqpBHZkqR3a38qffGFzdUAAAAAgPMQuNPUz4bEmvYtDZH55mybqwEAAAAA5yFwp6ljj5VyfCFtVistev5ru8sBAAAAAMchcKcpn08ackJIkvTal62lnTttrggAAAAAnIXAncaGj47dx/2aOVx6802bqwEAAAAAZyFwp7Hhw2PruRqknTPn2FsMAAAAADgMgTuNtWsn9epQqajcevP1iBSN2l0SAAAAADgGgTvNnTY6S5L0WsWx0vz5NlcDAAAAAM5B4E5zw093S5Je13BF/vuazdUAAAAAgHMQuNPc4MFSQVZA29VMc59Za3c5AAAAAOAYBO405/VKI083JUkvfdNbWr3a5ooAAAAAwBkI3A4wekymJOkljZb50kybqwEAAAAAZyBwO8DQoVJ2Rkjr1F4LnlxidzkAAAAA4AgEbgfIzpaGDwlLkl5afIS0aZPNFQEAAABA+iNwO8TosbHHg72k0dLLL9tbDAAAAAA4AIHbIUaMkLzuiJaru7568nO7ywEAAACAtEfgdoiCAumU4wKSpBmfdpC2bLG5IgAAAABIbwRuB/nVb7MlSU/rVzKfmWFzNQAAAACQ3gjcDnLGGbHZylerkz57dJHd5QAAAABAWiNwO0hurnTm6RFJ0tPL+kkrV9pcEQAAAACkLwK3w4y9KFOS9KzOVfipZ2yuBgAAAADSF4HbYU45RWqaV6ttaq63/rlWMk27SwIAAACAtETgdhivVzrnl7Fmn775RGnePHsLAgAAAIA0ReB2oLHjMiRJMzVKVdOet7kaAAAAAEhPBG4HGjRI6tiiWpXK0/NP1kiBgN0lAQAAAEDaIXA7kGFIF02ITZ72aNUYaeZMmysCAAAAgPRD4HaoX1/kktuI6GMN1lf3vml3OQAAAACQdgjcDtWypXT6kNhQ8n9+2ktatcrmigAAAAAgvRC4Hex3l2dLkp7UBQo89LjN1QAAAABAeiFwO9iwYVLrJjXaoaaa+eh2KRSyuyQAAAAASBsEbgdzu6XfjI89IuzR8l9Ir75qc0UAAAAAkD4I3A530cVuuYyo3tHJWnbnf+0uBwAAAADSBoHb4UpLpZ+fUitJuu+TgdKSJTZXBAAAAADpgcANXX7t7snTdt31T5urAQAAAID0QOCGTjhB6tOpUtXK0T+nZ0s7dthdEgAAAACkPAI3ZBjS5dfmSJIeCP+fwlMftbkiAAAAAEh9BG5Ikn411lDTvFqtV6mev3u9VFtrd0kAAAAAkNII3JAkZWZKl/3RI0maUjZe5rQnbK4IAAAAAFIbgRtxl17uUa4vqMXqo/9NnieFw3aXBAAAAAApi8CNuKIi6ZJLYq9v3XqRzGefs7cgAAAAAEhhBG40cMX/y5DPE9YnGqT3rpstRaN2lwQAAAAAKYnAjQZatJAuujAiSbp+7UUyZzxrc0UAAAAAkJoI3NjLn270KdMT0kc6Vq9f9bYUCtldEgAAAACkHAI39tK6tfSHCaYk6c+bJyj62OM2VwQAAAAAqYfAjX2adH2G8nwBLVQ/Pf+nL6SaGrtLAgAAAICUQuDGPhUXS1f9P7ck6dpdV6v2jr/bXBEAAAAApBYCN37QH/+fR62KqrVGHfW322qkb7+1uyQAAAAASBkEbvyg3FzpzvuyJEm3hq7Wt5fdYXNFAAAAAJA6CNzYr1+NNXRMn0pVKVfXvDRQ+vhju0sCAAAAgJRA4MZ+GYb098dyZSiqf+t8vXXe41IwaHdZAAAAAJD0CNz4UQMGSJf8JiBJ+r8116j61r/ZXBEAAAAAJD8CNw7IbX/LUpsmVVqtTpp8i0davtzukgAAAAAgqRG4cUDy86Wp07IlSXdHJ+rTX/xVCodtrgoAAAAAkheBGwfs9JGGfnVGlaJy61dLrlXFDXfZXRIAAAAAJC1bA/f777+vkSNHqlWrVjIMQy+//LKd5eAA/GNajkqbVmq1OukPU1oxazkAAAAA/ABbA3dVVZX69u2rf/zjH3aWgYNQWCj9e2auXEZUT+hCPXvmdKm83O6yAAAAACDp2Bq4hw8frltuuUWjRo2yswwcpGOPlf58dUiSdPF3t2rluddJpmlzVQAAAACQXFLqHu5AICC/399ggT1uuNWnY/v65VeBzpw1XhU38agwAAAAANhTSgXuKVOmqKCgIL60bdvW7pIcy+ORnn8jX60KqvSVemrc5FJFX3vD7rIAAAAAIGmkVOC+9tprVV5eHl82bNhgd0mO1qKF9OLr2cpwhfSSztJ1o5ZKixfbXRYAAAAAJIWUCtw+n0/5+fkNFtjrp4MMPfxQ7PWU4JV66Pjp0saN9hYFAAAAAEkgpQI3ktO433l14zXVkqQJZbdo5uC/Sjt32lwVAAAAANjL1sBdWVmphQsXauHChZKkNWvWaOHChVq/fr2dZeEQXH9bti46t0JRuXXu+jv16lE3SWVldpcFAAAAALYxTNO+5zm99957Oumkk/baf+GFF2ratGk/+vN+v18FBQUqLy9neHkSCIel835ermdfL1CGAnq5yyQN/3Ry7OHdAAAAAJAGDiaH2hq4DxeBO/mEQtKY08r14lsF8iqop9pdp3M/uUJq2dLu0gAAAADgsB1MDuUebjQqr1ea/r8CnXNqmULK0Jj1t+uB3g9JK1faXRoAAAAAWIrAjUaXkSFNf61Ql5xXLlMu/WHHjbqs97sKv/We3aUBAAAAgGUI3EgIt1t64MkC3XpNhSTp/sDFGnFqUDv/+i8pde9iAAAAAIADRuBGwhiG9KcpeXpxekDZ7lq9aZ6qgVefoEWjJku1tXaXBwAAAAAJReBGwo0e49PHn/vUoUmZVquTfvqfa/TQEX+VuWy53aUBAAAAQMIQuGGJvj8x9PnKQg0bsF21ytLvv71OZ/VeoZ1/f4oh5gAAAADSEoEblmnSRPrfp01191/88hohzYycoZ9MPEEfn/oXye+3uzwAAAAAaFQEbljK5ZL+ODlfcz9164jindqgdjr+rRv0l9JpCr79gd3lAQAAAECjIXDDFv0HurRgTRP96tTvFJFHN5VdpqOG5OnLsXdINTV2lwcAAAAAh43ADdvk5Un/fqOZZvyrWsW+Ci3STzRg+hW6ud2jqnljjt3lAQAAAMBhIXDDVoYhnfvrbC1dl6czBm1VSBm6Yftl6jq8g57uf4+iy1bYXSIAAAAAHBICN5JC8+bSzI+aa/ojlWqbG7u3+7wFf9RPe/j14dn3Sjt22F0iAAAAABwUAjeShmFIY36XqxXbmui2K7Yp112teRqo416cqLNbfqhV1/5TCgTsLhMAAAAADgiBG0knK0u69p4SffNtti4e8a1ciujF0BnqfvsF+mPzf2vn4/+RolG7ywQAAACA/SJwI2k1by49/GprLVooDe21USFl6G/lF6ntb4bokiYztOIv06WqKrvLBAAAAIB9InAj6fXq69Ybi9vo9Zdq1Lf5FlUrR1PLf6VuN/1KIwo/0uxz/ylz/Qa7ywQAAACABgjcSBnDRmXpi80t9PZ/qzWy12oZiuq18Kk69bnfqndpuf7W53FtfXKWFArZXSoAAAAAyDBN07S7iEPl9/tVUFCg8vJy5efn210OLLZyeUT3X71O/3q9paoiWZIkt8IalvGuLhzyrUZe/xNlHt03NhsbAAAAADSCg8mhBG6kvLIyafpd3+rJf4X16ZbS+P5C7dLZRe/orJFB/eyPP1FG3+72FQkAAAAgLRC44VjLF4f05M1r9dSrRdpY0zS+v0BlGpH3gUYP8WvYxG7KOe5Ier4BAAAAHDQCNxwvGpXe+2+Fnv/bRr38SXNtCTSJH8tUjU7KnKuhfbdq6C+L1PWCo2U0KbKxWgAAAACpgsAN7CEalT55q1Iv3bdBM98r0uqqFg2Ol2qtTi1ZqKEnBPSzizqo6Gf9JK/XpmoBAAAAJDMCN/ADTFNa8nmt3nh4rWa96dIHG9sraGbEjxuKqrdrqY5vvUrHHxPRcee2Uovh/aTMTBurBgAAAJAsCNzAAaqqkua88J1mPbVNb35WoOUVbfY6p7NW6vjmKzSof0BHnVqkHmd1l7tNSxuqBQAAAGA3AjdwiLZujuqDZzbq/VfK9P6ifH1Z1k7m9x5Xn6NK9c9YoqNKt+qogaaOOr1E7U7rJaOA/wYBAACAdEfgBhpJ2S5TH72wWR/M3K7PFno1b2s7VUZz9jqvWNvVN+tr9W29Q317R9T3uAJ1P62DfF1KmQ0dAAAASCMEbiBBIhFpxRdV+uyljfrsvRp9tjxPi3a1U1h7T7LmUUjdXF+rb/G36tu5Wt1/4lP3Y4vV/qQOcjdvShAHAAAAUhCBG7BQba209MNdWvTGZi36rFaLvs7Sou2tVRbZ93+TPtWqi3uVuhduUbd2Verew6VuRxeo68ltlNW1neR2W/wNAAAAABwoAjdgM9OUNqwK6svXv9Wi98v05ZeGlm/O14qK1grIt8+fMRRVO21Qp+zN6tS0XJ3aBtWpq0ed+uWr46DmKujdTsrI2OfPAgAAALAGgRtIUpGItHZZjZa/u1nLPinX8mVRLVufo2VlLbUrUrDfny3WdnXK2KhORTvUqVWNOnR0qV3XLLXrXaC2/UuU1bElveMAAABAghG4gRRjmtJ3WyJa+cEWrZq3U6uW1mrVGpdWbc3RKn8zfRcp/tH3KNFWtcvYonb5ZWrXtEbt2kTUrqNXpT1z1a5fsZr2aSVXQZ4F3wYAAABIXwRuIM34y02t/my7Vn26XasWV2nVN6bWb87Q+vJ8raspUZW598zp3+dVUC2NLWrp26lWuX61KqpVq+YRtWrjUqsOPrXqkqtWvZqoqHsLGTnZFnwrAAAAIPUQuAEHMU1p1/aI1i/YrvULd2r98mqtXxPW+m89Wrc9R+srm2hzuOlezxP/IT7VqqWxVa0yd6hlbqVKCgNqXhxRSYlU0tqrktIslXTMVfOuhSro1JRwDgAAAEchcANoIBSStqyq0qYlO7R5uV+bVtdo0/qINm1xadMOnzZV5GpTTRPtiBYd1Pt6FVSJ8Z1KvLtUklWpkrwaNS8KqqRZVCUt3Gra2qfi1pkqbpej4g75KuxQJFd+Lo9EAwAAQMo6mBzqsagmADbyeqW23XLUttv+h57X1pja8k2lNi3Zqc0r/Nq0ulbfbYlo23fStp1ebavI1LbqXG0NFslv5iukDH1rtta3wdZSUFK5pI0//P6GoirSThW7y1ScUaHirGo1yQmoOD+s4qKIipsaKi7xqElLn4rbZKm4NFfFHQuU3aaJDB8ztAMAACC10MMN4JDU1pj6bm2Vtn29S9u+qdC2dTXa9m1IW7ea2rbdra3lPu2ozNSOQK52hPNVYR76hG0ZCqhQ5SpwV6rQW6VCX40KswIqzAmpMC+iwoKoCosMFRa7VdjMq8ISnwpbZqmgda4K2+Qqu1UhgR0AAACNgh5uAAmXmWWobfdcte2ee0DnB4PSrm+rtWN1uXasr9KODdXasTmgHVsj2rk9qh27XNrh98RCek22dgRztSNSqJAyFJRP21SibZESKSKpVrHe9APkUUiF+k6F7op4YM/PDCo/K6y87Ijyc6PKy5Py8g3lF7qU18Sr/GKv8pr6lF+Sqbzm2cpvka3s5nkyMn0MiQcAAMABIXADsERGhtS8Q7aadzjwSdZMU6qqiGrXOr/KNlaqbFO1yrbUqmxbUOU7wirbGVHZLqmswqWySo/KqjJUVpupsmCWysK5KovmKyKPwvJqu5ppe6TZIQX2ei5FlCu/8l2VynNXK99To7yMgPIzA8rLCis/O6K8nN3hPTffpZx8t3IKvcot9CinKEM5xZnKLfYpp2mWckpy5C3I5vnpAAAAaYrADSBpGYaUm+9Sbu9Cte1deNA/b5pSdUUkFtbrA/vmGu3aGlTFzpD8ZVFV+E35Kw1VVLrkr/aootYjf8CniqBP/nC2KiLZ8pt5MuVSVG75VSB/tECKSgpJqtEhhfd6GQooR+XKdVUrx1WrHE+tcr0B5XhDyvGFlZsZVk5WRDlZpnJzTOXkSrl5RizI53viQT67MLZkFfqUVehTdnGWfIVZsaH09MgDAADYgsANIG0ZhmLBtEeBWvcoOOT3qQ/u/s1VqthaLf+WalVsD6hie0D+HSFV7ArvFd4ratyqqvWoMuhVVShDVSGfKiOZqopmqcrMVlheSVJQPgXl065ok1iIDyvWA98Y319RZalaWapVtqtGWa6Ast0BZXlCyvaElOUNKTsjoqyMiLIzI8rymcrKMpWdJWVlG8rOlrJy3crOdSkrz6PsPLey8r3KLvDGQn1hhrKKMpVVlKnMIsI9AADA9xG4AeBHxIN7fr5adj38CRpNUwoGTFXtqFXVd9Wq2lGryu21qtoZUFVZSJVlIVWVhVVVEVVlhamqSlNVVVJltUtVNS5VBdyqDHhVVRfmK8OZqon4VG1mqsbMVEixCeJMuVStHFUrRzui2h3oA4f9FfYpQwFlqlaZRkCZRlCZrqAy3UFlukOxxRNWpieiTG9EPm9UmRlRZfqiyvSZyvRJmZlSZqapzCxDmVkuZWbHFl+2W5m5nt1Lnnf3kp+hzAKfMgsz5c72MTwfAAAkFQI3AFjMMCRfpiFf6yw1aZ3V6O8fDpmq8YdUvaNGNWUBVe+sVU15UNVlQdVUhFXtD8fWlVHVVEZUXW2qpspUdY2hmhqputalmoCh6oBHNUG3qkNe1YQ9qg5nqCaSoepIpmpMn6rNrHhPvbS7t95vSjK1O+BbxKOQfKpRphGQzwjKZwSV4QrL5wrJ5worwx2Wzx1RhidSt47K540ow2PK540qI8OUL0PK8Jry+aQMn+TzGbHXmS75Mg1lZLnky3LHtrPdyshyy5fjUUa2Z/c6L0MZOV75cr3y5fvkzcmQ4fXQ+w8AgAMRuAEgzXi8hvKKM5RXnPhHoYWCpmrKgwr4A6otD6i2IqRafzC2rgiptjKsQFVYtVUR1VZHVFsVVW2NqdqaqGprpdoaqTYg1dYaqg0aCgRdqg26VBtyx5awJ7ZEvKqNelUbyVCtmaHaqE8BZcR78yUpLK/C8qrKzI0Ffik2SV4SyFAgNt++EVSGEWpwMSDDFZbXFZHXFVWGOyKvOyKvO6oMd1ReT1QZnqi8HlNet6kMrymv15TXE5uI0OvdvfZmGMrwGbvXPpe8PkMZme661y5lZLnlzXTH196s2EUCb1Zsycjxxta5GfJkeWV43FwoAADgMBC4AQCHzJthyNvMJzXz2fL5kYhigb6sNhb0ywOq8YcUrAwqUB1WsCqsQE1UwerYOlATUbAmqkCtqWBtVIGAFAxEFQgYCgZMBYKGgiHVrV0KhAwFwy4FQm4FIy4Fwm4FIh4FI24FIl4Fox4Fol4FTY8CZoaCplcB+RT53l+v9b3/lfW9/1LSXAzYH0/dg/m8CslrhJVh1K/D8tZdKNjzgoHXHVt73FF5XKa87qg8blNeT2ztcSv22iN53WZs7ZU8Hu1+7ZW8XkOeusWbscc6w5A3wyVP3eL11a0z3bF9Pnf8tTfLs3s70yNPZuyiQv3ayPByMQEAkHAEbgBAynK7pex8j7LzD+x58FaJRKRgdVjBioAClXUXAKrCsdfVdeuaiAJVYYVqIwrWRhUKROPrUNBUMGDG1sHYSIJQKPY8+1BICoYMhUJSKBy7IBAKS8GwW6GwoVDEpWDEHVtH3QpF3ApFXQpGPQpFPQqaHoXMutfyKmR6FFTGXhcJpN2jBiTFLhSYe52SslzxhwaGYmsjLI8isbURkceIyOsKy2NEY2tXtO51RB53VF5XVG6XKU/92m3WraN1rxVb111ocLtj2x5P3es91h6P5PZIHo8ht8eIbXsNeTyxiw3u+rXX1WDtyXDt3pfh3r2uuyBR/zq+9sUuQuz52u3zyO3zyHC7uAABAAlA4AYAoJG53VJWnkdZeanz12w0KoWDUQWrQgpVh2LrmnDsdU1EoZqwgtWxCwSh2khsXyC6ex0wFQ5EFA6ZCoVMhUOx9wuFYvMKhMOxiwXhsBQKK3Y8IoVChsIRQ6FwbB2OGApFDIUjrtg66oq9jroUjroUirgVNl0KR90Kme7d67olZHoUlmf3Wvu+tSIqt4JyK6i60Rn1FxPS6KLCwai/AOFRWG5F5FFE7roLDx6F5Tai8hiR762j8rgichvmHuvYBQi3KyqXIbnrtw2zbn/d4jblcmmPfZLbXb9W7FjdhYrY+XWvPZLLZdS9NmI/4zbk9khutyGXO3aBInZMu1976o55XbuP1b+uW7s8rob79lhcntiFi/jx+tcZ7t3H9tznrfsSABwvdf4lAAAAEsblik0Ol5Hpk4rtuUUgUSJhMxb+q0MK14YVro1dOAjXhBQKRBQORHfvC8QuIISD0QavQ4Fo7MJBMKJQUAoFooqEzdh7h0xFIrsvLEQisQsOkYjqtmMXF8Jho+71Huvo7gsN9a8jUUPhaOxiQyTqUjhqxNamK74OR12KmC6FTfde67DpUUR1++RWrO/evc9RDPX2ugAhpd2oBqu5FZZLUbnrfvuxJSq3EVu7jOjubSMaWxSNv3YZZuyc+OvYEtvW7m1X/XHtsS25XGb8okfD7djFjr23Fb8Iste22/jeduzCh8sludyS26Xd57hjFzT2uV23z+2RXG7XHtvG7uN1F01cHtfubbdi23X79jru3X3M5am7gOJ1771d//Ne1+7tugsthtvFSA8kBIEbAACktVgPp1u+bGc/Ns40YxcfIsGIwrXh2DoQia/DtWFFQrELDQ3WdRcfImFz9zoYjR0L1b0OmwqHorELDaGoIhFT0XDsQkSkfr3H62hkj317LlEpGn9tKBKVIpHd62h0j/1RV93r3UvUrDtu1h9zKWLGtqP1+8369e4laroUUf22e/druXcfaxibf/T3HRsnIIX2aojvrZE0DEUVa+1og8WQWfc6dhFk39ux1y6ZMoy644a5x3bdccOMnR/ft+fxPbb3WAztvsDicv3Atmv3RRej/oJM3T5jjwsuu8/X947vse0y6n5+94WX3efsZ3uPCzHx4+49tve4CNPgeP123c/7st06/eaj7f2PoRERuAEAABzAMFQ3GV3sMXY4dKZZd9EgFFU0FLtoEQlF91pHQ3XbdRc6IqFofImGo7uPhc3dx+pem5Fo7CJCOKpoxIxfpIhGoorucdGi/lg0GhtdEdtW3bbix6JRM3bRIr5ddzy6e6m/4BGNKnbxou4iR9RU7IJF/Nz6ixuuhsdNI34sWvd6977YEjF3v47WXwzZazsWc/ferovDZiwax7e1+5y68QPxZX8jO/ZqV9VfXNnvSfvfxmErNnZo+812V9F4+NMWAAAAOAiGsXvkhLKcPXIiFdRfIKm/yBENRWIXMsK7L37Et8OmoqFI7GfqL3aEozKjZvx1NGLu3q5bGmzXnx/dfUHEjEQbbn//+J7b0fr3U3xf/Hj9trnH8agaHt/Xtln//rGLIQ2Om0bDc+oumph7XEzZ5/Yer2PvWfde9dvx48bu7QbHjX1u52cGJRXb/Z9NoyFwAwAAAEhbe14g8Wa6pfqnLwAWYPpEAAAAAAASgMANAAAAAEACELgBAAAAAEgAAjcAAAAAAAlA4AYAAAAAIAEI3AAAAAAAJACBGwAAAACABCBwAwAAAACQAARuAAAAAAASgMANAAAAAEACELgBAAAAAEgAAjcAAAAAAAlA4AYAAAAAIAEI3AAAAAAAJACBGwAAAACABCBwAwAAAACQAARuAAAAAAASgMANAAAAAEACELgBAAAAAEgAAjcAAAAAAAlA4AYAAAAAIAEI3AAAAAAAJACBGwAAAACABCBwAwAAAACQAARuAAAAAAASwGN3AYfDNE1Jkt/vt7kSAAAAAIAT1OfP+jy6PykduCsqKiRJbdu2tbkSAAAAAICTVFRUqKCgYL/nGOaBxPIkFY1GtWnTJuXl5ckwDLvL+UF+v19t27bVhg0blJ+fb3c5OAS0YWqj/VIfbZj6aMPURxumPtowtdF+ycM0TVVUVKhVq1ZyufZ/l3ZK93C7XC61adPG7jIOWH5+Pv9zpDjaMLXRfqmPNkx9tGHqow1TH22Y2mi/5PBjPdv1mDQNAAAAAIAEIHADAAAAAJAABG4L+Hw+/eUvf5HP57O7FBwi2jC10X6pjzZMfbRh6qMNUx9tmNpov9SU0pOmAQAAAACQrOjhBgAAAAAgAQjcAAAAAAAkAIEbAAAAAIAEIHADAAAAAJAABO4E+8c//qH27dsrMzNTRx99tD777DO7S0KdKVOmaODAgcrLy1NJSYnOPPNMrVixosE5tbW1mjBhgoqLi5Wbm6uzzjpLW7dubXDO+vXrNWLECGVnZ6ukpERXX321wuGwlV8Fkm6//XYZhqGJEyfG99F+ye/bb7/Veeedp+LiYmVlZal37976/PPP48dN09QNN9ygli1bKisrS0OGDNHKlSsbvMfOnTs1duxY5efnq7CwUBdddJEqKyut/iqOFIlEdP3116tDhw7KyspSp06ddPPNN2vP+Vhpw+Ty/vvva+TIkWrVqpUMw9DLL7/c4HhjtdeXX36p4447TpmZmWrbtq3uvPPORH81x9hfG4ZCIU2aNEm9e/dWTk6OWrVqpQsuuECbNm1q8B60oX1+7P/BPY0fP16GYejee+9tsJ/2Sy0E7gR69tln9cc//lF/+ctftGDBAvXt21dDhw7Vtm3b7C4NkubMmaMJEybok08+0ezZsxUKhXTqqaeqqqoqfs4VV1yh//73v3r++ec1Z84cbdq0SaNHj44fj0QiGjFihILBoD7++GM98cQTmjZtmm644QY7vpJjzZs3Tw8//LD69OnTYD/tl9x27dqlwYMHy+v16vXXX9dXX32lu+++W0VFRfFz7rzzTt1333166KGH9OmnnyonJ0dDhw5VbW1t/JyxY8dq6dKlmj17tl599VW9//77uvjii+34So5zxx13aOrUqXrggQe0bNky3XHHHbrzzjt1//33x8+hDZNLVVWV+vbtq3/84x/7PN4Y7eX3+3XqqaeqtLRU8+fP11133aXJkyfrkUceSfj3c4L9tWF1dbUWLFig66+/XgsWLNBLL72kFStW6Oc//3mD82hD+/zY/4P1Zs6cqU8++UStWrXa6xjtl2JMJMxRRx1lTpgwIb4diUTMVq1amVOmTLGxKvyQbdu2mZLMOXPmmKZpmmVlZabX6zWff/75+DnLli0zJZlz5841TdM0X3vtNdPlcplbtmyJnzN16lQzPz/fDAQC1n4Bh6qoqDA7d+5szp492zzhhBPMyy+/3DRN2i8VTJo0yTz22GN/8Hg0GjVbtGhh3nXXXfF9ZWVlps/nM5955hnTNE3zq6++MiWZ8+bNi5/z+uuvm4ZhmN9++23iiodpmqY5YsQI8ze/+U2DfaNHjzbHjh1rmiZtmOwkmTNnzoxvN1Z7Pfjgg2ZRUVGDP0cnTZpkdu3aNcHfyHm+34b78tlnn5mSzHXr1pmmSRsmkx9qv40bN5qtW7c2lyxZYpaWlpp/+9vf4sdov9RDD3eCBINBzZ8/X0OGDInvc7lcGjJkiObOnWtjZfgh5eXlkqQmTZpIkubPn69QKNSgDbt166Z27drF23Du3Lnq3bu3mjdvHj9n6NCh8vv9Wrp0qYXVO9eECRM0YsSIBu0k0X6p4JVXXtGAAQP0i1/8QiUlJerXr58effTR+PE1a9Zoy5YtDdqwoKBARx99dIM2LCws1IABA+LnDBkyRC6XS59++ql1X8ahjjnmGL399tv6+uuvJUmLFi3Shx9+qOHDh0uiDVNNY7XX3LlzdfzxxysjIyN+ztChQ7VixQrt2rXLom+DeuXl5TIMQ4WFhZJow2QXjUZ1/vnn6+qrr1bPnj33Ok77pR4Cd4Js375dkUikwT/kJal58+basmWLTVXhh0SjUU2cOFGDBw9Wr169JElbtmxRRkZG/C+oenu24ZYtW/bZxvXHkFgzZszQggULNGXKlL2O0X7Jb/Xq1Zo6dao6d+6sWbNm6fe//70uu+wyPfHEE5J2t8H+/hzdsmWLSkpKGhz3eDxq0qQJbWiBa665Rr/85S/VrVs3eb1e9evXTxMnTtTYsWMl0YapprHaiz9bk0dtba0mTZqkMWPGKD8/XxJtmOzuuOMOeTweXXbZZfs8TvulHo/dBQDJYMKECVqyZIk+/PBDu0vBAdqwYYMuv/xyzZ49W5mZmXaXg0MQjUY1YMAA3XbbbZKkfv36acmSJXrooYd04YUX2lwdDsRzzz2np59+WtOnT1fPnj21cOFCTZw4Ua1ataINAZuFQiGdc845Mk1TU6dOtbscHID58+fr73//uxYsWCDDMOwuB42EHu4Eadq0qdxu914zIm/dulUtWrSwqSrsy6WXXqpXX31V7777rtq0aRPf36JFCwWDQZWVlTU4f882bNGixT7buP4YEmf+/Pnatm2bjjzySHk8Hnk8Hs2ZM0f33XefPB6PmjdvTvsluZYtW6pHjx4N9nXv3l3r16+XtLsN9vfnaIsWLfaaiDIcDmvnzp20oQWuvvrqeC937969df755+uKK66IjzqhDVNLY7UXf7barz5sr1u3TrNnz473bku0YTL74IMPtG3bNrVr1y7+b5t169bpyiuvVPv27SXRfqmIwJ0gGRkZ6t+/v95+++34vmg0qrfffluDBg2ysTLUM01Tl156qWbOnKl33nlHHTp0aHC8f//+8nq9DdpwxYoVWr9+fbwNBw0apMWLFzf4g6/+L7bvBwk0rpNPPlmLFy/WwoUL48uAAQM0duzY+GvaL7kNHjx4r0fxff311yotLZUkdejQQS1atGjQhn6/X59++mmDNiwrK9P8+fPj57zzzjuKRqM6+uijLfgWzlZdXS2Xq+E/Jdxut6LRqCTaMNU0VnsNGjRI77//vkKhUPyc2bNnq2vXrg2eQoDEqA/bK1eu1FtvvaXi4uIGx2nD5HX++efryy+/bPBvm1atWunqq6/WrFmzJNF+KcnuWdvS2YwZM0yfz2dOmzbN/Oqrr8yLL77YLCwsbDAjMuzz+9//3iwoKDDfe+89c/PmzfGluro6fs748ePNdu3ame+88475+eefm4MGDTIHDRoUPx4Oh81evXqZp556qrlw4ULzjTfeMJs1a2Zee+21dnwlx9tzlnLTpP2S3WeffWZ6PB7z1ltvNVeuXGk+/fTTZnZ2tvnvf/87fs7tt99uFhYWmv/5z3/ML7/80jzjjDPMDh06mDU1NfFzhg0bZvbr18/89NNPzQ8//NDs3LmzOWbMGDu+kuNceOGFZuvWrc1XX33VXLNmjfnSSy+ZTZs2Nf/f//t/8XNow+RSUVFhfvHFF+YXX3xhSjLvuece84svvojPYN0Y7VVWVmY2b97cPP/8880lS5aYM2bMMLOzs82HH37Y8u+bjvbXhsFg0Pz5z39utmnTxly4cGGDf9/sOWM1bWifH/t/8Pu+P0u5adJ+qYbAnWD333+/2a5dOzMjI8M86qijzE8++cTuklBH0j6Xxx9/PH5OTU2Neckll5hFRUVmdna2OWrUKHPz5s0N3mft2rXm8OHDzaysLLNp06bmlVdeaYZCIYu/DUxz78BN+yW///73v2avXr1Mn89nduvWzXzkkUcaHI9Go+b1119vNm/e3PT5fObJJ59srlixosE5O3bsMMeMGWPm5uaa+fn55q9//WuzoqLCyq/hWH6/37z88svNdu3amZmZmWbHjh3NP//5zw3+YU8bJpd33313n3/3XXjhhaZpNl57LVq0yDz22GNNn89ntm7d2rz99tut+oppb39tuGbNmh/89827774bfw/a0D4/9v/g9+0rcNN+qcUwTdO0oicdAAAAAAAn4R5uAAAAAAASgMANAAAAAEACELgBAAAAAEgAAjcAAAAAAAlA4AYAAAAAIAEI3AAAAAAAJACBGwAAAACABCBwAwAAAACQAARuAABwwAzD0Msvv2x3GQAApAQCNwAAKWLcuHEyDGOvZdiwYXaXBgAA9sFjdwEAAODADRs2TI8//niDfT6fz6ZqAADA/tDDDQBACvH5fGrRokWDpaioSFJsuPfUqVM1fPhwZWVlqWPHjnrhhRca/PzixYv1s5/9TFlZWSouLtbFF1+sysrKBuf861//Us+ePeXz+dSyZUtdeumlDY5v375do0aNUnZ2tjp37qxXXnklsV8aAIAUReAGACCNXH/99TrrrLO0aNEijR07Vr/85S+1bNkySVJVVZWGDh2qoqIizZs3T88//7zeeuutBoF66tSpmjBhgi6++GItXrxYr7zyio444ogGn3HjjTfqnHPO0ZdffqnTTjtNY8eO1c6dOy39ngAApALDNE3T7iIAAMCPGzdunP79738rMzOzwf4//elP+tOf/iTDMDR+/HhNnTo1fuynP/2pjjzySD344IN69NFHNWnSJG3YsEE5OTmSpNdee00jR47Upk2b1Lx5c7Vu3Vq//vWvdcstt+yzBsMwdN111+nmm2+WFAvxubm5ev3117mXHACA7+EebgAAUshJJ53UIFBLUpMmTeKvBw0a1ODYoEGDtHDhQknSsmXL1Ldv33jYlqTBgwcrGo1qxYoVMgxDmzZt0sknn7zfGvr06RN/nZOTo/z8fG3btu1QvxIAAGmLwA0AQArJycnZa4h3Y8nKyjqg87xeb4NtwzAUjUYTURIAACmNe7gBAEgjn3zyyV7b3bt3lyR1795dixYtUlVVVfz4Rx99JJfLpa5duyovL0/t27fX22+/bWnNAACkK3q4AQBIIYFAQFu2bGmwz+PxqGnTppKk559/XgMGDNCxxx6rp59+Wp999pkee+wxSdLYsWP1l7/8RRdeeKEmT56s7777Tn/4wx90/vnnq3nz5pKkyZMna/z48SopKdHw4cNVUVGhjz76SH/4wx+s/aIAAKQBAjcAACnkjTfeUMuWLRvs69q1q5YvXy4pNoP4jBkzdMkll6hly5Z65pln1KNHD0lSdna2Zs2apcsvv1wDBw5Udna2zjrrLN1zzz3x97rwwgtVW1urv/3tb7rqqqvUtGlTnX322dZ9QQAA0gizlAMAkCYMw9DMmTN15pln2l0KAAAQ93ADAAAAAJAQBG4AAAAAABKAe7gBAEgT3CUGAEByoYcbAAAAAIAEIHADAAAAAJAABG4AAAAAABKAwA0AAAAAQAIQuAEAAAAASAACNwAAAAAACUDgBgAAAAAgAQjcAAAAAAAkwP8HPlm/CbebmfUAAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(1,epochs+1), train_loss, 'r', label=\"Train Loss\")\n",
    "ax.plot(np.arange(1,epochs+1), val_loss, 'b', label=\"Val Loss\")\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Train Curve')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "查看训练后的分类性能"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9860    0.9782    0.9821       504\n",
      "           2     0.9800    0.9761    0.9780       502\n",
      "           3     0.9600    0.9776    0.9687       491\n",
      "           4     0.9740    0.9819    0.9779       496\n",
      "           5     0.9800    0.9800    0.9800       500\n",
      "           6     0.9860    0.9860    0.9860       500\n",
      "           7     0.9740    0.9760    0.9750       499\n",
      "           8     0.9840    0.9743    0.9791       505\n",
      "           9     0.9680    0.9661    0.9670       501\n",
      "          10     0.9900    0.9861    0.9880       502\n",
      "\n",
      "    accuracy                         0.9782      5000\n",
      "   macro avg     0.9782    0.9782    0.9782      5000\n",
      "weighted avg     0.9783    0.9782    0.9782      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_prob = model(val_x)\n",
    "\n",
    "pred = np.argmax(pred_prob,axis=1) + 1\n",
    "from sklearn.metrics import classification_report\n",
    "target =np.argmax(val_y, axis=1) + 1\n",
    "report = classification_report(pred, target, digits=4)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可视化隐藏层参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 500x500 with 25 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_hidden_layer(theta):\n",
    "    \"\"\"\n",
    "    :param theta: with shape of (401, 25)\n",
    "    \"\"\"\n",
    "    hidden_layer = theta[1:, :]\n",
    "\n",
    "    fig, ax_array = plt.subplots(nrows=5, ncols=5, sharey=True, sharex=True, figsize=(5, 5))\n",
    "\n",
    "    for r in range(5):\n",
    "        for c in range(5):\n",
    "            ax_array[r, c].matshow(hidden_layer[:,5 * r + c].reshape((20, 20)), cmap=matplotlib.cm.gray)\n",
    "            plt.xticks(np.array([]))\n",
    "            plt.yticks(np.array([]))\n",
    "\n",
    "plot_hidden_layer(model.theta1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}